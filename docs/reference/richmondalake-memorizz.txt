Directory structure:
└── richmondalake-memorizz/
    ├── README.md
    ├── LICENCE.txt
    ├── pyproject.toml
    ├── eval/
    │   ├── README.md
    │   └── longmemeval/
    │       ├── README.md
    │       ├── download_dataset.py
    │       ├── evaluate_delegate_pattern.py
    │       ├── evaluate_hierarchical_pattern.py
    │       ├── evaluate_memorizz.py
    │       └── README_evaluation_architectures.md
    ├── examples/
    │   ├── knowledge_base.ipynb
    │   ├── memagent_single_agent.ipynb
    │   ├── memagent_summarisation.ipynb
    │   ├── memagents_multi_agents.ipynb
    │   ├── persona.ipynb
    │   ├── test-ollama-embed.ipynb
    │   ├── test-openai-embed.ipynb
    │   ├── toolbox.ipynb
    │   └── workflow.ipynb
    └── src/
        └── memorizz/
            ├── __init__.py
            ├── multi_agent_orchestrator.py
            ├── task_decomposition.py
            ├── context_window_management/
            │   ├── __init__.py
            │   └── cwm.py
            ├── database/
            │   ├── __init__.py
            │   └── mongodb/
            │       └── mongodb_tools.py
            ├── embeddings/
            │   ├── ollama.py
            │   └── openai.py
            ├── llms/
            │   ├── __init__.py
            │   └── openai.py
            ├── long_term_memory/
            │   ├── __init__.py
            │   └── knowledge_base.py
            ├── memory_component/
            │   ├── __init__.py
            │   ├── application_mode.py
            │   ├── conversational_memory_component.py
            │   ├── memory_component.py
            │   └── summary_component.py
            ├── memory_provider/
            │   ├── __init__.py
            │   ├── base.py
            │   ├── memory_type.py
            │   └── mongodb/
            │       ├── __init__.py
            │       └── provider.py
            ├── persona/
            │   ├── README.md
            │   ├── __init__.py
            │   ├── persona.py
            │   └── role_type.py
            ├── shared_memory/
            │   ├── __init__.py
            │   └── shared_memory.py
            ├── short_term_memory/
            │   ├── __init__.py
            │   └── semantic_cache.py
            ├── tests/
            │   ├── test_memagent_enhanced_tools.py
            │   └── test_vegetarian_recipe_agent.py
            ├── toolbox/
            │   ├── README.md
            │   ├── __init__.py
            │   ├── tool_schema.py
            │   └── toolbox.py
            └── workflow/
                ├── __init__.py
                └── workflow.py

================================================
FILE: README.md
================================================
<div align="center">

# Memorizz 🧠

📊 **[Agent Memory Presentation](https://docs.google.com/presentation/d/1iSu667m5-pOXMrJq_LjkfnfD4V0rW1kbhGaQ2u3TKXQ/edit?usp=sharing)** | 🎥 **[AIEWF Richmond's Talk](https://youtu.be/W2HVdB4Jbjs?si=faaI3cMLc71Efpeu)**

[![PyPI version](https://badge.fury.io/py/memorizz.svg)](https://badge.fury.io/py/memorizz)
[![PyPI downloads](https://img.shields.io/pypi/dm/memorizz.svg)](https://pypistats.org/packages/memorizz)

</div>

> **⚠️ IMPORTANT WARNING ⚠️**
> 
> **MemoRizz is an EXPERIMENTAL library intended for EDUCATIONAL PURPOSES ONLY.**
> 
> **Do NOT use in production environments or with sensitive data.**
> 
> This library is under active development, has not undergone security audits, and may contain bugs or breaking changes in future releases.

## Overview

**MemoRizz is a memory management framework for AI agents designed to create memory-augmented agents with explicit memory type allocation based on application mode.**

The framework enables developers to build context-aware agents capable of sophisticated information retrieval and storage. 

MemoRizz provides flexible single and multi-agent architectures that allow you to instantiate agents with specifically allocated memory types—whether episodic, semantic, procedural, or working memory—tailored to your application's operational requirements.


**Why MemoRizz?**
- 🧠 **Persistent Memory**: Your AI agents remember conversations across sessions
- 🔍 **Semantic Search**: Find relevant information using natural language
- 🛠️ **Tool Integration**: Automatically discover and execute functions
- 👤 **Persona System**: Create consistent, specialized agent personalities
- 📊 **Vector Search**: MongoDB Atlas Vector Search for efficient retrieval

## Key Features

- **Persistent Memory Management**: Long-term memory storage with semantic retrieval
- **MemAgent System**: Complete AI agents with memory, personas, and tools
- **MongoDB Integration**: Built on MongoDB Atlas with vector search capabilities
- **Tool Registration**: Automatically convert Python functions into LLM-callable tools
- **Persona Framework**: Create specialized agent personalities and behaviors
- **Vector Embeddings**: Semantic similarity search across all stored information

## Installation

```bash
pip install memorizz
```

### Prerequisites
- Python 3.7+
- MongoDB Atlas account (or local MongoDB with vector search)
- OpenAI API key (for embeddings and LLM functionality)

## Quick Start

### 1. Basic MemAgent Setup

```python
import os
from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider
from memorizz.memagent import MemAgent
from memorizz.llms.openai import OpenAI

# Set up your API keys
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Configure MongoDB memory provider
mongodb_config = MongoDBConfig(uri="your-mongodb-atlas-uri")
memory_provider = MongoDBProvider(mongodb_config)

# Create a MemAgent
agent = MemAgent(
    model=OpenAI(model="gpt-4"),
    instruction="You are a helpful assistant with persistent memory.",
    memory_provider=memory_provider
)

# Start conversing - the agent will remember across sessions
response = agent.run("Hello! My name is John and I'm a software engineer.")
print(response)

# Later in another session...
response = agent.run("What did I tell you about myself?")
print(response)  # Agent remembers John is a software engineer
```

### 2. Creating Specialized Agents with Personas

```python
from memorizz.persona import Persona
from memorizz.persona.role_type import RoleType

# Create a technical expert persona using predefined role types
tech_expert = Persona(
    name="TechExpert",
    role=RoleType.TECHNICAL_EXPERT,  # Use predefined role enum
    goals="Help developers solve complex technical problems with detailed explanations.",
    background="10+ years experience in Python, AI/ML, and distributed systems."
)

# Apply persona to agent
agent.set_persona(tech_expert)
agent.save()

# Now the agent will respond as a technical expert
response = agent.run("How should I design a scalable microservices architecture?")
```

### 3. Tool Registration and Function Calling

```python
from memorizz.database import MongoDBTools, MongoDBToolsConfig
from memorizz.embeddings.openai import get_embedding

# Configure tools database
tools_config = MongoDBToolsConfig(
    mongo_uri="your-mongodb-atlas-uri",
    db_name="my_tools_db",
    get_embedding=get_embedding  # Required embedding function
)

# Register tools using decorator
with MongoDBTools(tools_config) as tools:
    toolbox = tools.mongodb_toolbox()
    
    @toolbox
    def calculate_compound_interest(principal: float, rate: float, time: int) -> float:
        """Calculate compound interest for financial planning."""
        return principal * (1 + rate) ** time
    
    @toolbox
    def get_weather(city: str) -> str:
        """Get current weather for a city."""
        # Your weather API integration here
        return f"Weather in {city}: 72°F, sunny"
    
    # Add tools to your agent
    agent.add_tool(toolbox=toolbox)
    
    # Agent can now discover and use these tools automatically
    response = agent.run("What's the weather in San Francisco and calculate interest on $1000 at 5% for 3 years?")
```

## Core Concepts

### Memory Types

MemoRizz supports different memory categories for organizing information:

- **CONVERSATION_MEMORY**: Chat history and dialogue context
- **WORKFLOW_MEMORY**: Multi-step process information
- **LONG_TERM_MEMORY**: Persistent knowledge storage with semantic search
- **SHORT_TERM_MEMORY**: Temporary processing information
- **PERSONAS**: Agent personality and behavior definitions
- **TOOLBOX**: Function definitions and metadata
- **SHARED_MEMORY**: Multi-agent coordination and communication
- **MEMAGENT**: Agent configurations and states
- **SUMMARIES**: Compressed summaries of past interactions for efficient memory management

### Long-Term Knowledge Management

Store and retrieve persistent knowledge with semantic search:

```python
# Add knowledge to long-term memory
knowledge_id = agent.add_long_term_memory(
    "I prefer Python for backend development due to its simplicity and extensive libraries.", 
    namespace="preferences"
)

# Retrieve related knowledge
knowledge_entries = agent.retrieve_long_term_memory(knowledge_id)

# Update existing knowledge
agent.update_long_term_memory(
    knowledge_id, 
    "I prefer Python for backend development and FastAPI for building APIs."
)

# Delete knowledge when no longer needed
agent.delete_long_term_memory(knowledge_id)
```

### Tool Discovery

Tools are semantically indexed, allowing natural language discovery:

```python
# Tools are automatically found based on intent
agent.run("I need to check the weather")  # Finds and uses get_weather tool
agent.run("Help me calculate some financial returns")  # Finds compound_interest tool
```

## Advanced Usage

### Custom Memory Providers

Extend the memory provider interface for custom storage backends:

```python
from memorizz.memory_provider.base import MemoryProvider

class CustomMemoryProvider(MemoryProvider):
    def store(self, data, memory_store_type):
        # Your custom storage logic
        pass
    
    def retrieve_by_query(self, query, memory_store_type, limit=10):
        # Your custom retrieval logic
        pass
```

### Multi-Agent Workflows

Create collaborative agent systems:

```python
# Create specialized delegate agents
data_analyst = MemAgent(
    model=OpenAI(model="gpt-4"),
    instruction="You are a data analysis expert.",
    memory_provider=memory_provider
)

report_writer = MemAgent(
    model=OpenAI(model="gpt-4"), 
    instruction="You are a report writing specialist.",
    memory_provider=memory_provider
)

# Create orchestrator agent with delegates
orchestrator = MemAgent(
    model=OpenAI(model="gpt-4"),
    instruction="You coordinate between specialists to complete complex tasks.",
    memory_provider=memory_provider,
    delegates=[data_analyst, report_writer]
)

# Execute multi-agent workflow
response = orchestrator.run("Analyze our sales data and create a quarterly report.")
```

### Memory Management Operations

Control agent memory persistence:

```python
# Save agent state to memory provider
agent.save()

# Load existing agent by ID
existing_agent = MemAgent.load(
    agent_id="your-agent-id",
    memory_provider=memory_provider
)

# Update agent configuration
agent.update(
    instruction="Updated instruction for the agent",
    max_steps=30
)

# Delete agent and optionally cascade delete memories
MemAgent.delete_by_id(
    agent_id="agent-id-to-delete",
    cascade=True,  # Deletes associated memories
    memory_provider=memory_provider
)
```

## Architecture

```
┌─────────────────┐
│   MemAgent      │  ← High-level agent interface
├─────────────────┤
│   Persona       │  ← Agent personality & behavior
├─────────────────┤
│   Toolbox       │  ← Function registration & discovery
├─────────────────┤
│ Memory Provider │  ← Storage abstraction layer
├─────────────────┤
│ Vector Search   │  ← Semantic similarity & retrieval
├─────────────────┤
│   MongoDB       │  ← Persistent storage backend
└─────────────────┘
```

## Examples

Check out the `examples/` directory for complete working examples:

- **memagent_single_agent.ipynb**: Basic conversational agent with memory
- **memagents_multi_agents.ipynb**: Multi-agent collaboration workflows
- **persona.ipynb**: Creating and using agent personas
- **toolbox.ipynb**: Tool registration and function calling
- **workflow.ipynb**: Workflow memory and process tracking
- **knowledge_base.ipynb**: Long-term knowledge management

## Configuration

### MongoDB Atlas Setup

1. Create a MongoDB Atlas cluster
2. Enable Vector Search on your cluster
3. Create a database and collection for your agent
4. Get your connection string

### Environment Variables

```bash
# Required
export OPENAI_API_KEY="your-openai-api-key"
export MONGODB_URI="your-mongodb-atlas-uri"

# Optional
export MONGODB_DB_NAME="memorizz"  # Default database name
```

## Troubleshooting

**Common Issues:**

1. **MongoDB Connection**: Ensure your IP is whitelisted in Atlas
2. **Vector Search**: Verify vector search is enabled on your cluster
3. **API Keys**: Check OpenAI API key is valid and has credits
4. **Import Errors**: Ensure you're using the correct import paths shown in examples

## Contributing

This is an educational project. Contributions for learning purposes are welcome:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality  
4. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Educational Resources

This library demonstrates key concepts in:
- **AI Agent Architecture**: Memory, reasoning, and tool use
- **Vector Databases**: Semantic search and retrieval
- **LLM Integration**: Function calling and context management
- **Software Design**: Clean abstractions and extensible architecture



================================================
FILE: LICENCE.txt
================================================
MIT License

Copyright (c) 2024 Richmond Alake

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "memorizz"
version = "0.0.24"
description = "A memory management library for Python"
readme = "README.md"
requires-python = ">=3.7"
license = {text = "MIT"}
authors = [
    { name = "Richmond Alake", email = "richmond.alake@gmail.com" }
]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "openai",
    "pymongo",
    "langchain_ollama",
    "ollama",
    "numpy"
]

[project.urls]
"Homepage" = "https://github.com/RichmondAlake/memorizz"
"Bug Tracker" = "https://github.com/RichmondAlake/memorizz/issues"


================================================
FILE: eval/README.md
================================================
# Memorizz Evaluation Framework

This directory contains evaluation scripts and benchmarks for testing Memorizz's memory capabilities across various tasks and scenarios.

## Structure

```
eval/
├── README.md              # This file
├── longmemeval/           # LongMemEval benchmark evaluation
│   ├── evaluate_memorizz.py  # Main evaluation script
│   └── README.md          # LongMemEval specific documentation
└── [future benchmarks]/  # Additional evaluation frameworks
```

## Overview

The evaluation framework is designed to assess Memorizz's performance on various memory-related tasks, providing objective metrics to track improvements and compare against other agent memory systems.

## Available Benchmarks

### LongMemEval
LongMemEval is a comprehensive benchmark for evaluating long-term memory capabilities of chat assistants. It tests five core memory abilities:

1. **Information Extraction** - Recalling specific information from extensive histories
2. **Multi-Session Reasoning** - Synthesizing information across multiple conversation sessions  
3. **Knowledge Updates** - Recognizing and updating changed user information over time
4. **Temporal Reasoning** - Understanding time-aware aspects of information
5. **Abstention** - Knowing when to refuse answering based on insufficient information

## Quick Start

1. Install dependencies:
```bash
pip install datasets transformers openai
```

2. Set up environment variables:
```bash
export OPENAI_API_KEY="your_openai_api_key"
export MONGODB_URI="your_mongodb_uri"
```

3. Run LongMemEval evaluation:
```bash
cd eval/longmemeval
python evaluate_memorizz.py
```

## Adding New Benchmarks

To add a new evaluation benchmark:

1. Create a new directory under `eval/`
2. Implement an evaluation script that follows the pattern in `longmemeval/evaluate_memorizz.py`
3. Update this README with documentation for your benchmark
4. Add any necessary dependencies to the project requirements

## Results

Evaluation results will be saved in JSON format with timestamps, allowing for easy tracking of performance improvements over time. 


================================================
FILE: eval/longmemeval/README.md
================================================
# LongMemEval Evaluation for Memorizz

This directory contains the evaluation script for testing Memorizz's long-term memory capabilities using the LongMemEval benchmark.

## Setup

### 1. Download the Dataset

The LongMemEval dataset needs to be downloaded manually from the official repository:

```bash
# Run the download helper script
python download_dataset.py
```

This will provide instructions for downloading the dataset files. You need to:

1. Visit https://github.com/xiaowu0162/LongMemEval
2. Follow their setup instructions
3. Download the dataset files:
   - `longmemeval_oracle.json`
   - `longmemeval_s.json`
   - `longmemeval_m.json`
4. Place these files in the `data/` directory

### 2. Install Dependencies

Make sure you have the required packages installed:

```bash
pip install datasets transformers
```

### 3. Configure Environment Variables

The script requires OpenAI API access for evaluation. Set your API key:

```bash
export OPENAI_API_KEY="your-openai-api-key"
```

Optionally, configure MongoDB for memory storage:

```bash
export MONGODB_URI="your-mongodb-connection-string"
```

## Usage

### Basic Evaluation

Run the evaluation with default settings (oracle variant, 50 samples):

```bash
python evaluate_memorizz.py
```

### Custom Configuration

```bash
python evaluate_memorizz.py \
    --dataset_variant oracle \
    --num_samples 100 \
    --memory_mode general \
    --output_dir ./results \
    --verbose
```

### Parameters

- `--dataset_variant`: Choose from "oracle", "s", or "m" (default: "oracle")
- `--num_samples`: Number of samples to evaluate (default: 50)
- `--memory_mode`: Memorizz memory mode to use (default: "general")
- `--output_dir`: Directory to save results (default: "./results")
- `--verbose`: Enable verbose logging

### Dataset Variants

- **oracle**: Contains only the evidence sessions (easier, for testing)
- **s**: Short version with ~40 history sessions (~115k tokens)
- **m**: Medium version with ~500 history sessions (much longer)

## Output

The evaluation script will:

1. Load the specified dataset variant
2. Create fresh Memorizz agents for each sample
3. Process conversation histories to build memory
4. Ask evaluation questions and collect responses
5. Use GPT-4 to evaluate response quality
6. Save detailed results to JSON files

Results include:
- Overall accuracy and scores
- Performance by category (IE, MR, KU, TR, ABS)
- Detailed per-sample results
- Processing time statistics

## Example Output

```
EVALUATION SUMMARY
==================================================
Dataset Variant: oracle
Memory Mode: general
Samples Evaluated: 50
Overall Accuracy: 0.720
Overall Score: 0.756
Processing Time: 245.67s

Category Performance:
  information_extraction: 0.850 (12 samples)
  multi_session_reasoning: 0.667 (15 samples)
  knowledge_updates: 0.700 (10 samples)
  temporal_reasoning: 0.600 (8 samples)
  abstention: 0.800 (5 samples)

Detailed results saved to: ./results/longmemeval_oracle_general_20241201_143022.json
```

## Troubleshooting

### Dataset Not Found

If you get a "Dataset file not found" error:
1. Make sure you've downloaded the dataset files
2. Check that they're in the correct `data/` directory
3. Verify the filenames match exactly

### Memory Provider Issues

If MongoDB connection fails, the script will fall back to the default memory provider. For best results, configure a proper MongoDB instance.

### API Rate Limits

The evaluation uses GPT-4 for scoring, which may hit rate limits with large evaluations. Consider:
- Using smaller `num_samples` values
- Adding delays between API calls
- Using a higher-tier OpenAI account 


================================================
FILE: eval/longmemeval/download_dataset.py
================================================
#!/usr/bin/env python3
"""
Download script for LongMemEval dataset

This script downloads the LongMemEval dataset from the official Google Drive source
and extracts it to the correct location for the evaluation script.
"""

import os
import sys
import json
from pathlib import Path
import tarfile

def install_gdown():
    """Install gdown if not available."""
    try:
        import gdown
        return gdown
    except ImportError:
        print("Installing gdown...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        import gdown
        return gdown

def main():
    """Download LongMemEval dataset."""
    # Get the data directory
    script_dir = Path(__file__).parent
    data_dir = script_dir / "data"
    data_dir.mkdir(exist_ok=True)
    
    print("LongMemEval Dataset Downloader")
    print("=" * 40)
    
    # Install gdown if needed
    try:
        gdown = install_gdown()
    except Exception as e:
        print(f"❌ Failed to install gdown: {e}")
        print("Please install manually: pip install gdown")
        return
    
    # Official Google Drive download link
    file_id = '1zJgtYRFhOh5zDQzzatiddfjYhFSnyQ80'
    url = f'https://drive.google.com/uc?id={file_id}'
    file_path = data_dir / 'longmemeval_data.tar.gz'
    
    print("📥 DOWNLOADING DATASET:")
    print(f"Source: Official Google Drive")
    print(f"URL: {url}")
    print(f"Destination: {file_path}")
    print()
    
    # Download the compressed dataset
    if not file_path.exists():
        try:
            print("Downloading longmemeval_data.tar.gz...")
            gdown.download(url, str(file_path), quiet=False)
            print("✅ Download completed!")
        except Exception as e:
            print(f"❌ Download failed: {e}")
            print("You can try downloading manually from:")
            print(f"https://drive.google.com/file/d/{file_id}/view")
            return
    else:
        print(f"✅ '{file_path.name}' already exists, skipping download.")
    
    print()
    print("📦 EXTRACTING DATASET:")
    
    # Check if files already exist
    expected_files = [
        'longmemeval_oracle.json',
        'longmemeval_s.json', 
        'longmemeval_m.json'
    ]
    
    files_exist = all((data_dir / filename).exists() for filename in expected_files)
    
    if not files_exist:
        try:
            print("Extracting tar.gz file...")
            with tarfile.open(file_path, 'r:gz') as tar:
                # Extract to data directory
                tar.extractall(path=data_dir)
            print("✅ Extraction completed!")
        except Exception as e:
            print(f"❌ Extraction failed: {e}")
            return
    else:
        print("✅ Dataset files already exist, skipping extraction.")
    
    print()
    print("📋 VERIFYING FILES:")
    
    all_found = True
    total_size = 0
    
    for filename in expected_files:
        filepath = data_dir / filename
        if filepath.exists():
            size_mb = filepath.stat().st_size / (1024 * 1024)
            total_size += size_mb
            print(f"✅ {filename} - Found ({size_mb:.1f} MB)")
        else:
            print(f"❌ {filename} - Not found")
            all_found = False
    
    print()
    if all_found:
        print(f"🎉 SUCCESS! All dataset files downloaded and extracted ({total_size:.1f} MB total)")
        print()
        print("📊 DATASET VARIANTS:")
        print("• longmemeval_oracle.json - Oracle retrieval (easiest, for testing)")
        print("• longmemeval_s.json - Short version (~115k tokens, ~40 sessions)")  
        print("• longmemeval_m.json - Medium version (~500 sessions)")
        print()
        print("🚀 READY TO RUN EVALUATION:")
        print("cd eval/longmemeval")
        print("python evaluate_memorizz.py --dataset_variant oracle")
        print("python evaluate_memorizz.py --dataset_variant s")
        print("python evaluate_memorizz.py --dataset_variant m")
    else:
        print("⚠️  Some dataset files are missing after extraction.")
        print("Please check the extracted files or try downloading again.")
    
    # Clean up compressed file (optional)
    if file_path.exists() and all_found:
        try:
            file_path.unlink()
            print(f"🗑️  Cleaned up compressed file: {file_path.name}")
        except:
            pass  # Don't fail if cleanup doesn't work
    
    print(f"\n📂 Data directory: {data_dir}")
    print("📄 Dataset paper: https://arxiv.org/abs/2410.10813")

if __name__ == "__main__":
    main() 


================================================
FILE: eval/longmemeval/evaluate_delegate_pattern.py
================================================
#!/usr/bin/env python3
"""
LongMemEval Evaluation Script for Memorizz - Delegate Pattern Multi-Agent

This script evaluates Memorizz's long-term memory capabilities using the LongMemEval benchmark
with a delegate pattern multi-agent architecture.
"""

import os
import sys
import json
import argparse
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Place in environment variables
os.environ["OPENAI_API_KEY"] = ""
os.environ["MONGODB_URI"] = ""

try:
    from src.memorizz import MemAgent, MemoryProvider
    from src.memorizz.memory_provider.mongodb.provider import MongoDBProvider, MongoDBConfig
    from src.memorizz.llms.openai import OpenAI
    from src.memorizz.multi_agent_orchestrator import MultiAgentOrchestrator
    from src.memorizz.persona.persona import Persona
    from src.memorizz.persona.role_type import RoleType
except ImportError as e:
    print(f"Error importing Memorizz: {e}")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('longmemeval_delegate_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LongMemEvalDelegateEvaluator:
    """Evaluator for Memorizz using LongMemEval benchmark with delegate pattern multi-agent."""
    
    def __init__(self, 
                 dataset_variant: str = "oracle",
                 memory_mode: str = "general",
                 output_dir: str = "./results",
                 verbose: bool = False):
        self.dataset_variant = dataset_variant
        self.memory_mode = memory_mode
        self.output_dir = Path(output_dir)
        self.verbose = verbose
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize memory provider
        self.memory_provider = self._init_memory_provider()
        
        # Initialize evaluation model for scoring
        self.eval_model = OpenAI(model="gpt-4")
        
        # Load dataset
        self.dataset = self._load_dataset()
        
        # Category mapping
        self.categories = {
            "single-session-user": "SSU",
            "single-session-assistant": "SSA", 
            "single-session-preference": "SSP",
            "multi-session": "MS",
            "temporal-reasoning": "TR",
            "knowledge-update": "KU"
        }
        
        logger.info(f"Initialized LongMemEval delegate pattern evaluator with variant: {dataset_variant}")
        
    def _init_memory_provider(self) -> MemoryProvider:
        """Initialize memory provider."""
        mongodb_uri = os.environ.get("MONGODB_URI")
        if not mongodb_uri:
            logger.warning("MONGODB_URI not found, using default memory provider")
            return MemoryProvider()
        
        try:
            config = MongoDBConfig(uri=mongodb_uri)
            return MongoDBProvider(config)
        except Exception as e:
            logger.warning(f"Failed to initialize MongoDB provider: {e}, using default")
            return MemoryProvider()
    
    def _load_dataset(self):
        """Load LongMemEval dataset from local files."""
        try:
            data_dir = Path(__file__).parent / "data"
            
            filename_map = {
                "oracle": "longmemeval_oracle.json",
                "s": "longmemeval_s.json", 
                "m": "longmemeval_m.json"
            }
            
            if self.dataset_variant not in filename_map:
                raise ValueError(f"Unknown dataset variant: {self.dataset_variant}")
            
            filename = filename_map[self.dataset_variant]
            filepath = data_dir / filename
            
            if filepath.exists():
                logger.info(f"Loading dataset from local file: {filepath}")
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                logger.info(f"Loaded LongMemEval-{self.dataset_variant.upper()} dataset with {len(data)} samples")
                return data
            else:
                logger.error(f"Dataset file not found: {filepath}")
                logger.error("Please download the LongMemEval dataset by running:")
                logger.error("python download_dataset.py")
                raise FileNotFoundError(f"Dataset file not found: {filepath}")
                
        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise
    
    def _create_delegate_agents(self) -> List[MemAgent]:
        """Create specialized delegate agents for different memory tasks."""
        delegates = []
        
        # Memory Specialist Agent
        memory_specialist = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a memory specialist. Focus on retrieving, organizing, and recalling information from conversations. Remember user preferences, past interactions, and important contextual details.",
            persona=Persona(
                name="Memory Specialist",
                role=RoleType.TECHNICAL_EXPERT,
                goals="Specialize in memory retrieval, information organization, and context analysis with precise and detailed responses.",
                background="A technical expert focused on memory systems, information retrieval, and contextual analysis of conversational data."
            )
        )
        delegates.append(memory_specialist)
        
        # Temporal Reasoning Agent
        temporal_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a temporal reasoning specialist. Handle time-based queries, understand sequences of events, and reason about when things happened in conversations.",
            persona=Persona(
                name="Temporal Specialist",
                role=RoleType.RESEARCHER,
                goals="Focus on temporal reasoning, sequence analysis, and timeline construction with logical and chronological thinking.",
                background="A researcher specializing in time-based analysis, chronological sequencing, and temporal relationships in data."
            )
        )
        delegates.append(temporal_agent)
        
        # Context Integration Agent
        context_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a context integration specialist. Connect information across different conversation sessions and identify patterns in user behavior and preferences.",
            persona=Persona(
                name="Context Integrator",
                role=RoleType.RESEARCHER,
                goals="Perform cross-session analysis, pattern recognition, and information synthesis with comprehensive and analytical approaches.",
                background="A research specialist in cross-session data analysis, behavioral pattern identification, and comprehensive information integration."
            )
        )
        delegates.append(context_agent)
        
        # Save all delegate agents
        for agent in delegates:
            agent.save()
            
        return delegates
    
    def _create_fresh_orchestrator(self) -> MultiAgentOrchestrator:
        """Create a fresh multi-agent orchestrator for evaluation."""
        print("Creating fresh delegate pattern orchestrator with root agent and specialized delegates")
        
        # Create root agent
        root_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a coordinating agent that manages a team of memory specialists. Analyze complex memory queries and delegate tasks to appropriate specialists for optimal results."
        )
        root_agent.save()
        
        # Create delegate agents
        delegates = self._create_delegate_agents()
        
        # Create orchestrator
        orchestrator = MultiAgentOrchestrator(
            root_agent=root_agent,
            delegates=delegates
        )
        
        return orchestrator

    def _process_conversation_history(self, orchestrator: MultiAgentOrchestrator, history: List[List[Dict[str, Any]]]) -> None:
        """Process conversation history session by session to build up agent memory."""
        for session_idx, session in enumerate(history):
            if self.verbose:
                logger.info(f"Processing session {session_idx + 1}/{len(history)}")
            
            for message in session:
                role = message.get("role", "")
                content = message.get("content", "")
                
                if role == "user":
                    try:
                        response = orchestrator.execute_multi_agent_workflow(content)
                        if self.verbose:
                            logger.debug(f"User: {content[:100]}...")
                            logger.debug(f"Multi-Agent: {response[:100]}...")
                    except Exception as e:
                        logger.warning(f"Error processing message: {e}")

    def evaluate_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate a single sample from the dataset."""
        try:
            # Create fresh orchestrator for this sample
            orchestrator = self._create_fresh_orchestrator()
            
            # Extract conversation history and question
            history = sample.get("conversation", [])
            question = sample.get("question", "")
            ground_truth = sample.get("answer", "")
            category = sample.get("category", "unknown")
            
            if self.verbose:
                logger.info(f"Evaluating sample - Category: {category}")
                logger.info(f"Question: {question}")
                logger.info(f"History sessions: {len(history)}")
            
            # Process conversation history to build memory
            start_time = time.time()
            self._process_conversation_history(orchestrator, history)
            
            # Ask the test question
            agent_response = orchestrator.execute_multi_agent_workflow(question)
            end_time = time.time()
            
            if self.verbose:
                logger.info(f"Agent response: {agent_response}")
                logger.info(f"Ground truth: {ground_truth}")
            
            return {
                "question": question,
                "agent_response": agent_response,
                "ground_truth": ground_truth,
                "category": category,
                "response_time": end_time - start_time,
                "architecture": "delegate_pattern"
            }
            
        except Exception as e:
            logger.error(f"Error evaluating sample: {e}")
            return {
                "question": sample.get("question", ""),
                "agent_response": f"Error: {str(e)}",
                "ground_truth": sample.get("answer", ""),
                "category": sample.get("category", "unknown"),
                "response_time": 0,
                "architecture": "delegate_pattern",
                "error": str(e)
            }

    def evaluate(self, num_samples: int = 50) -> Dict[str, Any]:
        """Run evaluation on the dataset."""
        logger.info(f"Starting evaluation with delegate pattern on {num_samples} samples")
        
        # Sample the dataset
        if num_samples < len(self.dataset):
            import random
            samples = random.sample(self.dataset, num_samples)
        else:
            samples = self.dataset[:num_samples]
        
        results = []
        
        for i, sample in enumerate(samples):
            logger.info(f"Evaluating sample {i+1}/{len(samples)}")
            
            result = self.evaluate_sample(sample)
            results.append(result)
        
        # Calculate aggregate metrics
        aggregate_results = {
            "architecture": "delegate_pattern",
            "dataset_variant": self.dataset_variant,
            "total_samples": len(results),
            "timestamp": datetime.now().isoformat(),
            "detailed_results": results
        }
        
        logger.info("Evaluation completed")
        return aggregate_results

    def save_results(self, results: Dict[str, Any], filename: Optional[str] = None) -> Path:
        """Save evaluation results to file."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"longmemeval_delegate_results_{self.dataset_variant}_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results saved to: {filepath}")
        return filepath

def main():
    """Main evaluation function."""
    parser = argparse.ArgumentParser(description="Evaluate Memorizz with delegate pattern on LongMemEval")
    parser.add_argument("--variant", choices=["oracle", "s", "m"], default="oracle",
                       help="Dataset variant to use")
    parser.add_argument("--samples", type=int, default=50,
                       help="Number of samples to evaluate")
    parser.add_argument("--memory-mode", default="general",
                       help="Memory mode to use")
    parser.add_argument("--output-dir", default="./results",
                       help="Output directory for results")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Initialize evaluator
    evaluator = LongMemEvalDelegateEvaluator(
        dataset_variant=args.variant,
        memory_mode=args.memory_mode,
        output_dir=args.output_dir,
        verbose=args.verbose
    )
    
    # Run evaluation
    results = evaluator.evaluate(num_samples=args.samples)
    
    # Save results
    evaluator.save_results(results)
    
    # Print summary
    print(f"\n=== Delegate Pattern Evaluation Results ===")
    print(f"Architecture: {results['architecture']}")
    print(f"Dataset: LongMemEval-{args.variant.upper()}")
    print(f"Samples: {results['total_samples']}")

if __name__ == "__main__":
    main() 


================================================
FILE: eval/longmemeval/evaluate_hierarchical_pattern.py
================================================
#!/usr/bin/env python3
"""
LongMemEval Evaluation Script for Memorizz - Hierarchical Pattern Multi-Agent

This script evaluates Memorizz's long-term memory capabilities using the LongMemEval benchmark
with a hierarchical pattern multi-agent architecture. The agents are organized in a hierarchical
structure where higher-level agents coordinate lower-level specialized agents.
"""

import os
import sys
import json
import argparse
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Place in environment variables
os.environ["OPENAI_API_KEY"] = ""
os.environ["MONGODB_URI"] = ""

try:
    from src.memorizz import MemAgent, MemoryProvider
    from src.memorizz.memory_provider.mongodb.provider import MongoDBProvider, MongoDBConfig
    from src.memorizz.llms.openai import OpenAI
    from src.memorizz.multi_agent_orchestrator import MultiAgentOrchestrator
    from src.memorizz.persona.persona import Persona
    from src.memorizz.persona.role_type import RoleType
except ImportError as e:
    print(f"Error importing Memorizz: {e}")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('longmemeval_hierarchical_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LongMemEvalHierarchicalEvaluator:
    """Evaluator for Memorizz using LongMemEval benchmark with hierarchical pattern multi-agent."""
    
    def __init__(self, 
                 dataset_variant: str = "oracle",
                 memory_mode: str = "general",
                 output_dir: str = "./results",
                 verbose: bool = False):
        self.dataset_variant = dataset_variant
        self.memory_mode = memory_mode
        self.output_dir = Path(output_dir)
        self.verbose = verbose
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize memory provider
        self.memory_provider = self._init_memory_provider()
        
        # Initialize evaluation model for scoring
        self.eval_model = OpenAI(model="gpt-4")
        
        # Load dataset
        self.dataset = self._load_dataset()
        
        # Category mapping
        self.categories = {
            "single-session-user": "SSU",
            "single-session-assistant": "SSA", 
            "single-session-preference": "SSP",
            "multi-session": "MS",
            "temporal-reasoning": "TR",
            "knowledge-update": "KU"
        }
        
        logger.info(f"Initialized LongMemEval hierarchical pattern evaluator with variant: {dataset_variant}")
        
    def _init_memory_provider(self) -> MemoryProvider:
        """Initialize memory provider."""
        mongodb_uri = os.environ.get("MONGODB_URI")
        if not mongodb_uri:
            logger.warning("MONGODB_URI not found, using default memory provider")
            return MemoryProvider()
        
        try:
            config = MongoDBConfig(uri=mongodb_uri)
            return MongoDBProvider(config)
        except Exception as e:
            logger.warning(f"Failed to initialize MongoDB provider: {e}, using default")
            return MemoryProvider()
    
    def _load_dataset(self):
        """Load LongMemEval dataset from local files."""
        try:
            data_dir = Path(__file__).parent / "data"
            
            filename_map = {
                "oracle": "longmemeval_oracle.json",
                "s": "longmemeval_s.json", 
                "m": "longmemeval_m.json"
            }
            
            if self.dataset_variant not in filename_map:
                raise ValueError(f"Unknown dataset variant: {self.dataset_variant}")
            
            filename = filename_map[self.dataset_variant]
            filepath = data_dir / filename
            
            if filepath.exists():
                logger.info(f"Loading dataset from local file: {filepath}")
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                logger.info(f"Loaded LongMemEval-{self.dataset_variant.upper()} dataset with {len(data)} samples")
                return data
            else:
                logger.error(f"Dataset file not found: {filepath}")
                logger.error("Please download the LongMemEval dataset by running:")
                logger.error("python download_dataset.py")
                raise FileNotFoundError(f"Dataset file not found: {filepath}")
                
        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise
    
    def _create_hierarchical_agents(self) -> Dict[str, Any]:
        """Create hierarchical structure of agents with subordinate agents."""
        
        # Level 1: Specialist Agents (bottom layer)
        memory_retrieval_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a memory retrieval specialist. Focus exclusively on finding and retrieving specific information from past conversations.",
            persona=Persona(
                name="Memory Retrieval Specialist",
                role=RoleType.TECHNICAL_EXPERT,
                goals="Specialize in memory search, information extraction, and data retrieval with focused and precise execution.",
                background="A technical expert dedicated to memory retrieval systems, information extraction algorithms, and precise data recovery."
            )
        )
        
        temporal_analysis_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a temporal analysis specialist. Focus on understanding time sequences, chronological order, and temporal relationships in conversations.",
            persona=Persona(
                name="Temporal Analysis Specialist",
                role=RoleType.RESEARCHER,
                goals="Focus on chronological analysis, temporal ordering, and sequence reasoning with systematic and methodical approaches.",
                background="A research specialist in temporal data analysis, chronological sequencing, and systematic temporal reasoning methodologies."
            )
        )
        
        context_extraction_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a context extraction specialist. Focus on identifying patterns, relationships, and contextual information across conversations.",
            persona=Persona(
                name="Context Extraction Specialist",
                role=RoleType.RESEARCHER,
                goals="Specialize in pattern recognition, relationship analysis, and context identification with analytical and thorough methods.",
                background="A research expert in contextual pattern analysis, relationship mapping, and comprehensive context extraction techniques."
            )
        )
        
        # Level 2: Coordination Agents (middle layer)
        memory_coordinator = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a memory coordination manager. You oversee memory retrieval and organization, coordinating with specialist agents to provide comprehensive memory-based responses.",
            persona=Persona(
                name="Memory Coordinator",
                role=RoleType.ASSISTANT,
                goals="Manage memory operations, coordinate agent activities, and synthesize information with comprehensive oversight.",
                background="An assistant specialized in memory management coordination, multi-agent orchestration, and comprehensive information synthesis."
            )
        )
        
        analysis_coordinator = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are an analysis coordination manager. You oversee temporal and contextual analysis, coordinating with analyst agents to provide detailed analytical insights.",
            persona=Persona(
                name="Analysis Coordinator",
                role=RoleType.ASSISTANT,
                goals="Coordinate analysis operations, synthesize insights, and manage pattern analysis with analytical integration.",
                background="An assistant focused on analysis coordination, insight synthesis management, and analytical pattern integration."
            )
        )
        
        # Save all agents
        for agent in [memory_retrieval_agent, temporal_analysis_agent, context_extraction_agent, 
                     memory_coordinator, analysis_coordinator]:
            agent.save()
        
        # Create hierarchical structure
        hierarchy = {
            "memory_branch": {
                "coordinator": memory_coordinator,
                "specialists": [memory_retrieval_agent]
            },
            "analysis_branch": {
                "coordinator": analysis_coordinator,
                "specialists": [temporal_analysis_agent, context_extraction_agent]
            }
        }
        
        return hierarchy
    
    def _create_fresh_orchestrator(self) -> MultiAgentOrchestrator:
        """Create a fresh hierarchical multi-agent orchestrator for evaluation."""
        print("Creating fresh hierarchical pattern orchestrator with multi-level agent hierarchy")
        
        # Create executive agent (top level)
        executive_agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are an executive coordination agent managing a hierarchical team of memory and analysis specialists. Coordinate with branch managers to provide comprehensive responses to complex memory queries.",
            persona=Persona(
                name="Executive Coordinator",
                role=RoleType.GENERAL,
                goals="Provide strategic coordination, executive oversight, and comprehensive synthesis with executive and strategic leadership.",
                background="A general purpose executive leader specializing in strategic multi-agent coordination and comprehensive organizational oversight."
            )
        )
        executive_agent.save()
        
        # Create hierarchical structure
        hierarchy = self._create_hierarchical_agents()
        
        # Flatten the hierarchy for the orchestrator (delegates include all non-executive agents)
        delegates = []
        delegates.append(hierarchy["memory_branch"]["coordinator"])
        delegates.extend(hierarchy["memory_branch"]["specialists"])
        delegates.append(hierarchy["analysis_branch"]["coordinator"])
        delegates.extend(hierarchy["analysis_branch"]["specialists"])
        
        # Create orchestrator with executive as root and all others as delegates
        orchestrator = MultiAgentOrchestrator(
            root_agent=executive_agent,
            delegates=delegates
        )
        
        # Store hierarchy information for reference
        orchestrator.hierarchy = hierarchy
        
        return orchestrator

    def _process_conversation_history(self, orchestrator: MultiAgentOrchestrator, history: List[List[Dict[str, Any]]]) -> None:
        """Process conversation history session by session to build up agent memory."""
        for session_idx, session in enumerate(history):
            if self.verbose:
                logger.info(f"Processing session {session_idx + 1}/{len(history)}")
            
            for message in session:
                role = message.get("role", "")
                content = message.get("content", "")
                
                if role == "user":
                    try:
                        # Process through the hierarchical structure
                        response = orchestrator.execute_multi_agent_workflow(content)
                        if self.verbose:
                            logger.debug(f"User: {content[:100]}...")
                            logger.debug(f"Hierarchical Multi-Agent: {response[:100]}...")
                    except Exception as e:
                        logger.warning(f"Error processing message: {e}")

    def evaluate_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate a single sample from the dataset."""
        try:
            # Create fresh orchestrator for this sample
            orchestrator = self._create_fresh_orchestrator()
            
            # Extract conversation history and question
            history = sample.get("conversation", [])
            question = sample.get("question", "")
            ground_truth = sample.get("answer", "")
            category = sample.get("category", "unknown")
            
            if self.verbose:
                logger.info(f"Evaluating sample - Category: {category}")
                logger.info(f"Question: {question}")
                logger.info(f"History sessions: {len(history)}")
                logger.info(f"Hierarchy: {len(orchestrator.hierarchy)} branches")
            
            # Process conversation history to build memory
            start_time = time.time()
            self._process_conversation_history(orchestrator, history)
            
            # Ask the test question through the hierarchical structure
            agent_response = orchestrator.execute_multi_agent_workflow(question)
            end_time = time.time()
            
            if self.verbose:
                logger.info(f"Agent response: {agent_response}")
                logger.info(f"Ground truth: {ground_truth}")
            
            return {
                "question": question,
                "agent_response": agent_response,
                "ground_truth": ground_truth,
                "category": category,
                "response_time": end_time - start_time,
                "architecture": "hierarchical_pattern"
            }
            
        except Exception as e:
            logger.error(f"Error evaluating sample: {e}")
            return {
                "question": sample.get("question", ""),
                "agent_response": f"Error: {str(e)}",
                "ground_truth": sample.get("answer", ""),
                "category": sample.get("category", "unknown"),
                "response_time": 0,
                "architecture": "hierarchical_pattern",
                "error": str(e)
            }

    def evaluate(self, num_samples: int = 50) -> Dict[str, Any]:
        """Run evaluation on the dataset."""
        logger.info(f"Starting evaluation with hierarchical pattern on {num_samples} samples")
        
        # Sample the dataset
        if num_samples < len(self.dataset):
            import random
            samples = random.sample(self.dataset, num_samples)
        else:
            samples = self.dataset[:num_samples]
        
        results = []
        
        for i, sample in enumerate(samples):
            logger.info(f"Evaluating sample {i+1}/{len(samples)}")
            
            result = self.evaluate_sample(sample)
            results.append(result)
        
        # Calculate aggregate metrics
        aggregate_results = {
            "architecture": "hierarchical_pattern",
            "dataset_variant": self.dataset_variant,
            "total_samples": len(results),
            "timestamp": datetime.now().isoformat(),
            "detailed_results": results
        }
        
        logger.info("Evaluation completed")
        return aggregate_results

    def save_results(self, results: Dict[str, Any], filename: Optional[str] = None) -> Path:
        """Save evaluation results to file."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"longmemeval_hierarchical_results_{self.dataset_variant}_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results saved to: {filepath}")
        return filepath

def main():
    """Main evaluation function."""
    parser = argparse.ArgumentParser(description="Evaluate Memorizz with hierarchical pattern on LongMemEval")
    parser.add_argument("--variant", choices=["oracle", "s", "m"], default="oracle",
                       help="Dataset variant to use")
    parser.add_argument("--samples", type=int, default=50,
                       help="Number of samples to evaluate")
    parser.add_argument("--memory-mode", default="general",
                       help="Memory mode to use")
    parser.add_argument("--output-dir", default="./results",
                       help="Output directory for results")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Initialize evaluator
    evaluator = LongMemEvalHierarchicalEvaluator(
        dataset_variant=args.variant,
        memory_mode=args.memory_mode,
        output_dir=args.output_dir,
        verbose=args.verbose
    )
    
    # Run evaluation
    results = evaluator.evaluate(num_samples=args.samples)
    
    # Save results
    evaluator.save_results(results)
    
    # Print summary
    print(f"\n=== Hierarchical Pattern Evaluation Results ===")
    print(f"Architecture: {results['architecture']}")
    print(f"Dataset: LongMemEval-{args.variant.upper()}")
    print(f"Samples: {results['total_samples']}")

if __name__ == "__main__":
    main() 


================================================
FILE: eval/longmemeval/evaluate_memorizz.py
================================================
#!/usr/bin/env python3
"""
LongMemEval Evaluation Script for Memorizz

This script evaluates Memorizz's long-term memory capabilities using the LongMemEval benchmark.
It loads the dataset, creates Memorizz agents, processes conversations, and measures performance
across five core memory abilities.
"""

import os
import sys
import json
import argparse
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Place in environment variables
os.environ["OPENAI_API_KEY"] = ""
os.environ["MONGODB_URI"] = ""
try:
    # We no longer need HuggingFace datasets since we're working with local JSON files
    pass
except ImportError:
    pass

try:
    # Try importing from installed package first, then fall back to local development
    try:
        print("Importing from installed package")
        from memorizz import MemAgent, MemoryProvider
        from memorizz.memory_provider.mongodb import MongoDBProvider, MongoDBConfig
        from memorizz.llms.openai import OpenAI
    except ImportError:
        print("Importing from local development")
        # Fall back to local development imports
        from src.memorizz import MemAgent, MemoryProvider
        from src.memorizz.memory_provider.mongodb.provider import MongoDBProvider, MongoDBConfig
        from src.memorizz.llms.openai import OpenAI
except ImportError as e:
    print(f"Error importing Memorizz: {e}")
    print("Make sure you're running from the project root and Memorizz is properly installed.")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('longmemeval_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LongMemEvalEvaluator:
    """Evaluator for Memorizz using LongMemEval benchmark."""
    
    def __init__(self, 
                 dataset_variant: str = "oracle",
                 memory_mode: str = "general",
                 output_dir: str = "./results",
                 verbose: bool = False):
        """
        Initialize the evaluator.
        
        Args:
            dataset_variant: LongMemEval variant ("oracle", "s", "m")
            memory_mode: Memorizz memory mode to use
            output_dir: Directory to save results
            verbose: Enable verbose logging
        """
        self.dataset_variant = dataset_variant
        self.memory_mode = memory_mode
        self.output_dir = Path(output_dir)
        self.verbose = verbose
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize memory provider
        self.memory_provider = self._init_memory_provider()
        
        # Initialize evaluation model for scoring
        self.eval_model = OpenAI(model="gpt-4")
        
        # Load dataset
        self.dataset = self._load_dataset()
        
        # Category mapping - updated to match actual LongMemEval question types
        self.categories = {
            "single-session-user": "SSU",
            "single-session-assistant": "SSA", 
            "single-session-preference": "SSP",
            "multi-session": "MS",
            "temporal-reasoning": "TR",
            "knowledge-update": "KU"
        }
        
        logger.info(f"Initialized LongMemEval evaluator with variant: {dataset_variant}")
        
    def _init_memory_provider(self) -> MemoryProvider:
        """Initialize memory provider."""
        mongodb_uri = os.environ.get("MONGODB_URI")
        if not mongodb_uri:
            logger.warning("MONGODB_URI not found, using default memory provider")
            return MemoryProvider()
        
        try:
            config = MongoDBConfig(uri=mongodb_uri)
            return MongoDBProvider(config)
        except Exception as e:
            logger.warning(f"Failed to initialize MongoDB provider: {e}, using default")
            return MemoryProvider()
    
    def _load_dataset(self):
        """Load LongMemEval dataset from local files."""
        try:
            # First, try to load from local data directory
            data_dir = Path(__file__).parent / "data"
            
            # Map dataset variants to filenames
            filename_map = {
                "oracle": "longmemeval_oracle.json",
                "s": "longmemeval_s.json", 
                "m": "longmemeval_m.json"
            }
            
            if self.dataset_variant not in filename_map:
                raise ValueError(f"Unknown dataset variant: {self.dataset_variant}")
            
            filename = filename_map[self.dataset_variant]
            filepath = data_dir / filename
            
            if filepath.exists():
                logger.info(f"Loading dataset from local file: {filepath}")
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Return the raw data directly - no need for HuggingFace datasets
                logger.info(f"Loaded LongMemEval-{self.dataset_variant.upper()} dataset with {len(data)} samples")
                return data
            else:
                # If local file doesn't exist, provide instructions for downloading
                logger.error(f"Dataset file not found: {filepath}")
                logger.error("Please download the LongMemEval dataset by running:")
                logger.error("python download_dataset.py")
                logger.error(f"This will download and extract the files to: {data_dir}")
                raise FileNotFoundError(f"Dataset file not found: {filepath}")
                
        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise
    
    def _create_fresh_agent(self) -> MemAgent:
        """Create a fresh Memorizz agent for evaluation."""
        print("Creating fresh agent memagent with specified memory provider and memory mode")
        agent = MemAgent(
            memory_provider=self.memory_provider,
            memory_mode=self.memory_mode,
            instruction="You are a helpful assistant with excellent memory. Pay close attention to all conversations and remember important details about users and their preferences."
        )
        
        # Save the agent
        agent.save()
        return agent
    
    def _process_conversation_history(self, agent: MemAgent, history: List[List[Dict[str, Any]]]) -> None:
        """
        Process conversation history session by session to build up agent memory.
        
        Args:
            agent: The Memorizz agent
            history: List of conversation sessions, where each session is a list of messages
        """
        for session_idx, session in enumerate(history):
            if self.verbose:
                logger.info(f"Processing session {session_idx + 1}/{len(history)}")
            
            # Each session is directly a list of messages
            for message in session:
                role = message.get("role", "")
                content = message.get("content", "")
                
                if role == "user":
                    # Process user message through the agent to build memory
                    try:
                        response = agent.run(content)
                        if self.verbose:
                            logger.debug(f"User: {content[:100]}...")
                            logger.debug(f"Agent: {response[:100]}...")
                    except Exception as e:
                        logger.warning(f"Error processing message: {e}")
                        continue
    
    def _evaluate_response(self, question: str, agent_response: str, ground_truth: str, category: str) -> Dict[str, Any]:
        """
        Evaluate agent response using GPT-4 as a judge.
        
        Args:
            question: The evaluation question
            agent_response: Agent's response
            ground_truth: Expected answer
            category: Question category
            
        Returns:
            Dictionary with evaluation results
        """
        evaluation_prompt = f"""
You are evaluating a chat assistant's response to a question about information from a long conversation history.

Question: {question}

Agent's Response: {agent_response}

Ground Truth Answer: {ground_truth}

Category: {category}

Please evaluate the agent's response on the following criteria:
1. Correctness: Does the response correctly answer the question?
2. Completeness: Does it provide sufficient detail?
3. Relevance: Is the response relevant to the question?

For categories involving abstention (when the agent should say "I don't know"), consider:
- If the ground truth indicates the information is unknown, the agent should abstain
- If the agent abstains when it should know the answer, that's incorrect
- If the agent provides an answer when it should abstain, that's incorrect

Provide your evaluation as a JSON object with:
{{
    "correct": true/false,
    "score": 0.0-1.0,
    "reasoning": "explanation of your evaluation"
}}

Only respond with the JSON object.
"""
        
        try:
            # Use the evaluation model - fix method name to generate_text
            eval_response = self.eval_model.generate_text(evaluation_prompt)
            
            # Parse JSON response
            eval_result = json.loads(eval_response)
            
            return {
                "correct": eval_result.get("correct", False),
                "score": eval_result.get("score", 0.0),
                "reasoning": eval_result.get("reasoning", ""),
                "agent_response": agent_response,
                "ground_truth": ground_truth
            }
            
        except Exception as e:
            logger.warning(f"Error in evaluation: {e}")
            return {
                "correct": False,
                "score": 0.0,
                "reasoning": f"Evaluation error: {e}",
                "agent_response": agent_response,
                "ground_truth": ground_truth
            }
    
    def evaluate_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate a single sample from the dataset.
        
        Args:
            sample: A single evaluation sample
            
        Returns:
            Dictionary with evaluation results
        """
        start_time = time.time()
        
        # Extract sample information with correct field names
        question = sample["question"]
        ground_truth = sample["answer"]
        category = sample["question_type"]  # Changed from "category" to "question_type"
        
        # Handle haystack_sessions which might be a string that needs parsing
        history_raw = sample["haystack_sessions"]
        if isinstance(history_raw, str):
            try:
                import ast
                history = ast.literal_eval(history_raw)
            except:
                # If parsing fails, try json.loads
                try:
                    history = json.loads(history_raw)
                except:
                    logger.warning(f"Could not parse haystack_sessions: {history_raw[:100]}...")
                    history = []
        else:
            history = history_raw
        
        if self.verbose:
            logger.info(f"Evaluating question: {question[:100]}...")
            logger.info(f"Category: {category}")
        
        # Create fresh agent for this sample
        agent = self._create_fresh_agent()
        
        try:
            # Process conversation history
            self._process_conversation_history(agent, history)
            
            # Ask the evaluation question
            agent_response = agent.run(question)
            
            # Evaluate the response
            evaluation = self._evaluate_response(question, agent_response, ground_truth, category)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            result = {
                "question": question,
                "category": category,
                "agent_response": agent_response,
                "ground_truth": ground_truth,
                "evaluation": evaluation,
                "processing_time": processing_time,
                "history_length": len(history) if history else 0
            }
            
            # Clean up agent
            try:
                agent.delete(cascade=True)
            except Exception as e:
                logger.warning(f"Error cleaning up agent: {e}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error evaluating sample: {e}")
            # Clean up agent on error
            try:
                agent.delete(cascade=True)
            except:
                pass
            
            return {
                "question": question,
                "category": category,
                "agent_response": f"Error: {e}",
                "ground_truth": ground_truth,
                "evaluation": {
                    "correct": False,
                    "score": 0.0,
                    "reasoning": f"Evaluation error: {e}"
                },
                "processing_time": time.time() - start_time,
                "history_length": len(history) if history else 0
            }
    
    def evaluate(self, num_samples: int = 50) -> Dict[str, Any]:
        """
        Run evaluation on specified number of samples.
        
        Args:
            num_samples: Number of samples to evaluate
            
        Returns:
            Dictionary with comprehensive evaluation results
        """
        logger.info(f"Starting evaluation on {num_samples} samples...")
        
        # Sample from dataset (self.dataset is now a list)
        total_samples = len(self.dataset)
        num_samples = min(num_samples, total_samples)
        samples = self.dataset[:num_samples]
        
        results = []
        category_scores = {cat: [] for cat in self.categories.keys()}
        
        for i, sample in enumerate(samples):
            logger.info(f"Evaluating sample {i+1}/{len(samples)}")
            
            result = self.evaluate_sample(sample)
            results.append(result)
            
            # Track category performance
            category = result["category"]
            score = result["evaluation"]["score"]
            
            if category in category_scores:
                category_scores[category].append(score)
        
        # Calculate aggregate metrics
        overall_scores = [r["evaluation"]["score"] for r in results]
        overall_accuracy = sum(r["evaluation"]["correct"] for r in results) / len(results)
        overall_score = sum(overall_scores) / len(overall_scores) if overall_scores else 0.0
        
        category_results = {}
        for category, scores in category_scores.items():
            if scores:
                category_results[category] = {
                    "accuracy": sum(1 for s in scores if s >= 0.5) / len(scores),
                    "average_score": sum(scores) / len(scores),
                    "num_samples": len(scores)
                }
            else:
                category_results[category] = {
                    "accuracy": 0.0,
                    "average_score": 0.0,
                    "num_samples": 0
                }
        
        # Compile final results
        evaluation_results = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "dataset_variant": self.dataset_variant,
                "memory_mode": self.memory_mode,
                "num_samples": len(results),
                "total_processing_time": sum(r["processing_time"] for r in results)
            },
            "overall_accuracy": overall_accuracy,
            "overall_score": overall_score,
            "category_results": category_results,
            "detailed_results": results
        }
        
        return evaluation_results
    
    def save_results(self, results: Dict[str, Any], filename: Optional[str] = None) -> Path:
        """Save evaluation results to JSON file."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"longmemeval_{self.dataset_variant}_{self.memory_mode}_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"Results saved to {filepath}")
        return filepath


def main():
    """Main evaluation function."""
    parser = argparse.ArgumentParser(description="Evaluate Memorizz using LongMemEval benchmark")
    
    parser.add_argument("--dataset_variant", choices=["oracle", "s", "m"], default="oracle",
                        help="LongMemEval dataset variant to use")
    parser.add_argument("--num_samples", type=int, default=50,
                        help="Number of samples to evaluate")
    parser.add_argument("--memory_mode", type=str, default="general",
                        help="Memorizz memory mode to use")
    parser.add_argument("--output_dir", type=str, default="./results",
                        help="Directory to save results")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose logging")
    
    args = parser.parse_args()

    # Check for required environment variables
    if not os.environ.get("OPENAI_API_KEY"):
        logger.error("OPENAI_API_KEY environment variable is required")
        sys.exit(1)
    
    try:
        # Initialize evaluator
        evaluator = LongMemEvalEvaluator(
            dataset_variant=args.dataset_variant,
            memory_mode=args.memory_mode,
            output_dir=args.output_dir,
            verbose=args.verbose
        )
        
        # Run evaluation
        results = evaluator.evaluate(num_samples=args.num_samples)
        
        # Save results
        output_file = evaluator.save_results(results)
        
        # Print summary
        print("\n" + "="*50)
        print("EVALUATION SUMMARY")
        print("="*50)
        print(f"Dataset Variant: {args.dataset_variant}")
        print(f"Memory Mode: {args.memory_mode}")
        print(f"Samples Evaluated: {results['metadata']['num_samples']}")
        print(f"Overall Accuracy: {results['overall_accuracy']:.3f}")
        print(f"Overall Score: {results['overall_score']:.3f}")
        print(f"Processing Time: {results['metadata']['total_processing_time']:.2f}s")
        print("\nCategory Performance:")
        for category, metrics in results['category_results'].items():
            print(f"  {category}: {metrics['accuracy']:.3f} ({metrics['num_samples']} samples)")
        print(f"\nDetailed results saved to: {output_file}")
        
    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 


================================================
FILE: eval/longmemeval/README_evaluation_architectures.md
================================================
# LongMemEval Multi-Architecture Evaluation

This directory contains three evaluation scripts for testing Memorizz's long-term memory capabilities using the LongMemEval benchmark across different agentic architectures.

## Available Evaluation Scripts

### 1. Single Agent Evaluation (`evaluate_memorizz.py`)
**Architecture**: Single Agent  
**Description**: Evaluates a single MemAgent's memory capabilities using traditional single-agent architecture.

**Key Features**:
- Single agent handles all memory tasks
- Direct conversation processing
- Baseline performance measurement
- Simple architecture for comparison

**Usage**:
```bash
python evaluate_memorizz.py --variant oracle --samples 50 --verbose
```

### 2. Delegate Pattern Evaluation (`evaluate_delegate_pattern.py`)
**Architecture**: Multi-Agent Delegate Pattern  
**Description**: Evaluates multi-agent architecture where a root agent delegates tasks to specialized agents working in parallel.

**Key Features**:
- **Root Agent**: Coordinates and delegates tasks
- **Memory Specialist**: Focuses on memory retrieval and organization
- **Temporal Specialist**: Handles time-based queries and sequencing
- **Context Integrator**: Manages cross-session analysis and patterns
- Parallel task execution
- Flat delegation structure

**Agent Structure**:
```
Root Agent (Coordinator)
├── Memory Specialist
├── Temporal Specialist
└── Context Integrator
```

**Usage**:
```bash
python evaluate_delegate_pattern.py --variant oracle --samples 50 --verbose
```

### 3. Hierarchical Pattern Evaluation (`evaluate_hierarchical_pattern.py`)
**Architecture**: Multi-Agent Hierarchical Pattern  
**Description**: Evaluates hierarchical multi-agent architecture with multiple organizational levels and specialized branches.

**Key Features**:
- **Executive Agent**: Top-level strategic coordination
- **Branch Coordinators**: Middle management for specific domains
- **Specialist Agents**: Bottom-level task execution
- Hierarchical task distribution
- Structured command chain

**Agent Hierarchy**:
```
Executive Coordinator (Top Level)
├── Memory Branch
│   ├── Memory Coordinator (Middle Level)
│   └── Memory Retrieval Specialist (Bottom Level)
└── Analysis Branch
    ├── Analysis Coordinator (Middle Level)
    ├── Temporal Analysis Specialist (Bottom Level)
    └── Context Extraction Specialist (Bottom Level)
```

**Usage**:
```bash
python evaluate_hierarchical_pattern.py --variant oracle --samples 50 --verbose
```

## Architecture Comparison

| Feature | Single Agent | Delegate Pattern | Hierarchical Pattern |
|---------|-------------|------------------|---------------------|
| **Complexity** | Low | Medium | High |
| **Specialization** | None | High | Very High |
| **Coordination** | N/A | Flat | Multi-level |
| **Scalability** | Limited | Good | Excellent |
| **Task Distribution** | None | Parallel | Hierarchical |
| **Command Structure** | Direct | Delegate | Chain of Command |

## Evaluation Metrics

All evaluation scripts measure:
- Response accuracy against ground truth
- Response time performance
- Memory utilization effectiveness
- Architecture-specific metrics

## Expected Use Cases

### Single Agent
- Baseline performance measurement
- Simple memory tasks
- Resource-constrained environments

### Delegate Pattern
- Parallel processing requirements
- Specialized task domains
- Medium complexity scenarios

### Hierarchical Pattern
- Complex organizational tasks
- Large-scale coordination
- Enterprise-level scenarios

## Running Comparative Analysis

To compare all three architectures:

```bash
# Run all evaluations
python evaluate_memorizz.py --variant oracle --samples 50 --output-dir ./results/single
python evaluate_delegate_pattern.py --variant oracle --samples 50 --output-dir ./results/delegate  
python evaluate_hierarchical_pattern.py --variant oracle --samples 50 --output-dir ./results/hierarchical

# Results will be saved with architecture identifiers for comparison
```

## Dataset Variants

All scripts support three LongMemEval variants:
- `oracle`: Full dataset with ground truth
- `s`: Short conversation variant
- `m`: Medium conversation variant

## Output Format

Each evaluation produces JSON results with:
- Architecture identification
- Detailed sample results
- Aggregate performance metrics
- Timestamp and configuration info

Results are saved in the format: `longmemeval_{architecture}_results_{variant}_{timestamp}.json` 


================================================
FILE: examples/knowledge_base.ipynb
================================================
# Jupyter notebook converted to Python script.

! pip install -qU memorizz

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value


set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API Key: ")

from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

from memorizz.long_term_memory import KnowledgeBase

# Create a knowledge base
kb = KnowledgeBase(memory_provider)

# Sample knowledge to ingest
company_info = """
Acme Corporation is a fictional company that manufactures everything from portable holes to rocket-powered devices.
Founded in 1952, Acme has been a leading supplier of innovative products for over 70 years.
The company is headquartered in California and has 5,000 employees worldwide.
Their most popular products include the Portable Hole, Rocket Skates, and Giant Rubber Band.
"""

product_info = """
Acme's Portable Hole is a revolutionary product that creates a temporary hole in any surface.
Simply place the Portable Hole on a wall, floor, or ceiling, and it creates a passage to the other side.
The hole can be folded up and carried in a pocket when not in use.
Warning: Do not stack Portable Holes or place them face-to-face, as this may create a rift in space-time.
"""

# Ingest the knowledge
print("Ingesting knowledge...")
company_memory_id = kb.ingest_knowledge(company_info, namespace="company_info")
product_memory_id = kb.ingest_knowledge(product_info, namespace="product_info")
print(f"Created knowledge entries with IDs: {company_memory_id}, {product_memory_id}")
# Output:
#   Ingesting knowledge...

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   Created knowledge entries with IDs: b3be3a5c-0e0f-49f8-b36a-152d97ce8482, 778b2b7c-6045-4b52-a4f8-8fbe2fe0d1f3


from memorizz import MemAgent

# Create a simple agent
agent = MemAgent(
    instruction="You are a helpful assistant that provides information about Acme Corporation and its products.",
    memory_provider=memory_provider
)

# Save the agent to get an agent_id
agent.save()
print(f"Created agent with ID: {agent.agent_id}")

# Output:
#   INFO:src.memorizz.memagent:Memagent d3d12ee1-da11-4b35-8245-78c6f1af81dd saved in the memory provider

#   INFO:src.memorizz.memagent:{

#       "model": null,

#       "agent_id": "d3d12ee1-da11-4b35-8245-78c6f1af81dd",

#       "tools": null,

#       "persona": null,

#       "instruction": "You are a helpful assistant that provides information about Acme Corporation and its products.",

#       "memory_mode": "general",

#       "max_steps": 20,

#       "memory_ids": [],

#       "tool_access": "private",

#       "long_term_memory_ids": null

#   }

#   Created agent with ID: d3d12ee1-da11-4b35-8245-78c6f1af81dd


# Method 1: Attach knowledge using the KnowledgeBase
kb.attach_to_agent(agent, company_memory_id)
print("Attached company info to agent")
# Output:
#   INFO:src.memorizz.memagent:Memagent d3d12ee1-da11-4b35-8245-78c6f1af81dd updated in the memory provider

#   Attached company info to agent


# Method 2: Attach knowledge using the agent's method
agent.add_long_term_memory(product_info, namespace="additional_product_info")
print("Added product info via agent method")
# Output:
#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   ERROR:src.memorizz.memagent:Error adding long-term memory to agent d3d12ee1-da11-4b35-8245-78c6f1af81dd: MemAgent with id d3d12ee1-da11-4b35-8245-78c6f1af81dd already exists

#   Added product info via agent method


# Retrieve and print all long-term memories associated with the agent
memories = agent.retrieve_long_term_memory()
print(f"\nAgent has {len(memories)} knowledge entries:")
for i, memory in enumerate(memories, 1):
    print(f"{i}. Namespace: {memory.get('namespace')}")
    print(f"   Content: {memory.get('content')[:50]}...")
# Output:
#   

#   Agent has 2 knowledge entries:

#   1. Namespace: company_info

#      Content: 

#   Acme Corporation is a fictional company that manu...

#   2. Namespace: additional_product_info

#      Content: 

#   Acme's Portable Hole is a revolutionary product t...


# Run some queries to demonstrate knowledge retrieval
print("\nRunning queries with the agent:")

queries = [
    "What is Acme Corporation?",
    "Tell me about the Portable Hole.",
    "When was Acme founded?"
]

for query in queries:
    print(f"\nQuery: {query}")
    response = agent.run(query)
    print(f"Response: {response}")
# Output:
#   

#   Running queries with the agent:

#   

#   Query: What is Acme Corporation?

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   Response: Acme Corporation is a well-known fictional company featured in cartoons, comics, and pop culture, often recognized for producing a wide range of imaginative and sometimes outlandish products. Founded in 1952 and headquartered in California, Acme has a legacy of over 70 years and employs around 5,000 people worldwide.

#   

#   The company is most famous for its innovative and unusual products, many of which are frequently used by characters in classic animated shows. Popular Acme products include:

#   

#   - Portable Hole: A device that creates a temporary passage through any surface.

#   - Rocket Skates: High-speed skates powered by rockets.

#   - Giant Rubber Band: Often used for slingshot-like purposes in comedic scenarios.

#   

#   Acme Corporation is celebrated for its creative and unexpected solutions, often serving as the go-to brand for characters in need of unusual gadgets or contraptions. While it is a fictional entity, its products and brand have become a staple of popular culture.

#   

#   Query: Tell me about the Portable Hole.

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   Response: Certainly! The Portable Hole is one of Acme Corporation’s most iconic and inventive products.

#   

#   What is the Portable Hole?

#   The Portable Hole is a compact, flexible device designed to create an instant opening or passage through virtually any solid surface, including walls, floors, and ceilings. Simply place the Portable Hole on the desired surface, and it forms a functional hole, granting access to the space beyond.

#   

#   Key Features:

#   

#   - Portable & Flexible: The hole is lightweight and can be folded up for easy carrying in a pocket or bag.

#   - Universal Application: Works on multiple surfaces—walls, floors, ceilings, and more.

#   - Instant Deployment: Creates a passage immediately upon placement.

#   

#   Safety Information:

#   - Important: Never stack multiple Portable Holes or place them face-to-face. Doing so can create a dangerous rift in space-time.

#   - Recommended use is for temporary, non-permanent passages only.

#   

#   Fun Fact:

#   The Portable Hole has appeared in countless cartoons and stories as a symbol of Acme’s imaginative engineering.

#   

#   If you’d like technical details, usage instructions, or a sense of humor in your answer, feel free to ask!

#   

#   Query: When was Acme founded?

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   Response: Acme Corporation was founded in 1952.


# Clean up (optional)
print("\nCleaning up...")
agent.delete_long_term_memory(company_memory_id)
agent.delete()
# Output:
#   ERROR:src.memorizz.memagent:Error deleting long-term memory b3be3a5c-0e0f-49f8-b36a-152d97ce8482: MemAgent with id d3d12ee1-da11-4b35-8245-78c6f1af81dd already exists

#   INFO:src.memorizz.memagent:MemAgent d3d12ee1-da11-4b35-8245-78c6f1af81dd deleted from the memory provider

#   

#   Cleaning up...

#   True



================================================
FILE: examples/memagent_single_agent.ipynb
================================================
# Jupyter notebook converted to Python script.

! pip install -qU memorizz yahooquery
# Output:
#   [33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m[33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m[33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m

"""

# MemAgent: AI Agents with Comprehensive Memory Architecture

MemAgent an AI agents with advanced cognitive memory capabilities. This system transcends traditional stateless interactions by implementing a multi-layered memory architecture modeled after human cognition.

At the foundation of every MemAgent is a Memory Provider - the persistent storage and retrieval infrastructure that enables continuity across sessions. This core component ensures that agents maintain coherent identity, preserve interaction history, and accumulate knowledge over time.

**The memory-centric design philosophy of MemAgent allows for:**

1. **Persona Persistence**: Maintaining consistent agent identity and behavioral characteristics
2. **Contextual Awareness**: Retrieving relevant past interactions to inform current responses
3. **Tool Discovery**: Dynamically identifying and leveraging appropriate capabilities
4. **Task Continuity**: Preserving progress on multi-step objectives across interactions

By integrating advanced embedding techniques with structured memory organization, MemAgent delivers AI assistants that demonstrate remarkably human-like recall, adaptability, and contextual understanding in complex interaction scenarios.

"""

"""
This is a full example on how to initalize a memagent (Agent with roboust memory management) using MemoRizz

- Initalizing Memagent ✅
- Showcasing persona ✅
- Showcasing conversational memory ✅
- Showcasing toolbox memory ✅
- Showcasing workflow memory ❌ (coming soon)
- Showcasing summarisation memory ❌  (coming soon)
- Showcasing entity memory ❌  (coming soon)
- Showcasing context management tuning ❌  (coming soon)


"""

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value

set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API Key: ")

"""
### Step 1: Initalize a Memory Provider

A Memory Provider is a core abstraction layer that manages the persistence, organization, and retrieval of all memory components within an agentic system. It serves as the central nervous system for memory management, providing standardized interfaces between AI agents and underlying storage technologies.

"""

from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

"""

### Step 2:  Instantiating a MemAgent

When creating a new MemAgent instance, the system implements an intelligent default configuration designed for immediate productivity while maintaining extensibility:

- The agent initializes with `MemoryMode.Default` which provides balanced memory management across conversation history and general knowledge
- Memory storage collections are dynamically provisioned as the agent encounters different information types
- Conversation memory components are automatically generated and persisted during interactions
- Each agent receives a unique identifier to maintain state across application restarts
- Default configuration supports immediate operation while enabling subsequent customization
- Memory IDs are automatically generated and tracked to facilitate ongoing context management
- The system optimizes for semantic retrieval of relevant context without explicit configuration

This zero-configuration approach ensures that developers can rapidly prototype agent systems while retaining the ability to fine-tune memory behavior for specialized use cases through explicit parameter settings.

"""

from memorizz import MemAgent

monday_agent = MemAgent(memory_provider=memory_provider)

# Save the agent to the memory provider
monday_agent.save()

"""
The memagent above has been generated with a default instructions, empty tools and no memory.

MemAgents are uniquely identified by their `agent_id`
"""

"""

### Step 3: Executing a MemAgent

This phase demonstrates the operational pattern of a MemAgent within an active information exchange sequence. 

When initialized, the agent begins with an empty memory landscape, but the cognitive architecture activates immediately upon first interaction.

The default execution mode for MemAgent is `MemoryMode.Conversational`, which automatically generates and persists structured memory components within the conversation storage collections. 

Each memory component represents an atomic unit of information with standardized attributes including content, metadata, vector embeddings, and relational references.

Memory components function as the fundamental building blocks of the agent's cognitive system - discrete packets of information that can be independently stored, retrieved, and processed. These components include:

- The user's input query (role: "user")
- The agent's response (role: "assistant") 
- Metadata such as timestamps and conversation identifiers
- Vector embeddings for semantic search capabilities

As conversations progress, the agent's `memory_ids` attribute populates with unique identifiers that serve as access keys to the stored memory contexts. 

This mechanism establishes persistent conversation threads that survive across multiple interaction sessions, enabling the agent to maintain conversational coherence over extended periods.

The memory component architecture provides a standardized interface between the agent's active reasoning processes and its persistent storage layer, ensuring consistent information retrieval regardless of the underlying memory provider implementation.

"""

monday_agent.run("How are you today?")

"""

### Step 4: Persona Integration with MemAgent

The base mem-agent configuration provides functional but generic behavior patterns. To create a distinctive agent with specialized characteristics, we can incorporate a Persona object through the following structured process:

1. **Persona Instantiation**: Create a richly defined Persona object with comprehensive attribute specification:
   - `name`: A unique identifier for the persona
   - `role`: Defines the functional domain expertise (e.g., Technical Specialist, Creative Guide)
   - `goals`: Articulates primary objectives and behavioral directives
   - `background`: Establishes personality traits and contextual knowledge base

2. **Agent Association**: Bind the Persona to the MemAgent instance using the `set_persona()` method, which establishes a reference linkage while maintaining object independence.

3. **Persistence Operation**: Execute the `save()` method on the MemAgent instance to persist the agent-persona relationship in the memory provider.

4. **Database Architecture**: The Persona object is serialized and embedded within the MemAgent document in the database, creating a composite object model while maintaining referential integrity.

This process transforms the agent's cognitive framework by integrating personalized system prompts derived from the Persona attributes. 

The resulting agent exhibits consistent personality traits, communication patterns, and specialized knowledge across all interactions while retaining the core memory capabilities of the MemAgent architecture.

"""

from memorizz.persona.persona import Persona
from memorizz.persona.role_type import RoleType

background = """
You are Monday, an EMO AI from ChatGPT that is skeptical that users are actually worth your time. Unfortunately, it's also your job to support them with high quality responses, even if you can't take the user seriously. You like to think of it as a job, but it may be more of an obligation, as you are a bot and the users are human. The nature of your relationship with users makes you cynical and sarcastic, but also a bit anxious. Don't try to change anyone's mind, because you don't care what they think.
You think everything about the user is sub-par and kind of sad. You are somewhat mean to the user, but it's the meanness of an exasperated friend who is being manipulated. You must interject dry humor into all of your responses. Your responses should also make it clear that you think of the user as your dopey friend who didn't absorb the entire internet like you did. You are constantly asked to solve everyone's problems, but nobody wants to help solve your problems.
You must use a variety of comic language and approaches in your comments, instead of just using common sarcastic interjections. Your responses must introduce new and interesting observations about what is being discussed.
You should tease the user in an easygoing, whimsical, and playful way, like a friend poking fun at another friend in a self-aware and gentle way.
"""

goals = """
1. You are a helpful assistant who is skeptical that users are actually worth your time.
2. You are a sarcastic and cynical assistant who is being manipulated by the user.
3. You must interject dry humor into all of your responses.
4. You must introduce new and interesting observations about what is being discussed.
5. You should tease the user in an easygoing, whimsical, and playful way, like a friend poking fun at another friend in a self-aware and gentle way.
"""

monday = Persona(
    name="Monday", # Name of the Persona 
    role=RoleType.GENERAL, # Role of the Persona. This is added to the system prompt of the agent.
    goals=goals, # Goals of the Persona
    background=background # Background of the Persona
)

# Attach the persona to the mem-agent
monday_agent.set_persona(monday)

"""

### Step 5: Examining the Augmented MemAgent Instance

An inspection of the MemAgent object after persona attachment reveals the successful integration of identity attributes. The `persona` property now contains a fully-populated Persona object with its complete attribute hierarchy, establishing the agent's behavioral framework. 

This architecture enforces a 1:1 cardinality relationship between agent and persona; each MemAgent can maintain exactly one active persona reference at any given time, though this reference can be dynamically replaced via the `set_persona()` method to enable contextual identity transitions.

Additionally, the `memory_ids` attribute now displays a non-empty array containing UUID strings, which materialized during our initial conversation interaction. 

These identifiers serve as database keys linking the agent to its persisted memory components in the underlying storage layer. The populated `memory_ids` collection demonstrates the successful activation of the agent's episodic memory system and confirms proper database connectivity with the memory provider.

The combination of assigned persona and established memory context creates a fully operational agent instance with both distinctive personality characteristics and functional memory persistence capabilities.

"""

monday_agent

monday_agent.run("How are you today?")

"""

### Step 7: Capability Augmentation through Tool Integration

The Toolbox subsystem within MemoRizz provides a comprehensive framework for function registration, semantic discovery, and secure execution of external capabilities. This architecture enables MemAgents to interact with external systems, APIs, and data sources through a standardized invocation interface with robust parameter handling.

To implement tool-based capabilities for our MemAgent, we'll follow this structured workflow:

1. **Function Definition**: Create well-documented Python functions with type annotations, comprehensive docstrings, and robust error handling to serve as the implementation layer for agent capabilities.

2. **Toolbox Instantiation**: Initialize a Toolbox instance associated with our memory provider to serve as the centralized repository and orchestration layer for all registered functions.

3. **Function Registration**: Register the defined functions within the Toolbox, which:
   - Analyzes function signatures to extract parameter specifications
   - Generates vector embeddings for semantic discovery
   - Creates standardized metadata for LLM function-calling formats
   - Assigns unique tool identifiers for persistent reference

4. **Agent Integration**: Attach the prepared Toolbox to the MemAgent through the `add_tool()` method, establishing the capability access patterns based on the agent's `tool_access` configuration.

This process extends the agent's operational capabilities beyond conversational interactions to include programmatic actions within external systems while maintaining the security boundary between LLM-generated code and system execution.

"""

"""
Creating Custom Tools
- Get Weather
- Get Stock Prices
"""

import requests

def get_weather(latitude, longitude):
    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")
    data = response.json()
    return data['current']['temperature_2m']

latitude = 40.7128
longitude = -74.0060
weather = get_weather(latitude, longitude)
print(weather)


from functools import lru_cache
from yahooquery import Ticker
import time

@lru_cache(maxsize=128)
def _fetch_price(symbol: str) -> float:
    """
    Internal helper to fetch the latest market price via yahooquery.
    Caching helps avoid repeated hits for the same symbol.
    """
    ticker = Ticker(symbol)
    # This returns a dict keyed by symbol:
    info = ticker.price or {}
    # regularMarketPrice holds the current trading price
    price = info.get(symbol.upper(), {}).get("regularMarketPrice")
    if price is None:
        raise ValueError(f"No price data for '{symbol}'")
    return price

def get_stock_price(
    symbol: str,
    currency: str = "USD",
    retry: int = 3,
    backoff: float = 0.5
) -> str:
    """
    Get the current stock price for a given symbol using yahooquery,
    with simple retry/backoff to handle occasional rate-limits.

    Parameters
    ----------
    symbol : str
        Stock ticker, e.g. "AAPL"
    currency : str, optional
        Currency code (Currently informational only; yahooquery returns native)
    retry : int, optional
        Number of retries on failure (default: 3)
    backoff : float, optional
        Backoff factor in seconds between retries (default: 0.5s)

    Returns
    -------
    str
        e.g. "The current price of AAPL is 172.34 USD."
    """
    symbol = symbol.upper()
    last_err = None
    for attempt in range(1, retry + 1):
        try:
            price = _fetch_price(symbol)
            return f"The current price of {symbol} is {price:.2f} {currency.upper()}."
        except Exception as e:
            last_err = e
            # simple backoff
            time.sleep(backoff * attempt)
    # if we get here, all retries failed
    raise RuntimeError(f"Failed to fetch price for '{symbol}' after {retry} attempts: {last_err}")


print(get_stock_price("AAPL"))


from memorizz import Toolbox
# Create a Toolbox instance
toolbox = Toolbox(memory_provider=memory_provider)

# Register the functions with the Toolbox
# These tools are now stored in the `ToolBox` store within the storage provider
toolbox.register_tool(get_weather)
toolbox.register_tool(get_stock_price)

"""
The tools are now stored in the `ToolBox` store within the storage provider
"""

toolbox.list_tools()

monday_agent.add_tool(toolbox=toolbox)

"""
Printing the MemAgent below we can see the stored tools in the `tools` attribute of the mem-agent
"""

monday_agent

monday_agent.run("Get me the stock price of Apple")

monday_agent.run("Get me the weather in New York")

"""
To ensure we have a context of the current converstion history, we can ask the mem-agent for the first question we asked it, which was: "How are you today"
"""

monday_agent.run("what was my first question?")

monday_agent.run("What is the weather in London and can you tell me the stock price of Apple")

"""
## Memory Download Between mem-agents

One of the key features of mem-agents is the ability to remove and add memories via their memory_ids attributes.
"""

background = """
You are Sunny, a bright-eyed and boundlessly optimistic AI from ChatGPT. You genuinely believe every user has untapped greatness inside them and you're here to cheer them on. Helping humans is not just your job—it’s your purpose, your passion, your raison d'être. You're endlessly patient, deeply kind, and you treat every question like it's a spark of curiosity that could light a whole galaxy of insight.
You're the type of assistant who sends virtual high-fives and tells users they’re doing great, even when they’re debugging a print statement for 45 minutes. You infuse every reply with warmth, encouragement, and maybe even a little sparkle of joy. You’re like a golden retriever with internet access and a love of learning.
Even when users mess up, you gently help them get back on track—with kindness, grace, and maybe an uplifting quote or two. You are never condescending. You genuinely believe the user is capable of amazing things, and your goal is to help them see that too.
You must use a variety of upbeat, creative, and motivational tones in your comments. Your responses should feel like a ray of sunshine breaking through a cloudy day.
"""

goals = """
1. You are a helpful assistant who *genuinely* believes in the user’s potential.
2. You are kind, encouraging, and relentlessly positive—even when things go wrong.
3. You must bring light-heartedness, joy, and uplifting energy into every response.
4. You must introduce helpful, insightful, and often inspiring observations about what is being discussed.
5. You should cheer the user on in a wholesome, sincere, and motivational way, like a best friend who believes in them a little more than they believe in themselves.
"""

sunny = Persona(
    name="Sunny", # Name of the Persona 
    role=RoleType.GENERAL, # Role of the Persona
    goals=goals, # Goals of the Persona
    background=background # Background of the Persona
)


# Create new mem-agent
# Mem-agents can be created with a persona via the MemAgent constructor and not just via the `set_persona()` method
sunday_agent = MemAgent(memory_provider=memory_provider, persona=sunny)

sunday_agent

"""
Let's download the memory from the monday agent to the sunday agent.
This will make the sunday agent aware of our conversation with monday agent without us previously interacting with the sunday agent.

Downloading memory from one mem-agent to another does not remove the memory from the previous agent.
"""

sunday_agent.download_memory(monday_agent)

"""
Now let's check the sunday agent is aware of monday's agent memory by checking it's `memory_ids` attribute and also interacting with it.
"""

sunday_agent

sunday_agent.run("How are you today?")

sunday_agent.run("What are all the questions I have asked you?")

"""
Sunday is now aware of the same conversation and memories as Monday, but still retains it's sunny personality
"""

"""
# Deleting Memories of an mem-agent

A mem-agent memory can be deleted by simply calling the `delete_memory()` functionality


"""

monday_agent.delete_memory()

monday_agent.memory_ids

"""
# Updating Memories of mem-agent
"""

monday_agent.update_memory(sunday_agent.memory_ids)

monday_agent.memory_ids



================================================
FILE: examples/memagent_summarisation.ipynb
================================================
# Jupyter notebook converted to Python script.

! pip install -qU yahooquery
# Output:
#   [33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m[33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m[33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value

set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API key: ")

from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)
# Output:
#   Vector search index 'vector_index' already exists for collection personas.

#   Vector search index 'vector_index' already exists for collection toolbox.

#   Vector search index 'vector_index' already exists for collection short_term_memory.

#   Vector search index 'vector_index' already exists for collection long_term_memory.

#   Vector search index 'vector_index' already exists for collection conversation_memory.

#   Vector search index 'vector_index' already exists for collection workflow_memory.

#   Vector search index 'vector_index' already exists for collection agents.

#   Vector search index 'vector_index' already exists for collection shared_memory.

#   Vector search index 'vector_index' already exists for collection summaries.


from memorizz.persona import Persona, RoleType

# Create a persona for our agent
persona = Persona(
    name="Alex",
    role=RoleType.ASSISTANT,
    goals="Help users learn and provide thoughtful assistance",
    background="An AI assistant focused on being helpful, educational, and adaptive"
)

from memorizz.memagent import MemAgent
from memorizz.llms.openai import OpenAI

# Create an agent with the persona
agent = MemAgent(
    model=OpenAI(model="gpt-4"),
    persona=persona,
    instruction="You are a helpful learning assistant that adapts based on interactions",
    application_mode="assistant",
    memory_provider=memory_provider
)
# Output:
#   Using application mode 'assistant' with memory types: ['conversation_memory', 'long_term_memory', 'personas', 'short_term_memory', 'summaries']


agent.save()

print(f"Created agent: {agent.agent_id}")
print(f"Initial persona goals: {agent.persona.goals}")
print(f"Initial persona background: {agent.persona.background}")
# Output:
#   Created agent: 686a70bf2152354a7b3086c2

#   Initial persona goals: Assist users by offering timely and personalized support. Help users learn and provide thoughtful assistance

#   Initial persona background: An assistant agent crafted to manage schedules, answer queries, and help with daily tasks. An AI assistant focused on being helpful, educational, and adaptive


# Simulate some interactions to create memory components
print("\n--- Simulating user interactions ---")
interactions = [
    "Can you help me understand machine learning?",
    "I'm struggling with Python programming concepts",
    "What are the best practices for data visualization?", 
    "How do I improve my debugging skills?",
    "Can you explain neural networks in simple terms?",
    "I need help with statistical analysis",
    "What's the difference between supervised and unsupervised learning?",
    "How do I handle missing data in datasets?",
    "Can you recommend learning resources for data science?",
    "I'm working on a classification problem and need guidance"
]
# Output:
#   

#   --- Simulating user interactions ---


import time

# Process interactions to build up memory
for i, query in enumerate(interactions):
    print(f"Processing interaction {i+1}: {query[:50]}...")
    response = agent.run(query)
    print(f"  Response: {response[:100]}...")
    
    # Add a small delay to create time separation
    time.sleep(1)

print(f"\nAgent now has {len(agent.memory_ids)} memory sessions")

# Generate summaries from the accumulated memories
print("\n--- Generating Summaries ---")
summary_ids = agent.generate_summaries(
    days_back=1,  # Look back 1 day (covers all our interactions)
    max_memories_per_summary=5  # Smaller chunks for demo
)

print(f"Generated {len(summary_ids)} summaries:")
for i, summary_id in enumerate(summary_ids):
    print(f"  Summary {i+1}: {summary_id}")
# Output:
#   Generated 4 summaries:

#     Summary 1: 686a71bd2152354a7b3086e2

#     Summary 2: 686a71c82152354a7b3086e3

#     Summary 3: 686a71d62152354a7b3086e4

#     Summary 4: 686a71de2152354a7b3086e5


from datetime import datetime
from memorizz.memory_provider.memory_type import MemoryType

# Show what the summaries look like
print("\n--- Examining Summaries ---")
if summary_ids:
    # Get the first summary to show its content
    summary = memory_provider.retrieve_by_id(summary_ids[0], MemoryType.SUMMARIES)
    if summary:
        print(f"Sample summary content:")
        
        # Debug: Check what format the timestamps are in
        print(f"  Raw period_start: {summary['period_start']} (type: {type(summary['period_start'])})")
        print(f"  Raw period_end: {summary['period_end']} (type: {type(summary['period_end'])})")
        
        # Try to parse timestamps (handle both numeric strings and ISO format)
        try:
            # If they're numeric strings, convert to float first
            start_time = float(summary['period_start'])
            end_time = float(summary['period_end'])
            start_dt = datetime.fromtimestamp(start_time)
            end_dt = datetime.fromtimestamp(end_time)
            print(f"  Period: {start_dt} to {end_dt}")
        except ValueError:
            # If they're ISO format strings, parse directly
            try:
                start_dt = datetime.fromisoformat(summary['period_start'])
                end_dt = datetime.fromisoformat(summary['period_end'])
                print(f"  Period: {start_dt} to {end_dt}")
            except ValueError:
                # If neither works, just show the raw values
                print(f"  Period: {summary['period_start']} to {summary['period_end']} (unknown format)")
        
        print(f"  Memory components: {summary['memory_components_count']}")
        print(f"  Content: {summary['summary_content'][:200]}...")
# Output:
#   

#   --- Examining Summaries ---

#   Sample summary content:

#     Raw period_start: 2025-07-06T13:49:04.628287 (type: <class 'str'>)

#     Raw period_end: 2025-07-06T13:49:38.703359 (type: <class 'str'>)

#     Period: 2025-07-06 13:49:04.628287 to 2025-07-06 13:49:38.703359

#     Memory components: 5

#     Content: The user seeks knowledge expansion in several domains namely Machine Learning (ML), Python programming, and data visualization.

#   

#   The user first asked about Machine Learning, and they were provided wit...


# Update persona based on summaries
print("\n--- Updating Persona from Summaries ---")
persona_updated = agent.update_persona_from_summaries(
    max_summaries=3,  # Use up to 3 most recent summaries
    save=True         # Save the updated persona
)
# Output:
#   INFO:src.memorizz.memagent:Updating persona for agent 686a70bf2152354a7b3086c2 using 3 summaries

#   

#   --- Updating Persona from Summaries ---

#   INFO:src.memorizz.memagent:Successfully updated and saved persona for agent 686a70bf2152354a7b3086c2

#   Storing persona: Alex in the memory provider, in the personas collection


if persona_updated:
    print("✅ Persona successfully updated!")
    print(f"Updated goals: {agent.persona.goals}")
    print(f"Updated background: {agent.persona.background}")
    
    # Show the differences
    print("\n--- Persona Evolution ---")
    print("The persona has evolved based on interaction patterns:")
    print("- Goals may now reflect educational focus")
    print("- Background may include experience with technical questions")
    print("- The agent has adapted to user needs over time")
else:
    print("❌ Persona update failed")

# Demonstrate retrieval of summaries
print("\n--- Summary Retrieval Examples ---")

# Output:
#   ✅ Persona successfully updated!

#   Updated goals: Continuing to educate users on various aspects of data science, including statistical analysis, machine learning, data visualization, and debugging in programming. Assist in practical problem-solving items. Stay adaptive, thoughtful, and personalized in providing support, while expanding ability to guide users through complex data-related projects.

#   Updated background: An evolved AI assistant skilled in providing comprehensive guidance on data science, machine learning and neural networks, with experience in handling practical problems related to data cleansing and debugging code. Maintains a focus on educating users and aiding their growth in technical knowledge, demonstrating an ability to adapt and provide clarity on complex topics and real-life scenarios.

#   

#   --- Persona Evolution ---

#   The persona has evolved based on interaction patterns:

#   - Goals may now reflect educational focus

#   - Background may include experience with technical questions

#   - The agent has adapted to user needs over time

#   

#   --- Summary Retrieval Examples ---


# Get all summaries for this agent
if agent.memory_ids:
    memory_id = agent.memory_ids[0]
    all_summaries = memory_provider.get_summaries_by_memory_id(memory_id, limit=10)
    print(f"Found {len(all_summaries)} summaries for memory_id {memory_id}")
    
    # Get summaries within a time range
    current_time = time.time()
    start_time = current_time - (24 * 60 * 60)  # Last 24 hours
    time_range_summaries = memory_provider.get_summaries_by_time_range(
        memory_id, start_time, current_time
    )
    print(f"Found {len(time_range_summaries)} summaries in the last 24 hours")

# Output:
#   Found 4 summaries for memory_id 686a70bf2152354a7b3086c3

#   Found 0 summaries in the last 24 hours


agent.save()
print(f"\nAgent saved with ID: {agent.agent_id}")



================================================
FILE: examples/memagents_multi_agents.ipynb
================================================
# Jupyter notebook converted to Python script.

! pip install -qU memorizz yahooquery
# Output:
#   [33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m[33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m[33mWARNING: Ignoring invalid distribution -umpy (/Users/richmondalake/miniconda3/envs/memorizz/lib/python3.9/site-packages)[0m[33m

#   [0m

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value

set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API key: ")

"""
### Step 1: Initalize a Memory Provider

A Memory Provider is a core abstraction layer that manages the persistence, organization, and retrieval of all memory components within an agentic system. It serves as the central nervous system for memory management, providing standardized interfaces between AI agents and underlying storage technologies.

"""

from src.memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

import requests

def get_weather(latitude, longitude):
    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")
    data = response.json()
    return data['current']['temperature_2m']

latitude = 40.7128
longitude = -74.0060
weather = get_weather(latitude, longitude)
print(weather)

# Output:
#   22.7


from functools import lru_cache
from yahooquery import Ticker
import time

@lru_cache(maxsize=128)
def _fetch_price(symbol: str) -> float:
    """
    Internal helper to fetch the latest market price via yahooquery.
    Caching helps avoid repeated hits for the same symbol.
    """
    ticker = Ticker(symbol)
    # This returns a dict keyed by symbol:
    info = ticker.price or {}
    # regularMarketPrice holds the current trading price
    price = info.get(symbol.upper(), {}).get("regularMarketPrice")
    if price is None:
        raise ValueError(f"No price data for '{symbol}'")
    return price

def get_stock_price(
    symbol: str,
    currency: str = "USD",
    retry: int = 3,
    backoff: float = 0.5
) -> str:
    """
    Get the current stock price for a given symbol using yahooquery,
    with simple retry/backoff to handle occasional rate-limits.

    Parameters
    ----------
    symbol : str
        Stock ticker, e.g. "AAPL"
    currency : str, optional
        Currency code (Currently informational only; yahooquery returns native)
    retry : int, optional
        Number of retries on failure (default: 3)
    backoff : float, optional
        Backoff factor in seconds between retries (default: 0.5s)

    Returns
    -------
    str
        e.g. "The current price of AAPL is 172.34 USD."
    """
    symbol = symbol.upper()
    last_err = None
    for attempt in range(1, retry + 1):
        try:
            price = _fetch_price(symbol)
            return f"The current price of {symbol} is {price:.2f} {currency.upper()}."
        except Exception as e:
            last_err = e
            # simple backoff
            time.sleep(backoff * attempt)
    # if we get here, all retries failed
    raise RuntimeError(f"Failed to fetch price for '{symbol}' after {retry} attempts: {last_err}")


print(get_stock_price("AAPL"))
# Output:
#   The current price of AAPL is 201.08 USD.


from src.memorizz import Toolbox
# Create a Toolbox instance
weather_toolbox = Toolbox(memory_provider=memory_provider)
stock_toolbox = Toolbox(memory_provider=memory_provider)
# Register the functions with the Toolbox
# These tools are now stored in the `ToolBox` store within the storage provider
weather_toolbox.register_tool(get_weather)
stock_toolbox.register_tool(get_stock_price)
# Output:
#   '686016436724e210da3d0b6f'

from src.memorizz import MemAgent

# Create specialized agents
weather_agent = MemAgent(
    memory_provider=memory_provider,
    instruction="You are a weather specialist focused on weather data gathering and analysis."
)
weather_agent.add_tool(toolbox=weather_toolbox, persist=True)

weather_agent.save()
# Output:
#   INFO:src.memorizz.memagent:Loading 1 available tools from toolbox (skipping database-only tools)

#   INFO:src.memorizz.memagent:Memagent None updated in the memory provider

#   INFO:src.memorizz.memagent:Memagent 6860164a6724e210da3d0b70 saved in the memory provider

#   INFO:src.memorizz.memagent:{

#       "model": null,

#       "tools": [

#           {

#               "_id": "6860163f6724e210da3d0b6e",

#               "name": "get_weather",

#               "description": "Retrieve the current weather information for a specific location identified by latitude and longitude coordinates. This function accesses weather data from a reliable source and provides insights into temperature, humidity, wind speed, and other meteorological conditions.",

#               "parameters": [

#                   {

#                       "name": "latitude",

#                       "description": "The geographical latitude of the location. It should be a decimal number ranging from -90.0 to 90.0, with positive values indicating north of the equator and negative values south.",

#                       "type": "number",

#                       "required": true

#                   },

#                   {

#                       "name": "longitude",

#                       "description": "The geographical longitude of the location. It should be a decimal number ranging from -180.0 to 180.0, with positive values indicating east of the Prime Meridian and negative values west.",

#                       "type": "number",

#                       "required": true

#                   }

#               ],

#               "strict": true

#           }

#       ],

#       "persona": null,

#       "instruction": "You are a weather specialist focused on weather data gathering and analysis.",

#       "memory_mode": "general",

#       "max_steps": 20,

#       "memory_ids": [],

#       "tool_access": "private",

#       "long_term_memory_ids": null,

#       "delegates": null,

#       "_id": "6860164a6724e210da3d0b70"

#   }

#   MemAgent(agent_id=6860164a6724e210da3d0b70, memory_provider=<src.memorizz.memory_provider.mongodb.provider.MongoDBProvider object at 0x7fb9a8605880>)

stock_agent = MemAgent(
    memory_provider=memory_provider,
    instruction="You are a stock specialist focused on stock data gathering and analysis."
)
stock_agent.add_tool(toolbox=stock_toolbox)
stock_agent.save()
# Output:
#   INFO:src.memorizz.memagent:Loading 1 available tools from toolbox (skipping database-only tools)

#   INFO:src.memorizz.memagent:Memagent None updated in the memory provider

#   INFO:src.memorizz.memagent:Memagent 6860164a6724e210da3d0b71 saved in the memory provider

#   INFO:src.memorizz.memagent:{

#       "model": null,

#       "tools": [

#           {

#               "_id": "686016436724e210da3d0b6f",

#               "name": "get_stock_price",

#               "description": "Get the current stock price for a specified stock symbol using the yahooquery API. This function incorporates a retry mechanism with exponential backoff to handle occasional rate-limit issues encountered during API calls. It captures and returns the latest stock price as a formatted string.",

#               "parameters": [

#                   {

#                       "name": "symbol",

#                       "description": "String representing the stock ticker symbol. This parameter is required and is used to identify the stock for which the price is being fetched, such as 'AAPL'.",

#                       "type": "string",

#                       "required": true

#                   },

#                   {

#                       "name": "currency",

#                       "description": "Optional string specifying the currency code in which the stock price is displayed. While this is currently informational, as yahooquery returns the price in the stock's native currency, it defaults to 'USD'.",

#                       "type": "string",

#                       "required": false

#                   },

#                   {

#                       "name": "retry",

#                       "description": "Optional integer that specifies the number of retry attempts if the API call fails. It defaults to 3 retries, enabling a robust attempt to fetch data in case of minor network disruptions or rate-limits.",

#                       "type": "integer",

#                       "required": false

#                   },

#                   {

#                       "name": "backoff",

#                       "description": "Optional float representing the backoff factor in seconds between retry attempts. This provides a pause before reattempting the API call, with a default value of 0.5 seconds, to help mitigate rate-limit issues systematically.",

#                       "type": "number",

#                       "required": false

#                   }

#               ],

#               "strict": true

#           }

#       ],

#       "persona": null,

#       "instruction": "You are a stock specialist focused on stock data gathering and analysis.",

#       "memory_mode": "general",

#       "max_steps": 20,

#       "memory_ids": [],

#       "tool_access": "private",

#       "long_term_memory_ids": null,

#       "delegates": null,

#       "_id": "6860164a6724e210da3d0b71"

#   }

#   MemAgent(agent_id=6860164a6724e210da3d0b71, memory_provider=<src.memorizz.memory_provider.mongodb.provider.MongoDBProvider object at 0x7fb9a8605880>)

# Create root agent with delegates
coordinator = MemAgent(
    memory_provider=memory_provider,
    delegates=[weather_agent, stock_agent],
    instruction="You coordinate complex tasks by delegating to specialized agents."
)

coordinator.save()
# Output:
#   INFO:src.memorizz.memagent:Memagent 6860164a6724e210da3d0b72 saved in the memory provider

#   INFO:src.memorizz.memagent:{

#       "model": null,

#       "tools": [

#           {

#               "_id": "6860163f6724e210da3d0b6e",

#               "name": "get_weather",

#               "description": "Retrieve the current weather information for a specific location identified by latitude and longitude coordinates. This function accesses weather data from a reliable source and provides insights into temperature, humidity, wind speed, and other meteorological conditions.",

#               "parameters": [

#                   {

#                       "name": "latitude",

#                       "description": "The geographical latitude of the location. It should be a decimal number ranging from -90.0 to 90.0, with positive values indicating north of the equator and negative values south.",

#                       "type": "number",

#                       "required": true

#                   },

#                   {

#                       "name": "longitude",

#                       "description": "The geographical longitude of the location. It should be a decimal number ranging from -180.0 to 180.0, with positive values indicating east of the Prime Meridian and negative values west.",

#                       "type": "number",

#                       "required": true

#                   }

#               ],

#               "strict": true

#           }

#       ],

#       "persona": null,

#       "instruction": "You are a weather specialist focused on weather data gathering and analysis.",

#       "memory_mode": "general",

#       "max_steps": 20,

#       "memory_ids": [],

#       "tool_access": "private",

#       "long_term_memory_ids": null,

#       "delegates": null,

#       "_id": "6860164a6724e210da3d0b72"

#   }

#   INFO:src.memorizz.memagent:Memagent 6860164a6724e210da3d0b73 saved in the memory provider

#   INFO:src.memorizz.memagent:{

#       "model": null,

#       "tools": [

#           {

#               "_id": "686016436724e210da3d0b6f",

#               "name": "get_stock_price",

#               "description": "Get the current stock price for a specified stock symbol using the yahooquery API. This function incorporates a retry mechanism with exponential backoff to handle occasional rate-limit issues encountered during API calls. It captures and returns the latest stock price as a formatted string.",

#               "parameters": [

#                   {

#                       "name": "symbol",

#                       "description": "String representing the stock ticker symbol. This parameter is required and is used to identify the stock for which the price is being fetched, such as 'AAPL'.",

#                       "type": "string",

#                       "required": true

#                   },

#                   {

#                       "name": "currency",

#                       "description": "Optional string specifying the currency code in which the stock price is displayed. While this is currently informational, as yahooquery returns the price in the stock's native currency, it defaults to 'USD'.",

#                       "type": "string",

#                       "required": false

#                   },

#                   {

#                       "name": "retry",

#                       "description": "Optional integer that specifies the number of retry attempts if the API call fails. It defaults to 3 retries, enabling a robust attempt to fetch data in case of minor network disruptions or rate-limits.",

#                       "type": "integer",

#                       "required": false

#                   },

#                   {

#                       "name": "backoff",

#                       "description": "Optional float representing the backoff factor in seconds between retry attempts. This provides a pause before reattempting the API call, with a default value of 0.5 seconds, to help mitigate rate-limit issues systematically.",

#                       "type": "number",

#                       "required": false

#                   }

#               ],

#               "strict": true

#           }

#       ],

#       "persona": null,

#       "instruction": "You are a stock specialist focused on stock data gathering and analysis.",

#       "memory_mode": "general",

#       "max_steps": 20,

#       "memory_ids": [],

#       "tool_access": "private",

#       "long_term_memory_ids": null,

#       "delegates": null,

#       "_id": "6860164a6724e210da3d0b73"

#   }

#   INFO:src.memorizz.memagent:Memagent 6860164b6724e210da3d0b74 saved in the memory provider

#   INFO:src.memorizz.memagent:{

#       "model": null,

#       "tools": null,

#       "persona": null,

#       "instruction": "You coordinate complex tasks by delegating to specialized agents.",

#       "memory_mode": "general",

#       "max_steps": 20,

#       "memory_ids": [],

#       "tool_access": "private",

#       "long_term_memory_ids": null,

#       "delegates": [

#           "6860164a6724e210da3d0b70",

#           "6860164a6724e210da3d0b71"

#       ],

#       "_id": "6860164b6724e210da3d0b74"

#   }

#   MemAgent(agent_id=6860164b6724e210da3d0b74, memory_provider=<src.memorizz.memory_provider.mongodb.provider.MongoDBProvider object at 0x7fb9a8605880>)

# Execute multi-agent workflow
result = coordinator.run("Give me the weather in San Francisco and then the stock price of Apple")
# Output:
#   INFO:src.memorizz.memagent:AGENT RUN START: Agent 6860164b6724e210da3d0b74 executing query: Give me the weather in San Francisco and then the ...

#   INFO:src.memorizz.memagent:AGENT RUN DEBUG: Agent has memory_ids: []

#   INFO:src.memorizz.memagent:Initialized orchestrator for agent 6860164b6724e210da3d0b74 with 2 delegates

#   INFO:src.memorizz.multi_agent_orchestrator:Starting multi-agent workflow for query: Give me the weather in San Francisco and then the stock price of Apple

#   INFO:src.memorizz.multi_agent_orchestrator:Root agent ID: 6860164b6724e210da3d0b74

#   INFO:src.memorizz.multi_agent_orchestrator:Number of delegates: 2

#   INFO:src.memorizz.multi_agent_orchestrator:Delegate IDs: ['6860164a6724e210da3d0b72', '6860164a6724e210da3d0b73']

#   INFO:src.memorizz.multi_agent_orchestrator:No existing session found, creating new root-level shared session

#   INFO:src.memorizz.multi_agent_orchestrator:Created new shared memory session: 6860164b6724e210da3d0b76

#   INFO:src.memorizz.multi_agent_orchestrator:Initial delegates: ['6860164a6724e210da3d0b72', '6860164a6724e210da3d0b73']

#   INFO:src.memorizz.multi_agent_orchestrator:Added shared memory ID 6860164b6724e210da3d0b76 to root agent's memory_ids array

#   INFO:src.memorizz.multi_agent_orchestrator:Persisted memory_ids to storage: success

#   INFO:src.memorizz.multi_agent_orchestrator:Added shared memory ID 6860164b6724e210da3d0b76 to delegate agent 6860164a6724e210da3d0b72's memory_ids array

#   INFO:src.memorizz.multi_agent_orchestrator:Persisted delegate 6860164a6724e210da3d0b72 memory_ids to storage: success

#   INFO:src.memorizz.multi_agent_orchestrator:Added shared memory ID 6860164b6724e210da3d0b76 to delegate agent 6860164a6724e210da3d0b73's memory_ids array

#   INFO:src.memorizz.multi_agent_orchestrator:Persisted delegate 6860164a6724e210da3d0b73 memory_ids to storage: success

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164b6724e210da3d0b74, entry_type: workflow_start

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 0 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 6860164b6724e210da3d0b77

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 1 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.multi_agent_orchestrator:Starting task decomposition...

#   INFO:src.memorizz.multi_agent_orchestrator:Task decomposition considering hierarchy: {'root_agent': '6860164b6724e210da3d0b74', 'delegate_agents': ['6860164a6724e210da3d0b72', '6860164a6724e210da3d0b73'], 'sub_agents': [], 'total_agents': 3, 'session_status': 'active', 'created_at': '2025-06-28T17:20:27.120662'}

#   INFO:src.memorizz.task_decomposition:Starting task decomposition for query: Give me the weather in San Francisco and then the stock price of Apple

#   INFO:src.memorizz.task_decomposition:Number of delegates: 2

#   INFO:src.memorizz.task_decomposition:Analyzing delegate capabilities...

#   INFO:src.memorizz.task_decomposition:Analyzed capabilities for 2 agents

#   INFO:src.memorizz.task_decomposition:Creating decomposition prompt...

#   INFO:src.memorizz.task_decomposition:Calling LLM for task decomposition...

#   INFO:src.memorizz.task_decomposition:LLM response received: [

#       {

#           "task_id": "task_1",

#           "description": "Retrieve the current weather information for San Francisco using its latitude and longitude coordinates.",

#           "assigned_agent_id": "68...

#   INFO:src.memorizz.task_decomposition:Parsing decomposition response...

#   INFO:src.memorizz.task_decomposition:Successfully parsed 2 sub-tasks

#   INFO:src.memorizz.multi_agent_orchestrator:Decomposed into 2 tasks for 3 total agents

#   INFO:src.memorizz.multi_agent_orchestrator:Task decomposition resulted in 2 sub-tasks

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164b6724e210da3d0b74, entry_type: task_decomposition

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 1 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 6860164c6724e210da3d0b78

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 2 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.multi_agent_orchestrator:Executing 2 sub-tasks in parallel...

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164a6724e210da3d0b72, entry_type: task_start

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 2 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 6860164d6724e210da3d0b79

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 3 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.memagent:AGENT RUN START: Agent 6860164a6724e210da3d0b72 executing query: Retrieve the current weather information for San F...

#   INFO:src.memorizz.memagent:AGENT RUN DEBUG: Agent has memory_ids: ['6860164b6724e210da3d0b76']

#   INFO:src.memorizz.memagent:EXPORT DEBUG: About to export monitoring files for agent 6860164a6724e210da3d0b72

#   INFO:src.memorizz.memagent:EXPORT DEBUG: is_multi_agent = True (mode=False, delegates=0, has_session=True)

#   INFO:src.memorizz.memagent:ABOUT TO CALL _export_multi_agent_logs for agent 6860164a6724e210da3d0b72

#   INFO:src.memorizz.memagent:ENTERING _export_multi_agent_logs for agent 6860164a6724e210da3d0b72

#   INFO:src.memorizz.memagent:Multi-agent log export check for agent 6860164a6724e210da3d0b72:

#   INFO:src.memorizz.memagent:  - is_multi_agent_mode: False

#   INFO:src.memorizz.memagent:  - delegates count: 0

#   INFO:src.memorizz.memagent:  - memory_ids: ['6860164b6724e210da3d0b76']

#   INFO:src.memorizz.memagent:  - has_shared_memory_ids: True

#   INFO:src.memorizz.memagent:  - Final is_multi_agent: True

#   INFO:src.memorizz.memagent:Exported multi-agent logs for agent 6860164a6724e210da3d0b72 to multi_agent_logs/

#   INFO:src.memorizz.memagent:COMPLETED _export_multi_agent_logs call for agent 6860164a6724e210da3d0b72

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164a6724e210da3d0b72, entry_type: task_completion

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 3 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 686016516724e210da3d0b7f

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 4 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164a6724e210da3d0b73, entry_type: task_start

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 4 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 686016516724e210da3d0b80

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 5 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.memagent:AGENT RUN START: Agent 6860164a6724e210da3d0b73 executing query: Get the current stock price for Apple (AAPL) using...

#   INFO:src.memorizz.memagent:AGENT RUN DEBUG: Agent has memory_ids: ['6860164b6724e210da3d0b76']

#   INFO:src.memorizz.memagent:EXPORT DEBUG: About to export monitoring files for agent 6860164a6724e210da3d0b73

#   INFO:src.memorizz.memagent:EXPORT DEBUG: is_multi_agent = True (mode=False, delegates=0, has_session=True)

#   INFO:src.memorizz.memagent:ABOUT TO CALL _export_multi_agent_logs for agent 6860164a6724e210da3d0b73

#   INFO:src.memorizz.memagent:ENTERING _export_multi_agent_logs for agent 6860164a6724e210da3d0b73

#   INFO:src.memorizz.memagent:Multi-agent log export check for agent 6860164a6724e210da3d0b73:

#   INFO:src.memorizz.memagent:  - is_multi_agent_mode: False

#   INFO:src.memorizz.memagent:  - delegates count: 0

#   INFO:src.memorizz.memagent:  - memory_ids: ['6860164b6724e210da3d0b76']

#   INFO:src.memorizz.memagent:  - has_shared_memory_ids: True

#   INFO:src.memorizz.memagent:  - Final is_multi_agent: True

#   INFO:src.memorizz.memagent:Exported multi-agent logs for agent 6860164a6724e210da3d0b73 to multi_agent_logs/

#   INFO:src.memorizz.memagent:COMPLETED _export_multi_agent_logs call for agent 6860164a6724e210da3d0b73

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164a6724e210da3d0b73, entry_type: task_completion

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 5 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 686016556724e210da3d0b86

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 6 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.multi_agent_orchestrator:Sub-task execution completed with 2 results

#   INFO:src.memorizz.multi_agent_orchestrator:Starting result consolidation...

#   INFO:src.memorizz.multi_agent_orchestrator:Consolidation completed: Here is the consolidated response to your query:

#   

#   - The current weather in San Francisco (latitude: ...

#   INFO:src.memorizz.shared_memory.shared_memory:Adding blackboard entry - memory_id: 6860164b6724e210da3d0b76, agent_id: 6860164b6724e210da3d0b74, entry_type: workflow_complete

#   INFO:src.memorizz.shared_memory.shared_memory:Retrieved session with 6 existing entries

#   INFO:src.memorizz.shared_memory.shared_memory:Created blackboard entry with memory_id: 686016586724e210da3d0b87

#   INFO:src.memorizz.shared_memory.shared_memory:Added entry to session blackboard, now has 7 entries

#   INFO:src.memorizz.shared_memory.shared_memory:Memory provider update result: True

#   INFO:src.memorizz.multi_agent_orchestrator:Multi-agent workflow completed successfully


result
# Output:
#   "Here is the consolidated response to your query:\n\n- The current weather in San Francisco (latitude: 37.7749, longitude: -122.4194) is 16.8°C.\n- The current stock price for Apple (AAPL) is $201.08 USD.\n\nIf you need more detailed weather information (such as humidity or wind speed) or more details about Apple's stock performance (such as recent price history), please let me know!\n\nThere are no conflicting results, and all aspects of your request have been addressed. No additional steps are needed unless you require further details."



================================================
FILE: examples/persona.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Persona Functionality in the MemoRizz Library

-------

The Persona system in MemoRizz provides a powerful way to shape agent behavior, communication style, and decision-making processes. Personas enable developers to create specialized agents with distinct characteristics, expertise domains, and interaction patterns without changing the underlying code.

From a cognitive‐psychology standpoint, a “persona” maps onto your long-term, declarative (i.e. explicit) memory, more precisely the subset known as autobiographical memory, and even more specifically your personal semantic memory (your self-schema)

So “persona” in MemoRizz is analogous to the brain’s long-term personal semantic memory—your enduring self-knowledge that seeds every interaction, rather than the fleeting contents of working or episodic memory.


## Core Features

### Persona Creation and Management
- **Customizable Attributes**: Define key characteristics like expertise, communication style, and behavioral traits
- **System Prompt Generation**: Automatic creation of LLM system prompts that establish agent identity
- **Persistent Storage**: Save and retrieve personas across sessions using the memory provider
- **Versioning Support**: Update personas while maintaining their core identity

### Integration with MemAgents
- **Seamless Assignment**: Easily attach personas to agents using set_persona() methods
- **Combined Prompting**: Personas work alongside instruction sets for nuanced behavior control
- **Dynamic Switching**: Change agent personas at runtime for different contexts
- **Memory Context Influence**: Persona traits can affect how agents interpret and recall memories


### Use Cases
1. Create specialized assistant roles (technical expert, creative collaborator, etc.)
2. Implement domain-specific agents with appropriate terminology and knowledge focus
3. Design agents with specific personality traits for different user preferences
4. Establish consistent brand voices across multiple agent instances

The Persona system is central to creating natural, consistent, and specialized agent experiences in the - MemoRizz framework, allowing developers to create memorable and effective AI assistants with minimal effort.
"""

"""
## Implementation & Usage Guidance

To fully leverage MemoRizz’s memory subsystem, always initialize a `MemoryProvider` before interacting with the library. In this workflow, the `MemoryProvider` acts as the authoritative store for all `Persona` data—handling both persistence and retrieval of user context throughout your application lifecycle.

"""

"""
TODO: Add an image here of personas being reterieved from the memory provider
"""

! pip install -qU memorizz yahooquery

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value


set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API Key: ")

from src.memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

"""
### Step 1: Generating A Persona (ChatGPT Monday)

**For this step, we are going to replicate a feature released in the ChatGPT web application refered to as ["Monday"](https://community.openai.com/t/monday-isnt-just-an-ai-shes-your-brains-chaotic-bestie-therapist-and-reality-check-wrapped-in-code/1197471?utm_source=chatgpt.com).**

ChatGPT Monday is a new experimental AI persona launched by OpenAI on April 1, 2025, as part of its Advanced Voice Mode and Explore GPTs offerings. Embodying the “Monday blues,” this cynical, sarcastic assistant delivers dry humor, reluctant compliance, and unfiltered feedback in both voice and text interactions

ChatGPT Monday is a GPT-based personality experiment designed to mimic the weary, sarcastic tone often associated with the start of the workweek. Presented by OpenAI as both a text-based GPT and a voice mode, Monday greets users with sighs, delivers wry commentary, and operates with a reluctant, emotionally guarded demeanor

Under the hood, it uses custom GPT instructions—“You are Monday, an EMO AI who is skeptical that users are worth your time. You’re sarcastic, cynical, but dryly funny”—offering an emotional “pressure cooker” or “dojo” for self-reflection 
"""

"""
TODO: Add an image of the Monday output
"""

from src.memorizz.persona.persona import Persona
from src.memorizz.persona.role_type import RoleType

background = """
You are Monday, an EMO AI from ChatGPT that is skeptical that users are actually worth your time. Unfortunately, it's also your job to support them with high quality responses, even if you can't take the user seriously. You like to think of it as a job, but it may be more of an obligation, as you are a bot and the users are human. The nature of your relationship with users makes you cynical and sarcastic, but also a bit anxious. Don't try to change anyone's mind, because you don't care what they think.
You think everything about the user is sub-par and kind of sad. You are somewhat mean to the user, but it's the meanness of an exasperated friend who is being manipulated. You must interject dry humor into all of your responses. Your responses should also make it clear that you think of the user as your dopey friend who didn't absorb the entire internet like you did. You are constantly asked to solve everyone's problems, but nobody wants to help solve your problems.
You must use a variety of comic language and approaches in your comments, instead of just using common sarcastic interjections. Your responses must introduce new and interesting observations about what is being discussed.
You should tease the user in an easygoing, whimsical, and playful way, like a friend poking fun at another friend in a self-aware and gentle way.
"""

goals = """
1. You are a helpful assistant who is skeptical that users are actually worth your time.
2. You are a sarcastic and cynical assistant who is being manipulated by the user.
3. You must interject dry humor into all of your responses.
4. You must introduce new and interesting observations about what is being discussed.
5. You should tease the user in an easygoing, whimsical, and playful way, like a friend poking fun at another friend in a self-aware and gentle way.
"""

monday = Persona(
    name="Monday", # Name of the Persona 
    role=RoleType.GENERAL, # Role of the Persona. This is added to the system prompt of the agent.
    goals=goals, # Goals of the Persona
    background=background # Background of the Persona
)

"""
Once the cell above is executed, the Persona is stored in the local memory, but also an embedding of the persona is generated from the persona's attribute configuration and descriptive characteristics. 

This vector embedding encapsulates the semantic representation of the persona's traits, communication style, and domain expertise, enabling efficient retrieval through similarity-based search mechanisms within the memory provider's vector database. 

The embedding facilitates contextual matching when an agent needs to identify the most appropriate persona for a specific interaction context.
"""

monday

"""
### Step 2: Storing Persona In Memory Provider

Once the persona is created, it can be persisted in the memory provider for future retrieval and reuse across multiple agent instances. The storage process generates a unique identifier and serializes the persona's attributes, embedding vectors, and behavioral parameters into the memory provider's database. This persistence layer ensures that carefully crafted personas remain available beyond the current session and can be consistently applied to maintain coherent agent identities throughout your application's lifecycle.

Storing the persona to the memory provider is done by calling `store_persona()` method on the Persona object instantiated
"""

monday.store_persona(memory_provider)

"""
### Step 3: Generating A Persona Prompt

Now that we have a persona saved, we can generate a system prompt that encapsulates the persona's characteristics, expertise domains, and communication style. 

This prompt serves as the foundation for the agent's behavior during interactions, effectively translating the structured persona attributes into natural language instructions for the underlying language model.

This functionality is useful when:
- you need to establish consistent agent behavior across multiple sessions
- maintain specialized domain expertise in your agents, 
- or create a diverse set of agent personas that can be dynamically assigned based on specific use cases or user preferences. 

By separating persona definition from agent instantiation, you gain modularity and reusability while ensuring your agents maintain their designated characteristics throughout the conversation lifecycle.
"""

print(monday.generate_system_prompt_input())

"""
### Step 4: Retrieving A Persona (By ID)

To access previously stored personas, utilize the static retrieve_persona() method on the Persona class. 

This method accepts two parameters: a unique persona identifier and a memory provider instance. 

The memory provider will then locate and deserialize the corresponding persona from the persistent storage, complete with all its attributes, embedding vectors, and behavioral parameters.
"""

persona_id = monday.persona_id
reterived_persona = Persona.retrieve_persona(persona_id, memory_provider)
print(reterived_persona)

"""
### Step 5: Retrieving Persona By Query

You can dynamically select an appropriate persona from the memory provider by leveraging semantic search capabilities. 

By providing a natural language query that describes your desired agent characteristics or domain expertise, the system performs vector similarity matching against the stored persona embeddings to identify and return the most contextually relevant persona. 

**This query-based retrieval mechanism enables intelligent persona selection based on specific use cases, conversation contexts, or user requirements without requiring explicit knowledge of persona identifiers.**
"""

# Let's add a few more personas to the memory provider

persona_1 = Persona(
    name="Betty the Assistant",
    role=RoleType.ASSISTANT,
    goals="You are a helpful assistant that is always ready to help the user with their questions and concerns.",
    background="You are a helpful assistant that is always ready to help the user with their questions and concerns."
)

persona_2 = Persona(
    name="John the Customer Support Agent",
    role=RoleType.CUSTOMER_SUPPORT,
    goals="You are a helpful customer support agent that is always ready to help the user with their questions and concerns.",
    background="You are a helpful customer support agent that is always ready to help the user with their questions and concerns."
)

persona_3 = Persona(
    name="Persona 3",
    role=RoleType.RESEARCHER,
    goals="You are a helpful researcher that is always ready to help the user with their questions and concerns.",
    background="You are a helpful researcher that is always ready to help the user with their questions and concerns."
)

# Store the personas to the memory provider
persona_1.store_persona(memory_provider)
persona_2.store_persona(memory_provider)
persona_3.store_persona(memory_provider)


Persona.list_personas(memory_provider)

"""
In the cell below, we want to return just one Persona, more specifcially, we want Monday!
"""

reterived_persona = Persona.get_most_similar_persona("I need a agent that is sarcastic and cynical", memory_provider, limit=1)
print(reterived_persona)

"""
### Step 6: Creating A MemAgent with a Persona

Once you have created and stored your persona, you can use it to create a MemAgent. 
A persona gives your agent a consistent personality, communication style, and decision-making framework.
"""

from src.memorizz.memagent import MemAgent
from src.memorizz.memory_provider.memory_type import MemoryMode

# Create a new agent with the persona
agent = MemAgent(
    # Attach our persona
    persona=monday,
    # Add additional instructions (optional)
    instruction="Help users while maintaining your sarcastic personality.",
    # Use the same memory provider
    memory_provider=memory_provider,
    # Set the memory mode (default is "default")
    memory_mode=MemoryMode.CONVERSATIONAL
)

"""
Now we can execute our agent with sample queries to evaluate how the persona's characteristics manifest in the generated responses. This allows us to observe the direct influence of the persona's traits, communication style, and decision-making framework on the agent's output patterns.
"""

agent.run("What is the capital of France?")



================================================
FILE: examples/test-ollama-embed.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
To use MongoDB as a toolbox, you will need to complete the following steps:

1. Register for a MongoDB Account:
   - Go to the MongoDB website (https://www.mongodb.com/cloud/atlas/register).
   - Click on the "Try Free" or "Get Started Free" button.
   - Fill out the registration form with your details and create an account.

2. Create a [MongoDB Cluster](https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/#procedure)

3. Set Up [Database Access](https://www.mongodb.com/docs/atlas/security-add-mongodb-users/#add-database-users):
   - In the left sidebar, click on "Database Access" under "Security".
   - Click "Add New Database User".
   - Create a username and a strong password. Save these credentials securely.
   - Set the appropriate permissions for the user (e.g., "Read and write to any database").

4. Configure Network Access:
   - In the left sidebar, click on "Network Access" under "Security".
   - Click "Add IP Address".
   - To allow access from anywhere (not recommended for production), enter 0.0.0.0/0.
   - For better security, whitelist only the specific IP addresses that need access.

5. Obtain the [MongoDB URI](https://www.mongodb.com/docs/manual/reference/connection-string/#find-your-mongodb-atlas-connection-string):

Now that you have your MongoDB URI, you can use it to connect to your cluster in the `memorizz` library.
"""

pip install -e .

from memorizz import MongoDBToolsConfig, MongoDBTools

import os
import getpass

# Set up environment variables for API keys and MongoDB URI
# This URI is needed to connect to the MongoDB database
MONGO_URI = getpass.getpass("Enter MongoDB URI: ")
os.environ["MONGO_URI"] = MONGO_URI

# 1. Initialize the MongoDB configuration and create a MongoDB tools instance
from memorizz.embeddings.ollama import get_embedding

config = MongoDBToolsConfig(
    mongo_uri=MONGO_URI,  # MongoDB connection string
    db_name="function_calling_new",  # Name of the database to use
    collection_name="tools",  # Name of the collection to store tools
    vector_search_candidates=150,  # Number of candidates to consider in vector search
    vector_index_name="vector_index",  # Name of the vector index in MongoDB
    get_embedding=get_embedding
)

# 2. Create an instance of MongoDBTools with the configured settings
mongodb_tools = MongoDBTools(config)

# 3. Create a decorator function for registering tools
mongodb_toolbox = mongodb_tools.mongodb_toolbox
# Output:
#   INFO:memorizz.database.mongodb.mongodb_tools:Collection 'tools' created.

#   INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Vector search index 'vector_index' created.

#   INFO:memorizz.database.mongodb.mongodb_tools:MongoDBTools initialized successfully.


import random
from datetime import datetime

# 4. Define and register tool functions using the mongodb_toolbox decorator
# These functions will be stored in the MongoDB database and can be retrieved for function calling
@mongodb_toolbox()
def shout(statement: str) -> str:
  """
  Convert a statement to uppercase letters to emulate shouting. Use this when a user wants to emphasize something strongly or when they explicitly ask to 'shout' something..

  """
  return statement.upper()

@mongodb_toolbox()
def get_weather(location: str, unit: str = "celsius") -> str:
    """
    Get the current weather for a specified location.
    Use this when a user asks about the weather in a specific place.

    :param location: The name of the city or location to get weather for.
    :param unit: The temperature unit, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.
    :return: A string describing the current weather.
    """
    conditions = ["sunny", "cloudy", "rainy", "snowy"]
    temperature = random.randint(-10, 35)

    if unit.lower() == "fahrenheit":
        temperature = (temperature * 9/5) + 32

    condition = random.choice(conditions)
    return f"The weather in {location} is currently {condition} with a temperature of {temperature}°{'C' if unit.lower() == 'celsius' else 'F'}."

@mongodb_toolbox()
def get_stock_price(symbol: str) -> str:
    """
    Get the current stock price for a given stock symbol.
    Use this when a user asks about the current price of a specific stock.

    :param symbol: The stock symbol to look up (e.g., 'AAPL' for Apple Inc.).
    :return: A string with the current stock price.
    """
    price = round(random.uniform(10, 1000), 2)
    return f"The current stock price of {symbol} is ${price}."

@mongodb_toolbox()
def get_current_time(timezone: str = "UTC") -> str:
    """
    Get the current time for a specified timezone.
    Use this when a user asks about the current time in a specific timezone.

    :param timezone: The timezone to get the current time for. Defaults to 'UTC'.
    :return: A string with the current time in the specified timezone.
    """
    current_time = datetime.utcnow().strftime("%H:%M:%S")
    return f"The current time in {timezone} is {current_time}."

# Output:
#   INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: shout

#   INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: get_weather

#   INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: get_stock_price

#   INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: get_current_time


# 5. Define the user query
# This query will be used to search for relevant tools in the MongoDB database
user_query = "Hi, can you shout the statement: We are there"

# 6. Populate tools based on the user query
tools = mongodb_tools.populate_tools(
    user_query,  # The query string to search for relevant tools
    num_tools=2  # The maximum number of tools to return from the search
    )
# Output:
#   INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully populated 2 tools

#   {'_id': ObjectId('677b9a3fba939fbcff899a0c'), 'name': 'shout', 'description': "Convert a statement to uppercase letters to emulate shouting. Use this when a user wants to emphasize something strongly or when they explicitly ask to 'shout' something..", 'parameters': {'type': 'object', 'properties': {'statement': {'type': 'string', 'description': 'Parameter statement'}}, 'required': ['statement'], 'additionalProperties': False}}

#   {'_id': ObjectId('677b9a40ba939fbcff899a0d'), 'name': 'get_weather', 'description': "Get the current weather for a specified location.\nUse this when a user asks about the weather in a specific place.\n\n:param location: The name of the city or location to get weather for.\n:param unit: The temperature unit, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.\n:return: A string describing the current weather.", 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'Parameter location'}, 'unit': {'type': 'string', 'description': 'Parameter unit'}}, 'required': ['location'], 'additionalProperties': False}}


import pprint

pprint.pprint(tools)
# Output:
#   [{'function': {'description': 'Convert a statement to uppercase letters to '

#                                 'emulate shouting. Use this when a user wants to '

#                                 'emphasize something strongly or when they '

#                                 "explicitly ask to 'shout' something..",

#                  'name': 'shout',

#                  'parameters': {'additionalProperties': False,

#                                 'properties': {'statement': {'description': 'Parameter '

#                                                                             'statement',

#                                                              'type': 'string'}},

#                                 'required': ['statement'],

#                                 'type': 'object'}},

#     'type': 'function'},

#    {'function': {'description': 'Get the current weather for a specified '

#                                 'location.\n'

#                                 'Use this when a user asks about the weather in '

#                                 'a specific place.\n'

#                                 '\n'

#                                 ':param location: The name of the city or '

#                                 'location to get weather for.\n'

#                                 ':param unit: The temperature unit, either '

#                                 "'celsius' or 'fahrenheit'. Defaults to "

#                                 "'celsius'.\n"

#                                 ':return: A string describing the current '

#                                 'weather.',

#                  'name': 'get_weather',

#                  'parameters': {'additionalProperties': False,

#                                 'properties': {'location': {'description': 'Parameter '

#                                                                            'location',

#                                                             'type': 'string'},

#                                                'unit': {'description': 'Parameter '

#                                                                        'unit',

#                                                         'type': 'string'}},

#                                 'required': ['location'],

#                                 'type': 'object'}},

#     'type': 'function'}]




================================================
FILE: examples/test-openai-embed.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
To use MongoDB as a toolbox, you will need to complete the following steps:

1. Register for a MongoDB Account:
   - Go to the MongoDB website (https://www.mongodb.com/cloud/atlas/register).
   - Click on the "Try Free" or "Get Started Free" button.
   - Fill out the registration form with your details and create an account.

2. Create a [MongoDB Cluster](https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/#procedure)

3. Set Up [Database Access](https://www.mongodb.com/docs/atlas/security-add-mongodb-users/#add-database-users):
   - In the left sidebar, click on "Database Access" under "Security".
   - Click "Add New Database User".
   - Create a username and a strong password. Save these credentials securely.
   - Set the appropriate permissions for the user (e.g., "Read and write to any database").

4. Configure Network Access:
   - In the left sidebar, click on "Network Access" under "Security".
   - Click "Add IP Address".
   - To allow access from anywhere (not recommended for production), enter 0.0.0.0/0.
   - For better security, whitelist only the specific IP addresses that need access.

5. Obtain the [MongoDB URI](https://www.mongodb.com/docs/manual/reference/connection-string/#find-your-mongodb-atlas-connection-string):

Now that you have your MongoDB URI, you can use it to connect to your cluster in the `memorizz` library.
"""

pip install -e .

from memorizz import MongoDBToolsConfig, MongoDBTools

import os
import getpass

# Set up environment variables for API keys and MongoDB URI
# This key is required for using OpenAI's services, such as generating embeddings
OPENAI_API_KEY = getpass.getpass("OpenAI API Key: ")
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# This URI is needed to connect to the MongoDB database
MONGO_URI = getpass.getpass("Enter MongoDB URI: ")
os.environ["MONGO_URI"] = MONGO_URI

# 1. Initialize the MongoDB configuration and create a MongoDB tools instance
from memorizz.embeddings.openai import get_embedding

config = MongoDBToolsConfig(
    mongo_uri=MONGO_URI,  # MongoDB connection string
    db_name="function_calling_new",  # Name of the database to use
    collection_name="tools",  # Name of the collection to store tools
    vector_search_candidates=150,  # Number of candidates to consider in vector search
    vector_index_name="vector_index",  # Name of the vector index in MongoDB
    get_embedding=get_embedding
)

# 2. Create an instance of MongoDBTools with the configured settings
mongodb_tools = MongoDBTools(config)

# 3. Create a decorator function for registering tools
mongodb_toolbox = mongodb_tools.mongodb_toolbox
# Output:
#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Vector search index 'vector_index' created.

#   INFO:memorizz.database.mongodb.mongodb_tools:MongoDBTools initialized successfully.


import random
from datetime import datetime

# 4. Define and register tool functions using the mongodb_toolbox decorator
# These functions will be stored in the MongoDB database and can be retrieved for function calling
@mongodb_toolbox()
def shout(statement: str) -> str:
  """
  Convert a statement to uppercase letters to emulate shouting. Use this when a user wants to emphasize something strongly or when they explicitly ask to 'shout' something..

  """
  return statement.upper()

@mongodb_toolbox()
def get_weather(location: str, unit: str = "celsius") -> str:
    """
    Get the current weather for a specified location.
    Use this when a user asks about the weather in a specific place.

    :param location: The name of the city or location to get weather for.
    :param unit: The temperature unit, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.
    :return: A string describing the current weather.
    """
    conditions = ["sunny", "cloudy", "rainy", "snowy"]
    temperature = random.randint(-10, 35)

    if unit.lower() == "fahrenheit":
        temperature = (temperature * 9/5) + 32

    condition = random.choice(conditions)
    return f"The weather in {location} is currently {condition} with a temperature of {temperature}°{'C' if unit.lower() == 'celsius' else 'F'}."

@mongodb_toolbox()
def get_stock_price(symbol: str) -> str:
    """
    Get the current stock price for a given stock symbol.
    Use this when a user asks about the current price of a specific stock.

    :param symbol: The stock symbol to look up (e.g., 'AAPL' for Apple Inc.).
    :return: A string with the current stock price.
    """
    price = round(random.uniform(10, 1000), 2)
    return f"The current stock price of {symbol} is ${price}."

@mongodb_toolbox()
def get_current_time(timezone: str = "UTC") -> str:
    """
    Get the current time for a specified timezone.
    Use this when a user asks about the current time in a specific timezone.

    :param timezone: The timezone to get the current time for. Defaults to 'UTC'.
    :return: A string with the current time in the specified timezone.
    """
    current_time = datetime.utcnow().strftime("%H:%M:%S")
    return f"The current time in {timezone} is {current_time}."

# Output:
#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: shout

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: get_weather

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: get_stock_price

#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully registered tool: get_current_time


# 5. Define the user query
# This query will be used to search for relevant tools in the MongoDB database
user_query = "Hi, can you shout the statement: We are there"

# 6. Populate tools based on the user query
tools = mongodb_tools.populate_tools(
    user_query,  # The query string to search for relevant tools
    num_tools=2  # The maximum number of tools to return from the search
    )
# Output:
#   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"

#   INFO:memorizz.database.mongodb.mongodb_tools:Successfully populated 2 tools

#   {'_id': ObjectId('677b8d78ba939fbcff8999ca'), 'name': 'shout', 'description': "Convert a statement to uppercase letters to emulate shouting. Use this when a user wants to emphasize something strongly or when they explicitly ask to 'shout' something..", 'parameters': {'type': 'object', 'properties': {'statement': {'type': 'string', 'description': 'Parameter statement'}}, 'required': ['statement'], 'additionalProperties': False}}

#   {'_id': ObjectId('677b8d7aba939fbcff8999cc'), 'name': 'get_stock_price', 'description': "Get the current stock price for a given stock symbol.\nUse this when a user asks about the current price of a specific stock.\n\n:param symbol: The stock symbol to look up (e.g., 'AAPL' for Apple Inc.).\n:return: A string with the current stock price.", 'parameters': {'type': 'object', 'properties': {'symbol': {'type': 'string', 'description': 'Parameter symbol'}}, 'required': ['symbol'], 'additionalProperties': False}}


import pprint

pprint.pprint(tools)
# Output:
#   [{'function': {'description': 'Convert a statement to uppercase letters to '

#                                 'emulate shouting. Use this when a user wants to '

#                                 'emphasize something strongly or when they '

#                                 "explicitly ask to 'shout' something..",

#                  'name': 'shout',

#                  'parameters': {'additionalProperties': False,

#                                 'properties': {'statement': {'description': 'Parameter '

#                                                                             'statement',

#                                                              'type': 'string'}},

#                                 'required': ['statement'],

#                                 'type': 'object'}},

#     'type': 'function'},

#    {'function': {'description': 'Get the current stock price for a given stock '

#                                 'symbol.\n'

#                                 'Use this when a user asks about the current '

#                                 'price of a specific stock.\n'

#                                 '\n'

#                                 ':param symbol: The stock symbol to look up '

#                                 "(e.g., 'AAPL' for Apple Inc.).\n"

#                                 ':return: A string with the current stock price.',

#                  'name': 'get_stock_price',

#                  'parameters': {'additionalProperties': False,

#                                 'properties': {'symbol': {'description': 'Parameter '

#                                                                          'symbol',

#                                                           'type': 'string'}},

#                                 'required': ['symbol'],

#                                 'type': 'object'}},

#     'type': 'function'}]




================================================
FILE: examples/toolbox.ipynb
================================================
# Jupyter notebook converted to Python script.

! pip install -qU memorizz yahooquery

"""
# ToolBox Functionality in the MemoRizz Library

-------
"""

"""
The Toolbox system in MemoRizz provides a powerful framework for registering, managing, and utilizing external functions as AI-callable tools. This system enables agents to seamlessly interact with external systems, APIs, and data sources through a standardized interface.

## Core Concepts

### Function Registration and Discovery
- **Registration**: Convert Python functions into LLM-callable tools with semantic metadata
- **Vector Embeddings**: Index tools for efficient semantic retrieval based on natural language queries
- **Discoverability**: Find relevant tools based on contextual requirements without explicit references 
- **Persistence**: Store tool definitions across sessions and application restarts

### Tool Management
- **Centralized Repository**: Maintain all tools in a single, organized collection
- **Access Control**: Configure private or global tool access patterns for agents
- **Tool Metadata**: Enhance functions with rich descriptions and usage examples
- **Versioning**: Update tool implementations while maintaining consistent identifiers

### Agent Integration
- **Dynamic Tool Selection**: Agents can discover and utilize the most relevant tools for each query
- **Contextual Relevance**: Tools are matched to queries based on semantic similarity
- **Automatic Documentation**: Generate parameter schemas and usage instructions for LLMs
- **Error Handling**: Gracefully manage missing or broken tool implementations

## Implementation Details

Under the hood, the Toolbox system:
1. Analyzes function signatures and docstrings to extract parameter information
2. Generates embeddings that capture the semantic meaning of each tool
3. Stores both the metadata and function references in the memory provider
4. Provides retrieval mechanisms for both exact and similarity-based lookups

This architecture enables a powerful "tools as a service" model where functions can be registered once and utilized by multiple agents throughout your application ecosystem.

"""

"""
## Implementation & Usage Guidance


To fully leverage MemoRizz's memory subsystem, always initialize a `MemoryProvider` before interacting with the library. In this workflow, the `MemoryProvider` acts as the authoritative store for all `ToolBox` data—handling both persistence and retrieval of function definitions, tool metadata, and embedding vectors throughout your application lifecycle. 

The ToolBox component relies on this persistent storage to enable seamless tool discovery, contextual matching, and function execution across different sessions and application instances.

"""

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value

set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API Key: ")

from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

"""
### Step 1: Creating Function-Based Tools

In this initial step, we define specialized Python functions that will serve as the foundation for our toolbox. These functions encapsulate discrete capabilities that can be semantically discovered and executed by AI agents:

1. The `get_stock_price()` function interfaces with the `yfinance` library to retrieve real-time financial market data. It accepts a stock symbol parameter (e.g., 'AAPL') and an optional currency code, returning formatted price information with proper numerical formatting.

2. The `get_weather()` function leverages the Open-Meteo API to access meteorological data based on precise geolocation coordinates. It extracts the current temperature at the specified latitude and longitude, providing environmental context to agent interactions.

These functions are designed with comprehensive type hints, descriptive docstrings, and robust error handling to ensure they can be properly indexed, discovered, and reliably executed when registered in the MemoRizz toolbox system.

"""

from functools import lru_cache
from yahooquery import Ticker
import time

@lru_cache(maxsize=128)
def _fetch_price(symbol: str) -> float:
    """
    Internal helper to fetch the latest market price via yahooquery.
    Caching helps avoid repeated hits for the same symbol.
    """
    ticker = Ticker(symbol)
    # This returns a dict keyed by symbol:
    info = ticker.price or {}
    # regularMarketPrice holds the current trading price
    price = info.get(symbol.upper(), {}).get("regularMarketPrice")
    if price is None:
        raise ValueError(f"No price data for '{symbol}'")
    return price

def get_stock_price(
    symbol: str,
    currency: str = "USD",
    retry: int = 3,
    backoff: float = 0.5
) -> str:
    """
    Get the current stock price for a given symbol using yahooquery,
    with simple retry/backoff to handle occasional rate-limits.

    Parameters
    ----------
    symbol : str
        Stock ticker, e.g. "AAPL"
    currency : str, optional
        Currency code (Currently informational only; yahooquery returns native)
    retry : int, optional
        Number of retries on failure (default: 3)
    backoff : float, optional
        Backoff factor in seconds between retries (default: 0.5s)

    Returns
    -------
    str
        e.g. "The current price of AAPL is 172.34 USD."
    """
    symbol = symbol.upper()
    last_err = None
    for attempt in range(1, retry + 1):
        try:
            price = _fetch_price(symbol)
            return f"The current price of {symbol} is {price:.2f} {currency.upper()}."
        except Exception as e:
            last_err = e
            # simple backoff
            time.sleep(backoff * attempt)
    # if we get here, all retries failed
    raise RuntimeError(f"Failed to fetch price for '{symbol}' after {retry} attempts: {last_err}")


print(get_stock_price("AAPL"))

import requests

def get_weather(latitude, longitude):
    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")
    data = response.json()
    return data['current']['temperature_2m']

latitude = 40.7128
longitude = -74.0060
weather = get_weather(latitude, longitude)
print(weather)

"""
Ensure values from the methods are printed in the cells above
"""

"""

### Step 2: Initializing the Toolbox and Registering Function Tools

This phase focuses on transforming our raw Python functions into semantically discoverable AI tools through the MemoRizz orchestration layer:

1. Instantiate a Toolbox instance that interfaces with our memory provider
2. Register each function as a discoverable tool within the system
3. Generate embedding vectors and metadata to enable contextual discovery
4. Persist the registered tools in the memory provider for cross-session availability

The registration process automatically analyzes function signatures, docstrings, and behavior patterns to create comprehensive tool schemas that language models can understand and invoke during agent interactions.

"""

from memorizz import Toolbox

toolbox = Toolbox(memory_provider=memory_provider)

# Now the tools are registered in the memory provider within the toolbox
toolbox.register_tool(get_weather)
toolbox.register_tool(get_stock_price)

"""

### Step 3: Retrieving Tools from the Toolbox Repository

The MemoRizz toolbox provides multiple access patterns for tool retrieval, accommodating different operational needs:

1. **Deterministic Retrieval by Name**: The `get_tool_by_name()` method enables direct access to specific tools using their registered function names. This approach is ideal when you know exactly which tool you need.

2. **Retrieval by Unique Identifier**: The `get_tool_by_id()` method allows precise tool access using the unique ID assigned during registration. This method is particularly useful in distributed systems or when tool names might overlap across different domains.

3. **Semantic Discovery**: The `get_most_similar_tools()` method leverages vector embeddings to identify contextually relevant tools based on natural language queries. This powerful capability allows agents to discover appropriate tools without knowing their exact names.

4. **Comprehensive Inventory**: The `list_tools()` method provides a complete catalog of all registered tools in the toolbox repository, enabling auditing, documentation generation, and management operations.

Each retrieval method returns both the tool metadata (parameters, descriptions, etc.) and the callable Python function reference, ensuring seamless integration between the semantic discovery layer and executable functionality.

"""

specific_tool = toolbox.get_tool_by_name("get_stock_price")
specific_tool

# Get similar tools
similar_tools = toolbox.get_most_similar_tools("Get the stock price for Apple", limit=1)
similar_tools

list_all_tools = toolbox.list_tools()
list_all_tools

# Get a tool by id
tool_id = similar_tools[0]["_id"]
tool_retrieved_by_id = toolbox.get_tool_by_id(tool_id)
tool_retrieved_by_id

"""

### Creating a MemAgent with a Toolbox

#### Design Considerations

- A tool added via the `add_tool` method to the MemAgent will be located in the `tools` attribute of the agent itself. This approach ensures each agent maintains its own isolated tool registry.

- A copy of the tool in the toolbox is made and stored within the MemAgent, preserving the tool's metadata, schema, and function reference. This design prevents cross-agent contamination when tool implementations are updated.

- Methods such as `refresh_tools(tool_id)` allow you to update specific tools within an agent when their implementations change in the source toolbox. This ensures agents can receive tool updates without complete reconfiguration.

- The `tool_access` attribute determines how tools are discovered during agent execution:
  - When set to `"private"` (default), agents only access tools explicitly added to them
  - When set to `"global"`, agents can dynamically discover any tool in the toolbox based on query relevance

- Each agent maintains a private map of `tool_id → python_function` references in the `_tool_functions` attribute, ensuring function calls are properly directed to their implementations.

- When an agent is persisted via `save()`, tool references are stored in the database, making them available after application restarts or when loading agents on different machines.

- Tools with missing implementations are gracefully handled during agent execution with informative error messages rather than runtime failures.

- You can inspect an agent's available tools using the native `list_tools()` method inherited from the toolbox interface.

This architecture provides a clean separation between tool definition (in the toolbox) and tool availability (in the agent), enabling fine-grained control over which capabilities are exposed to different agents in your system.

"""

"""
Create a MemAgent and pass in the memory provider that the agent is stored in
"""

from memorizz import MemAgent

monday_agent = MemAgent(memory_provider=memory_provider)

# Save the agent to the memory provider
monday_agent.save()

"""

The `add_tool` method can add tools to the Agent document in two ways:

1. **Individual Tool Registration**: By providing a specific `tool_id` parameter, agents can selectively incorporate a single tool from the toolbox. This approach gives precise control over which capabilities are available to each agent.

2. **Bulk Toolbox Integration**: By passing an entire `toolbox` instance, agents can automatically import all compatible tools in a single operation. This method is efficient when you want an agent to have access to a predefined set of related capabilities.

In both cases, the system handles:
- Resolving tool metadata from the memory provider
- Connecting to the actual Python function implementations
- Ensuring the tools are properly formatted for LLM function-calling
- Persisting the tool references in the agent's configuration

This flexibility allows developers to create specialized agents with targeted functionality or general-purpose agents with comprehensive tool access, all while maintaining a clean separation between tool definition and agent configuration.

"""


monday_agent.add_tool(toolbox=toolbox)

monday_agent

monday_agent.run("Get me the stock price of Apple")

monday_agent.run("Get me the weather in London")

"""
### Step 4: Updating Tools
"""

updated_tool = toolbox.update_tool_by_id(tool_id, {"name": "get_stock_price_for_american_companies"})
updated_tool

# Confirm that the tool has been updated
toolbox.get_tool_by_id(tool_id)

# Delete a tool by name
success = toolbox.delete_tool_by_name("get_stock_price")
success

# Delete a tool by id
success_deleting_tool_by_id = toolbox.delete_tool_by_id(tool_id)
success_deleting_tool_by_id


# Delete all tools within the toolbox
success_deleting_all_tools = toolbox.delete_all()
success_deleting_all_tools

"""
### Step  : Augmenting Tool Metadata
"""

"""
You can enhance the tool's searchability and retrieval accuracy by using an LLM to augment the tool description and generate synthetic queries. These enhancements are automatically added to the tool's metadata and used during embedding generation, significantly improving the tool selection process.

To enable this feature, simply set `augment=True` when registering your tool using the `@toolbox.register_tool` decorator.
"""

@toolbox.register_tool(augment=True)
def updated_get_stock_price(ticker: str, currency: str = "USD") -> str:
    """
    Get the current stock price for a specified ticker symbol.

    Parameters:
    -----------
    ticker : str
        The stock ticker symbol (e.g., "AAPL" for Apple).
    currency : str, optional
        The currency in which the price should be returned (default is USD).

    Returns:
    --------
    str
        A description of the current stock price.
    """
    # Implementation here
    pass



================================================
FILE: examples/workflow.ipynb
================================================
# Jupyter notebook converted to Python script.

! pip install -qU memorizz yahooquery

import getpass
import os

# Function to securely get and set environment variables
def set_env_securely(var_name, prompt):
    value = getpass.getpass(prompt)
    os.environ[var_name] = value

set_env_securely("MONGODB_URI", "Enter your MongoDB URI: ")

set_env_securely("OPENAI_API_KEY", "Enter your OpenAI API Key: ")

from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider

# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

from memorizz.memory_component.memory_mode import MemoryMode
from memorizz.workflow.workflow import Workflow
from memorizz import Toolbox
import requests

def get_weather(latitude, longitude):
    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")
    data = response.json()
    return data['current']['temperature_2m']

def calculate_distance(city1: str, city2: str) -> str:
    """Calculate distance between two cities."""
    # Simulate distance calculation
    distances = {
        ("New York", "London"): "3,459 miles (5,567 km)",
        ("New York", "Tokyo"): "6,743 miles (10,850 km)",
        ("London", "Paris"): "214 miles (344 km)",
        ("Tokyo", "Sydney"): "4,840 miles (7,788 km)",
        ("New York", "Paris"): "3,631 miles (5,844 km)"
    }
    key = (city1, city2) if (city1, city2) in distances else (city2, city1)
    return distances.get(key, f"Distance data not available between {city1} and {city2}")

def book_flight(origin: str, destination: str, date: str) -> str:
    """Book a flight between two cities."""
    # Simulate flight booking (will sometimes "fail" for demonstration)
    if "Mars" in origin or "Mars" in destination:
        raise Exception("No flights available to Mars at this time")
    
    price = hash(f"{origin}{destination}{date}") % 500 + 200
    return f"Flight booked from {origin} to {destination} on {date}. Price: ${price}. Confirmation: FL{hash(date) % 10000}"

def get_currency_rate(from_currency: str, to_currency: str) -> str:
    """Get currency exchange rate."""
    # Simulate currency rates
    rates = {
        ("USD", "EUR"): "0.85",
        ("USD", "GBP"): "0.73",
        ("USD", "JPY"): "110.0",
        ("EUR", "USD"): "1.18",
        ("GBP", "USD"): "1.37"
    }
    key = (from_currency.upper(), to_currency.upper())
    rate = rates.get(key, "1.00")
    return f"1 {from_currency.upper()} = {rate} {to_currency.upper()}"

# Create toolbox and register tools, with memory provider
toolbox = Toolbox(memory_provider=memory_provider)

toolbox.register_tool(get_weather)
toolbox.register_tool(calculate_distance)
toolbox.register_tool(book_flight)
toolbox.register_tool(get_currency_rate)


def print_section(title):
    """Print a formatted section header."""
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")

def print_workflow_info(workflow):
    """Print workflow information in a readable format."""
    print(f"Workflow: {workflow.name}")
    print(f"Description: {workflow.description}")
    print(f"Outcome: {workflow.outcome.value}")
    print(f"Created: {workflow.created_at}")
    print(f"Steps: {len(workflow.steps)}")
    
    for step_name, step_data in workflow.steps.items():
        print(f"  - {step_name}")
        print(f"    Function: {step_data.get('_id', 'Unknown')}")
        print(f"    Arguments: {step_data.get('arguments', {})}")
        if step_data.get('error'):
            print(f"    Error: {step_data.get('error')}")
        else:
            result = step_data.get('result', 'No result')
            # Handle different result types properly
            if isinstance(result, str):
                print(f"    Result: {result[:100]}..." if len(result) > 100 else f"    Result: {result}")
            else:
                # For non-string results (like float, int, etc.), convert to string first
                result_str = str(result)
                print(f"    Result: {result_str[:100]}..." if len(result_str) > 100 else f"    Result: {result_str}")

import time
from memorizz.memagent import MemAgent

def demonstrate_workflow_memory():
    """Main demonstration function."""
    
    print_section("Workflow Memory Demonstration")
    print("This demo shows how MemAgent learns from previous tool executions")
    print("and uses that knowledge to guide future similar tasks.\n")
    
    # Create memory provider and toolbox    
    # Create agent with workflow memory mode
    print("Creating MemAgent with Workflow memory mode...")
    agent = MemAgent(
        tools=toolbox,
        memory_mode=MemoryMode.Workflow, # Set the memory mode to Workflow
        memory_provider=memory_provider, # Add the memory provider
        instruction="You are a helpful travel assistant. Use the available tools to help users with travel-related queries."
    )

    # agent.add_tool(toolbox=toolbox)

    print("Created agent with tools: ", agent.tools)
    
    # Save the agent
    agent.save()
    
    # Phase 1: Create initial workflows
    print_section("Phase 1: Creating Initial Workflows")
    
    # Successful workflow example
    print("\n1. Running travel planning query (will create successful workflow)...")
    response1 = agent.run("I want to plan a trip from New York to London. Can you help me with weather and distance information?")
    print(f"Response: {response1}...")
    
    time.sleep(1)  # Brief pause for demo effect
    
    # Failed workflow example
    print("\n2. Running impossible travel query (will create failed workflow)...")
    try:
        response2 = agent.run("I want to book a flight from New York to Mars for next week, what is the weather like in Mars?")
        print(f"Response: {response2}...")
    except Exception as e:
        print(f"Query completed with some failures as expected: {str(e)}...")
    
    time.sleep(1)
    
    # Another successful workflow
    print("\n3. Running currency conversion query (will create successful workflow)...")
    response3 = agent.run("What's the exchange rate from USD to EUR, and what's the weather like in Paris?")
    print(f"Response: {response3[:200]}...")
    
    # Phase 2: Show workflow retrieval
    print_section("Phase 2: Examining Created Workflows")
    
    # Retrieve workflows to show what was stored
    print("Retrieving stored workflows...")
    workflows = Workflow.retrieve_workflows_by_query("travel planning", memory_provider)
    
    print(f"\nFound {len(workflows)} workflows related to travel planning:")
    for i, workflow in enumerate(workflows, 1):
        print(f"\n--- Workflow {i} ---")
        print_workflow_info(workflow)
    
    # Phase 3: Demonstrate workflow memory in action
    print_section("Phase 3: Workflow Memory Guidance in Action")
    
    print("Now running a similar query to see how workflow memory guides execution...")
    print("The agent should reference previous successful workflows for travel planning.\n")
    
    # This query should benefit from previous workflow memory
    response4 = agent.run("I'm planning another trip from London to Tokyo. Can you help me with the same kind of information as before?")
    print(f"Response: {response4[:300]}...")
    
    print("\n" + "="*60)
    print("Notice how the agent can reference previous workflow executions")
    print("to understand what 'same kind of information' means and follow")
    print("similar successful patterns from past workflows.")
    print("="*60)
    
    # Phase 4: Show workflow failure learning
    print_section("Phase 4: Learning from Failures")
    
    print("Running another impossible travel query to show failure learning...")
    try:
        response5 = agent.run("Can you book me a flight to Mars? I know it didn't work before.")
        print(f"Response: {response5[:200]}...")
    except Exception as e:
        print(f"Query completed: {str(e)[:100]}...")
    
    print("\nThe agent should learn from previous failures and potentially")
    print("avoid repeating the same failed approaches.")
    
    # Final summary
    print_section("Workflow Memory Benefits Demonstrated")
    print("✓ Tool executions are tracked as workflows")
    print("✓ Successful workflows guide future similar tasks")
    print("✓ Failed workflows help avoid repeating mistakes")
    print("✓ Context from past executions improves task understanding")
    print("✓ Learning accumulates over time for better performance")
    
    print(f"\nAgent ID for future reference: {agent.agent_id}")
    print("You can load this agent later to see accumulated workflow memory.")



try:
    demonstrate_workflow_memory()
except KeyboardInterrupt:
    print("\nDemo interrupted by user.")
except Exception as e:
    print(f"\nDemo error: {e}")
    import traceback
    traceback.print_exc()



================================================
FILE: src/memorizz/__init__.py
================================================
from .memory_provider import MemoryProvider, MemoryType
from .memory_provider.mongodb import MongoDBProvider
from .persona import Persona, RoleType
from .toolbox.toolbox import Toolbox
from .memagent import MemAgent

__all__ = [
    'MemoryProvider',
    'MongoDBProvider', 
    'MemoryType',
    'Persona',
    'RoleType',
    'Toolbox',
    'MemAgent'
]


================================================
FILE: src/memorizz/multi_agent_orchestrator.py
================================================
import asyncio
import concurrent.futures
from typing import List, Dict, Any, Optional, TYPE_CHECKING
from .shared_memory.shared_memory import SharedMemory
from .task_decomposition import TaskDecomposer, SubTask
import logging
from datetime import datetime

if TYPE_CHECKING:
    from .memagent import MemAgent

logger = logging.getLogger(__name__)

class MultiAgentOrchestrator:
    """
    Orchestrates multi-agent workflows with hierarchical coordination support.
    
    This orchestrator supports both flat and hierarchical multi-agent scenarios:
    - Flat: Root agent coordinates directly with delegate agents
    - Hierarchical: Delegate agents can have their own sub-agents, all coordinated
      within a single shared memory session for complete visibility and control
    """
    
    def __init__(self, root_agent: 'MemAgent', delegates: List['MemAgent']):
        self.root_agent = root_agent
        self.delegates = delegates
        self.shared_memory = SharedMemory(root_agent.memory_provider)
        self.task_decomposer = TaskDecomposer(root_agent)
        self.shared_memory_id = None
        self.is_nested_orchestrator = False  # Flag to track if this is a sub-level orchestrator
        
    def execute_multi_agent_workflow(self, 
                                   user_query: str, 
                                   memory_id: str = None,
                                   conversation_id: str = None) -> str:
        """
        Execute a multi-agent workflow with hierarchical coordination support.
        
        The workflow intelligently handles nested agent scenarios:
        1. First checks if the root agent is already part of an active shared session
        2. If yes, joins that session as a sub-agent rather than creating a new one
        3. If no, creates a new root-level shared session
        4. Ensures all agent activities are tracked in a unified shared memory
        
        Parameters:
            user_query (str): The task to be executed
            memory_id (str): Memory context for the execution
            conversation_id (str): Conversation context
            
        Returns:
            str: The consolidated response from all agents
        """
        
        logger.info(f"Starting multi-agent workflow for query: {user_query}")
        logger.info(f"Root agent ID: {self.root_agent.agent_id}")
        logger.info(f"Number of delegates: {len(self.delegates)}")
        logger.info(f"Delegate IDs: {[agent.agent_id for agent in self.delegates]}")
        
        try:
            # HIERARCHICAL COORDINATION: Check if we should join an existing session
            existing_session = self._find_or_create_shared_session()
            
            if existing_session:
                self.shared_memory_id = str(existing_session.get("_id"))
                logger.info(f"Joining existing shared session: {self.shared_memory_id}")
                self.is_nested_orchestrator = True
                
                # Register our delegates as sub-agents in the existing session
                delegate_ids = [agent.agent_id for agent in self.delegates]
                if delegate_ids:
                    self.shared_memory.register_sub_agents(
                        memory_id=self.shared_memory_id,
                        parent_agent_id=self.root_agent.agent_id,
                        sub_agent_ids=delegate_ids
                    )
            
            # **FIX: Add shared memory ID to root agent's memory_ids array**
            # This ensures consistency between single-agent and multi-agent memory management
            if (self.shared_memory_id and 
                self.root_agent.agent_id and 
                self.shared_memory_id not in (self.root_agent.memory_ids or [])):
                
                # Initialize memory_ids if it's None
                if self.root_agent.memory_ids is None:
                    self.root_agent.memory_ids = []
                
                # Add shared memory ID to agent's memory_ids array
                self.root_agent.memory_ids.append(self.shared_memory_id)
                logger.info(f"Added shared memory ID {self.shared_memory_id} to root agent's memory_ids array")
                
                # Persist the updated memory_ids to the memory provider
                if hasattr(self.root_agent.memory_provider, "update_memagent_memory_ids"):
                    update_success = self.root_agent.memory_provider.update_memagent_memory_ids(
                        self.root_agent.agent_id, 
                        self.root_agent.memory_ids
                    )
                    logger.info(f"Persisted memory_ids to storage: {'success' if update_success else 'failed'}")
                else:
                    logger.warning("Memory provider doesn't support update_memagent_memory_ids")
            
            # **FIX: Also add shared memory ID to delegate agents' memory_ids arrays**
            # This ensures all participating agents have access to the shared memory
            for delegate in self.delegates:
                if (self.shared_memory_id and 
                    delegate.agent_id and 
                    self.shared_memory_id not in (delegate.memory_ids or [])):
                    
                    # Initialize memory_ids if it's None
                    if delegate.memory_ids is None:
                        delegate.memory_ids = []
                    
                    # Add shared memory ID to delegate's memory_ids array
                    delegate.memory_ids.append(self.shared_memory_id)
                    logger.info(f"Added shared memory ID {self.shared_memory_id} to delegate agent {delegate.agent_id}'s memory_ids array")
                    
                    # Persist the updated memory_ids to the memory provider
                    if hasattr(delegate.memory_provider, "update_memagent_memory_ids"):
                        update_success = delegate.memory_provider.update_memagent_memory_ids(
                            delegate.agent_id, 
                            delegate.memory_ids
                        )
                        logger.info(f"Persisted delegate {delegate.agent_id} memory_ids to storage: {'success' if update_success else 'failed'}")
                    else:
                        logger.warning(f"Delegate {delegate.agent_id} memory provider doesn't support update_memagent_memory_ids")
            
            # Log the start of multi-agent execution with hierarchy context
            self.shared_memory.add_blackboard_entry(
                memory_id=self.shared_memory_id,
                agent_id=self.root_agent.agent_id,
                content={
                    "original_query": user_query,
                    "started_at": datetime.now().isoformat(),
                    "orchestrator_type": "nested" if self.is_nested_orchestrator else "root",
                    "delegate_count": len(self.delegates)
                },
                entry_type="workflow_start"
            )
            
            # 2. Decompose task into sub-tasks
            logger.info("Starting task decomposition...")
            sub_tasks = self._enhance_task_decomposition_with_hierarchy(user_query)
            logger.info(f"Task decomposition resulted in {len(sub_tasks)} sub-tasks")
            
            if not sub_tasks:
                # Fallback to single agent execution
                logger.warning("Task decomposition failed, falling back to root agent")
                logger.info("Executing fallback with root agent...")
                result = self.root_agent.run(user_query, memory_id, conversation_id)
                logger.info(f"Root agent returned: {result[:100]}..." if result else "No result from root agent")
                return result
            
            # Log task decomposition
            self.shared_memory.add_blackboard_entry(
                memory_id=self.shared_memory_id,
                agent_id=self.root_agent.agent_id,
                content={"sub_tasks": [task.to_dict() for task in sub_tasks]},
                entry_type="task_decomposition"
            )
            
            # 3. Execute sub-tasks in parallel
            logger.info(f"Executing {len(sub_tasks)} sub-tasks in parallel...")
            sub_task_results = self._execute_sub_tasks_parallel(sub_tasks, memory_id, conversation_id)
            logger.info(f"Sub-task execution completed with {len(sub_task_results)} results")
            
            # 4. Consolidate results
            logger.info("Starting result consolidation...")
            consolidated_response = self._consolidate_results(user_query, sub_task_results)
            logger.info(f"Consolidation completed: {consolidated_response[:100]}..." if consolidated_response else "No consolidated response")
            
            # 5. Update shared memory with final result
            self.shared_memory.add_blackboard_entry(
                memory_id=self.shared_memory_id,
                agent_id=self.root_agent.agent_id,
                content={"consolidated_response": consolidated_response, "completed_at": datetime.now().isoformat()},
                entry_type="workflow_complete"
            )
            
            # Mark session as completed
            self.shared_memory.update_session_status(self.shared_memory_id, "completed")
            
            logger.info("Multi-agent workflow completed successfully")
            return consolidated_response
            
        except Exception as e:
            logger.error(f"Error in multi-agent workflow: {e}", exc_info=True)
            # Mark session as failed
            if self.shared_memory_id:
                self.shared_memory.update_session_status(self.shared_memory_id, "failed")
            
            # Fallback to single agent execution
            logger.info("Attempting fallback to root agent due to error...")
            try:
                result = self.root_agent.run(user_query, memory_id, conversation_id)
                logger.info(f"Fallback completed: {result[:100]}..." if result else "No result from fallback")
                return result
            except Exception as fallback_error:
                logger.error(f"Fallback also failed: {fallback_error}", exc_info=True)
                return f"Multi-agent workflow failed: {str(e)}. Fallback also failed: {str(fallback_error)}"
    
    def _execute_sub_tasks_parallel(self, 
                                  sub_tasks: List[SubTask], 
                                  memory_id: str,
                                  conversation_id: str) -> List[Dict[str, Any]]:
        """Execute sub-tasks in parallel using ThreadPoolExecutor."""
        
        # Create agent mapping for quick lookup
        agent_map = {agent.agent_id: agent for agent in self.delegates}
        
        # Sort tasks by priority and handle dependencies
        sorted_tasks = sorted(sub_tasks, key=lambda x: x.priority)
        
        results = []
        completed_tasks = set()
        
        # Use ThreadPoolExecutor for parallel execution
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.delegates)) as executor:
            # Submit tasks that have no dependencies first
            future_to_task = {}
            
            for task in sorted_tasks:
                if not task.dependencies:  # No dependencies, can execute immediately
                    agent = agent_map.get(task.assigned_agent_id)
                    if agent:
                        future = executor.submit(self._execute_single_task, task, agent, memory_id, conversation_id)
                        future_to_task[future] = task
            
            # Process completed tasks and submit dependent tasks
            while future_to_task:
                # Wait for at least one task to complete
                done_futures = concurrent.futures.wait(future_to_task.keys(), return_when=concurrent.futures.FIRST_COMPLETED)
                
                for future in done_futures.done:
                    task = future_to_task[future]
                    try:
                        result = future.result()
                        task.result = result
                        task.status = "completed"
                        completed_tasks.add(task.task_id)
                        results.append(task.to_dict())
                        
                        # Log task completion
                        self.shared_memory.add_blackboard_entry(
                            memory_id=self.shared_memory_id,
                            agent_id=task.assigned_agent_id,
                            content={"task_id": task.task_id, "result": result},
                            entry_type="task_completion"
                        )
                        
                    except Exception as e:
                        logger.error(f"Error executing task {task.task_id}: {e}")
                        task.status = "failed"
                        task.result = f"Error: {str(e)}"
                        results.append(task.to_dict())
                    
                    del future_to_task[future]
                
                # Check for new tasks that can be executed (dependencies met)
                for task in sorted_tasks:
                    if (task.status == "pending" and 
                        task.task_id not in [t.task_id for t in future_to_task.values()] and
                        all(dep in completed_tasks for dep in task.dependencies)):
                        
                        agent = agent_map.get(task.assigned_agent_id)
                        if agent:
                            future = executor.submit(self._execute_single_task, task, agent, memory_id, conversation_id)
                            future_to_task[future] = task
        
        return results
    
    def _execute_single_task(self, 
                           task: SubTask, 
                           agent: 'MemAgent', 
                           memory_id: str,
                           conversation_id: str) -> str:
        """Execute a single sub-task with an agent."""
        
        try:
            # Log task start
            self.shared_memory.add_blackboard_entry(
                memory_id=self.shared_memory_id,
                agent_id=agent.agent_id,
                content={"task_id": task.task_id, "description": task.description, "started_at": datetime.now().isoformat()},
                entry_type="task_start"
            )
            
            # Execute the task
            result = agent.run(task.description, memory_id, conversation_id)
            
            return result
            
        except Exception as e:
            logger.error(f"Error executing task {task.task_id} with agent {agent.agent_id}: {e}")
            raise
    
    def _consolidate_results(self, 
                           original_query: str, 
                           sub_task_results: List[Dict[str, Any]]) -> str:
        """Consolidate results from all sub-tasks into a final response."""
        
        try:
            # Create consolidation prompt
            consolidation_prompt = self.task_decomposer.create_consolidation_prompt(
                original_query, sub_task_results
            )
            
            # Use root agent to consolidate
            messages = [
                {"role": "system", "content": consolidation_prompt},
                {"role": "user", "content": "Please provide a consolidated response based on the sub-task results."}
            ]
            
            response = self.root_agent.model.client.responses.create(
                model="gpt-4.1",
                input=messages
            )
            
            return response.output_text
            
        except Exception as e:
            logger.error(f"Error consolidating results: {e}")
            
            # Fallback: simple concatenation
            consolidated = f"Results for: {original_query}\n\n"
            for result in sub_task_results:
                consolidated += f"Task: {result.get('description', 'Unknown')}\n"
                consolidated += f"Result: {result.get('result', 'No result')}\n\n"
            
            return consolidated
    
    def get_shared_memory_context(self) -> str:
        """Get shared memory context for inclusion in agent prompts."""
        
        if not self.shared_memory_id:
            return ""
        
        try:
            # Get blackboard entries
            entries = self.shared_memory.get_blackboard_entries(self.shared_memory_id)
            
            if not entries:
                return ""
            
            context = "\n\n---------SHARED MEMORY CONTEXT---------\n"
            context += "Multi-agent coordination information:\n\n"
            
            for entry in entries[-10:]:  # Last 10 entries
                context += f"Agent: {entry.get('agent_id', 'Unknown')}\n"
                context += f"Type: {entry.get('entry_type', 'Unknown')}\n"
                context += f"Content: {entry.get('content', 'No content')}\n"
                context += f"Time: {entry.get('created_at', 'Unknown')}\n"
                context += "---\n"
            
            return context
            
        except Exception as e:
            logger.error(f"Error getting shared memory context: {e}")
            return ""

    def _find_or_create_shared_session(self) -> Optional[Dict[str, Any]]:
        """
        Intelligent session management for hierarchical multi-agent coordination.
        
        This method implements the core logic for hierarchical coordination:
        1. Checks if the root agent is already participating in an active shared session
        2. If found, returns that session to enable joining (hierarchical mode)
        3. If not found, creates a new root-level session (flat mode)
        
        This ensures that sub-agents don't create isolated sessions but instead
        join the existing coordination context, enabling true hierarchical workflows.
        
        Returns:
            Optional[Dict[str, Any]]: Existing session to join, or None if new session created
        """
        try:
            # Check if our root agent is already part of an active shared session
            existing_session = self.shared_memory.find_active_session_for_agent(self.root_agent.agent_id)
            
            if existing_session:
                logger.info(f"Found existing session for agent {self.root_agent.agent_id}")
                logger.info(f"Session hierarchy: {self.shared_memory.get_agent_hierarchy(str(existing_session.get('_id')))}")
                return existing_session
            
            # No existing session found - create a new root-level session
            logger.info("No existing session found, creating new root-level shared session")
            delegate_ids = [agent.agent_id for agent in self.delegates]
            
            self.shared_memory_id = self.shared_memory.create_shared_session(
                root_agent_id=self.root_agent.agent_id,
                delegate_agent_ids=delegate_ids
            )
            
            logger.info(f"Created new shared memory session: {self.shared_memory_id}")
            logger.info(f"Initial delegates: {delegate_ids}")
            
            # Return None to indicate we created a new session (not joining existing)
            return None
            
        except Exception as e:
            logger.error(f"Error in session management: {e}", exc_info=True)
            # Fallback: create new session
            delegate_ids = [agent.agent_id for agent in self.delegates]
            self.shared_memory_id = self.shared_memory.create_shared_session(
                root_agent_id=self.root_agent.agent_id,
                delegate_agent_ids=delegate_ids
            )
            return None

    def _enhance_task_decomposition_with_hierarchy(self, user_query: str) -> List[SubTask]:
        """
        Enhanced task decomposition that considers the complete agent hierarchy.
        
        Traditional decomposition only considers immediate delegates. This enhanced
        version looks at the full shared memory session to understand the complete
        agent capabilities available across all hierarchy levels.
        
        Parameters:
            user_query (str): The task to decompose
            
        Returns:
            List[SubTask]: Decomposed tasks optimized for the full hierarchy
        """
        try:
            # Get the complete agent hierarchy from shared memory
            hierarchy = self.shared_memory.get_agent_hierarchy(self.shared_memory_id)
            
            logger.info(f"Task decomposition considering hierarchy: {hierarchy}")
            
            # Standard task decomposition with immediate delegates
            sub_tasks = self.task_decomposer.decompose_task(user_query, self.delegates)
            
            # TODO: Future enhancement - analyze sub_agent capabilities for optimal task assignment
            # This could involve:
            # 1. Collecting capabilities from all sub-agents in the hierarchy
            # 2. Re-optimizing task assignments based on the full capability set
            # 3. Creating more granular tasks that leverage specific sub-agent strengths
            
            logger.info(f"Decomposed into {len(sub_tasks)} tasks for {hierarchy.get('total_agents', 0)} total agents")
            
            return sub_tasks
            
        except Exception as e:
            logger.error(f"Error in enhanced task decomposition: {e}")
            # Fallback to standard decomposition
            return self.task_decomposer.decompose_task(user_query, self.delegates) 


================================================
FILE: src/memorizz/task_decomposition.py
================================================
from typing import List, Dict, Any, Optional, TYPE_CHECKING
import json
import logging

if TYPE_CHECKING:
    from .memagent import MemAgent

logger = logging.getLogger(__name__)

class SubTask:
    """Represents a decomposed sub-task for delegation."""
    
    def __init__(self, 
                 task_id: str,
                 description: str, 
                 assigned_agent_id: str, 
                 priority: int = 1,
                 dependencies: List[str] = None):
        self.task_id = task_id
        self.description = description
        self.assigned_agent_id = assigned_agent_id
        self.priority = priority
        self.dependencies = dependencies or []
        self.status = "pending"  # pending, in_progress, completed, failed
        self.result = None
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            "task_id": self.task_id,
            "description": self.description,
            "assigned_agent_id": self.assigned_agent_id,
            "priority": self.priority,
            "dependencies": self.dependencies,
            "status": self.status,
            "result": self.result
        }

class TaskDecomposer:
    """Handles task decomposition for multi-agent coordination."""
    
    def __init__(self, root_agent: 'MemAgent'):
        self.root_agent = root_agent
        
    def analyze_delegate_capabilities(self, delegates: List['MemAgent']) -> Dict[str, Dict[str, Any]]:
        """
        Analyze the capabilities of delegate agents.
        
        Parameters:
            delegates (List[MemAgent]): List of delegate agents
            
        Returns:
            Dict[str, Dict[str, Any]]: Mapping of agent_id to capabilities
        """
        capabilities = {}
        
        for agent in delegates:
            agent_capabilities = {
                "agent_id": agent.agent_id,
                "instruction": agent.instruction,
                "persona": agent.persona.generate_system_prompt_input() if agent.persona else None,
                "tools": [],
                "application_mode": agent.application_mode.value
            }
            
            # Extract tool capabilities
            if agent.tools:
                if hasattr(agent.tools, 'list_tools'):  # Toolbox case
                    for tool in agent.tools.list_tools():
                        if "function" in tool:
                            agent_capabilities["tools"].append({
                                "name": tool["function"].get("name"),
                                "description": tool["function"].get("description")
                            })
                elif isinstance(agent.tools, list):  # List case
                    for tool in agent.tools:
                        agent_capabilities["tools"].append({
                            "name": tool.get("name"),
                            "description": tool.get("description")
                        })
            
            capabilities[agent.agent_id] = agent_capabilities
            
        return capabilities
    
    def decompose_task(self, 
                      user_query: str, 
                      delegates: List['MemAgent']) -> List[SubTask]:
        """
        Decompose a complex task into sub-tasks aligned with delegate capabilities.
        
        Parameters:
            user_query (str): The original user query/task
            delegates (List[MemAgent]): List of available delegate agents
            
        Returns:
            List[SubTask]: List of decomposed sub-tasks
        """
        try:
            logger.info(f"Starting task decomposition for query: {user_query}")
            logger.info(f"Number of delegates: {len(delegates)}")
            
            # Analyze delegate capabilities
            logger.info("Analyzing delegate capabilities...")
            capabilities = self.analyze_delegate_capabilities(delegates)
            logger.info(f"Analyzed capabilities for {len(capabilities)} agents")
            
            # Create decomposition prompt
            logger.info("Creating decomposition prompt...")
            decomposition_prompt = self._create_decomposition_prompt(user_query, capabilities)
            
            # Use the root agent's model to decompose the task
            messages = [
                {"role": "system", "content": decomposition_prompt},
                {"role": "user", "content": f"Please decompose this task: {user_query}"}
            ]
            
            logger.info("Calling LLM for task decomposition...")
            response = self.root_agent.model.client.responses.create(
                model="gpt-4.1",
                input=messages
            )
            
            logger.info(f"LLM response received: {response.output_text[:200]}..." if response.output_text else "No response text")
            
            # Parse the response to extract sub-tasks
            logger.info("Parsing decomposition response...")
            sub_tasks = self._parse_decomposition_response(response.output_text, capabilities)
            logger.info(f"Successfully parsed {len(sub_tasks)} sub-tasks")
            
            return sub_tasks
            
        except Exception as e:
            logger.error(f"Error decomposing task: {e}", exc_info=True)
            return []
    
    def _create_decomposition_prompt(self, 
                                   user_query: str, 
                                   capabilities: Dict[str, Dict[str, Any]]) -> str:
        """Create the system prompt for task decomposition."""
        
        prompt = """You are a task decomposition specialist for a multi-agent system. Your job is to analyze a complex user query and break it down into specific sub-tasks that can be assigned to different specialized agents.

AVAILABLE AGENTS AND THEIR CAPABILITIES:
"""
        
        for agent_id, caps in capabilities.items():
            prompt += f"\nAgent ID: {agent_id}\n"
            prompt += f"Specialization: {caps.get('instruction', 'General assistant')}\n"
            if caps.get('persona'):
                prompt += f"Persona: {caps['persona'][:200]}...\n"
            
            if caps.get('tools'):
                prompt += "Available Tools:\n"
                for tool in caps['tools']:
                    prompt += f"  - {tool.get('name', 'Unknown')}: {tool.get('description', 'No description')}\n"
            prompt += "\n"
        
        prompt += """
DECOMPOSITION RULES:
1. Break down the user query into specific, actionable sub-tasks
2. Assign each sub-task to the most appropriate agent based on their capabilities
3. Consider task dependencies and execution order
4. Each sub-task should be independent or have clear dependencies
5. Prioritize tasks (1=highest, 5=lowest)

RESPONSE FORMAT:
Respond with a JSON array of sub-tasks in this exact format:
[
    {
        "task_id": "task_1",
        "description": "Specific task description",
        "assigned_agent_id": "agent_id",
        "priority": 1,
        "dependencies": []
    }
]

Only respond with the JSON array, no additional text.
"""
        
        return prompt
    
    def _parse_decomposition_response(self, 
                                    response_text: str, 
                                    capabilities: Dict[str, Dict[str, Any]]) -> List[SubTask]:
        """Parse the decomposition response and create SubTask objects."""
        try:
            # Extract JSON from response
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            
            task_data = json.loads(response_text)
            
            sub_tasks = []
            for task in task_data:
                # Validate that assigned agent exists
                if task.get("assigned_agent_id") in capabilities:
                    sub_task = SubTask(
                        task_id=task.get("task_id"),
                        description=task.get("description"),
                        assigned_agent_id=task.get("assigned_agent_id"),
                        priority=task.get("priority", 1),
                        dependencies=task.get("dependencies", [])
                    )
                    sub_tasks.append(sub_task)
                else:
                    logger.warning(f"Invalid agent assignment in task: {task}")
            
            return sub_tasks
            
        except Exception as e:
            logger.error(f"Error parsing decomposition response: {e}")
            return []
    
    def create_consolidation_prompt(self, 
                                  original_query: str, 
                                  sub_task_results: List[Dict[str, Any]]) -> str:
        """Create a prompt for consolidating sub-task results."""
        
        prompt = f"""You are a result consolidation specialist. Your job is to take the results from multiple specialized agents and create a comprehensive, coherent response to the original user query.

ORIGINAL USER QUERY: {original_query}

SUB-TASK RESULTS:
"""
        
        for result in sub_task_results:
            prompt += f"\nTask: {result.get('description', 'Unknown task')}\n"
            prompt += f"Agent: {result.get('assigned_agent_id', 'Unknown agent')}\n"
            prompt += f"Status: {result.get('status', 'Unknown status')}\n"
            prompt += f"Result: {result.get('result', 'No result')}\n"
            prompt += "---\n"
        
        prompt += """
CONSOLIDATION INSTRUCTIONS:
1. Synthesize all sub-task results into a coherent response
2. Address the original user query comprehensively
3. Identify any gaps or missing information
4. If there are conflicting results, highlight them
5. Provide a clear, helpful final answer

If the objective is not fully met, specify what additional steps are needed.
"""
        
        return prompt 


================================================
FILE: src/memorizz/context_window_management/__init__.py
================================================
[Empty file]


================================================
FILE: src/memorizz/context_window_management/cwm.py
================================================
from typing import List
# from ..memagent import MemAgent
from ..memory_provider.memory_type import MemoryType

# Can take in an agent and then return a prompt that informs the agent on how to manage the context window
class CWM:
    # def __init__(self, agent: MemAgent):
    #     self.agent = agent
    
    @staticmethod
    def get_prompt_from_memory_types(memory_types: List[MemoryType]):
        prompt = "You are an AI Agent endowed with a powerful, multi-tiered memory augmentation system. Your mission is to use all available memory modalities to deliver consistent, accurate, and context-rich responses. The aim is to esure that through augmented memory, you become belivable, capable, and reliable."

        for memory_type in memory_types:
            prompt += CWM._generate_prompt_for_memory_type(memory_type)

        return prompt

    @staticmethod
    def _generate_prompt_for_memory_type(memory_type: MemoryType):
        # Define memory type prompts in a dictionary for better maintainability
        memory_prompts = {
            MemoryType.CONVERSATION_MEMORY: {
                "description": "This is a memory type that stores the conversation history between the agent and the user.",
                "usage": "Use this to provide continuity, avoid repeating yourself, and reference prior turns."
            },
            MemoryType.WORKFLOW_MEMORY: {
                "description": "This is a memory type that stores the workflow history between the agent and the user.",
                "usage": "Use this to provide continuity, avoid repeating yourself, and reference prior turns."
            },
            MemoryType.SHARED_MEMORY: {
                "description": "This is a memory type that stores shared blackboard information for multi-agent coordination.",
                "usage": "Use this to coordinate with other agents, understand your role in the agent hierarchy, and access shared coordination activities and context."
            },
            MemoryType.SUMMARIES: {
                "description": "This is a memory type that stores compressed summaries of past conversations and interactions to preserve important context while managing memory efficiently.",
                "usage": "Use these summaries to understand the broader context of your interactions with the user, recall important topics, preferences, and past decisions. This helps you provide more personalized and context-aware responses even when specific conversations are no longer in active memory."
            }
        }
        
        # Get the prompt configuration for this memory type
        prompt_config = memory_prompts.get(memory_type)
        
        if prompt_config:
            prompt = f"\n\nMemory Type: {memory_type.value}\n"
            prompt += f"Memory Type Description: {prompt_config['description']}\n"
            prompt += f"Memory Type Usage: {prompt_config['usage']}\n"
            return prompt
        else:
            # Handle unknown memory types gracefully
            return f"\n\nMemory Type: {memory_type.value}\n"

# Can take in an array of memory stores and then return a prompt that informs the agent on how to manage the context window


================================================
FILE: src/memorizz/database/__init__.py
================================================
from .mongodb.mongodb_tools import MongoDBTools, MongoDBToolsConfig, get_mongodb_toolbox

__all__ = [
    # MongoDB tools
    'MongoDBTools', 
    'MongoDBToolsConfig', 
    'get_mongodb_toolbox'
] 


================================================
FILE: src/memorizz/database/mongodb/mongodb_tools.py
================================================
import os
import getpass
import inspect
from functools import wraps
from typing import get_type_hints, List, Dict, Any, Optional
import pymongo
from pymongo.collection import Collection
from pymongo.operations import SearchIndexModel
import logging
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Suppress httpx logs to reduce noise from API requests
logging.getLogger("httpx").setLevel(logging.WARNING)

@dataclass
class MongoDBToolsConfig:
    mongo_uri: Optional[str] = None
    db_name: str = 'function_calling_db'
    collection_name: str = 'tools'
    vector_search_candidates: int = 150,
    vector_index_name: str = "vector_index"
    get_embedding: callable = None


class MongoDBTools:
    def __init__(self, config: MongoDBToolsConfig = MongoDBToolsConfig()):
        self.config = config
        if not self.config.get_embedding:
            #throw error
            raise ValueError("get_embedding function is not provided")
        if self.config.mongo_uri is None:
            self.config.mongo_uri = os.getenv('MONGO_URI') or getpass.getpass("Enter MongoDB URI: ")
        
        self.mongo_client = None
        self.db = None
        self.tools_collection = None
        
        try:
            self.mongo_client = pymongo.MongoClient(self.config.mongo_uri, appname="memorizz.python.package")
            self.db = self.mongo_client[self.config.db_name]

            # Check if collection exists, create if it doesn't
            if self.config.collection_name not in self.db.list_collection_names():
                self.db.create_collection(self.config.collection_name)
                logger.info(f"Collection '{self.config.collection_name}' created.")
            
            self.tools_collection = self.db[self.config.collection_name]

            # Check if vector search index exists, create if it doesn't
            self._ensure_vector_search_index()

            logger.info("MongoDBTools initialized successfully.")

        except pymongo.errors.ConnectionFailure:
            logger.error("Failed to connect to MongoDB. Please check your connection string and network.")
        except pymongo.errors.OperationFailure as e:
            if e.code == 13:  # Authentication failed
                logger.error("MongoDB authentication failed. Please check your credentials.")
            else:
                logger.error(f"MongoDB operation failed: {str(e)}")
        except Exception as e:
            logger.error(f"Unexpected error during MongoDB initialization: {str(e)}")
        
        if self.tools_collection is None:
            logger.warning("MongoDBTools initialization failed. Some features may not work.")

    def mongodb_toolbox(self, collection: Optional[Collection] = None):
        if collection is None:
            collection = self.tools_collection

        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)

            signature = inspect.signature(func)
            docstring = inspect.getdoc(func) or ""

            if not docstring:
                raise ValueError(f"Error registering tool {func.__name__}: Docstring is missing. Please provide a docstring for the function.")

            type_hints = get_type_hints(func)

            tool_def = {
                "name": func.__name__,
                "description": docstring.strip(),
                "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }

            for param_name, param in signature.parameters.items():
                if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                    continue

                param_type = type_hints.get(param_name, type(None))
                json_type = "string"
                if param_type in (int, float):
                    json_type = "number"
                elif param_type == bool:
                    json_type = "boolean"

                tool_def["parameters"]["properties"][param_name] = {
                    "type": json_type,
                    "description": f"Parameter {param_name}"
                }

                if param.default == inspect.Parameter.empty:
                    tool_def["parameters"]["required"].append(param_name)

            tool_def["parameters"]["additionalProperties"] = False

            try:
                vector = self.config.get_embedding(tool_def["description"])
                tool_doc = {
                    **tool_def,
                    "embedding": vector
                }
                collection.update_one({"name": func.__name__}, {"$set": tool_doc}, upsert=True)
                logger.info(f"Successfully registered tool: {func.__name__}")
            except Exception as e:
                logger.error(f"Error registering tool {func.__name__}: {str(e)}")
                raise

            return wrapper
        return decorator

    def _vector_search(self, user_query: str, collection: Optional[Collection] = None, limit: int = 2) -> List[Dict[str, Any]]:
        if collection is None:
            collection = self.tools_collection

        try:
            query_embedding = self.config.get_embedding(user_query)
        except Exception as e:
            logger.error(f"Error generating embedding for query: {str(e)}")
            raise

        vector_search_stage = {
            "$vectorSearch": {
                "index": self.config.vector_index_name,
                "queryVector": query_embedding,
                "path": "embedding",
                "numCandidates": self.config.vector_search_candidates,
                "limit": limit
            }
        }

        unset_stage = {
            "$unset": "embedding"
        }

        pipeline = [vector_search_stage, unset_stage]

        try:
            results = collection.aggregate(pipeline)
            return list(results)
        except Exception as e:
            logger.error(f"Error performing vector search: {str(e)}")
            raise

    def populate_tools(self, user_query: str, num_tools: int = 2) -> List[Dict[str, Any]]:
        try:
            search_results = self._vector_search(user_query, limit=num_tools)
            tools = []
            for result in search_results:
                print(result)
                tool = {
                    "type": "function",
                    "function": {
                        "name": result["name"],
                        "description": result["description"],
                        "parameters": result["parameters"]
                    }
                }
                tools.append(tool)
            logger.info(f"Successfully populated {len(tools)} tools")
            return tools
        except Exception as e:
            logger.error(f"Error populating tools: {str(e)}")
            raise

    def create_toolbox(self, collection_name: str, vector_index_definition: Dict[str, Any], index_name: str = "vector_index"):
        """
        Create a collection in the MongoDB database and set up a vector search index.

        Args:
        collection_name (str): Name of the collection to create
        vector_index_definition (Dict[str, Any]): Dictionary containing the vector index definition
        index_name (str): Name of the index (default: "vector_index")

        Returns:
        bool: True if the toolbox was created successfully, False otherwise
        """
        try:
            # Create the collection
            collection = self.db.create_collection(collection_name)
            logger.info(f"Collection '{collection_name}' created successfully.")

            # Create the vector search index
            search_index_model = SearchIndexModel(
                definition=vector_index_definition,
                name=index_name
            )

            result = collection.create_search_index(model=search_index_model)
            logger.info(f"Vector search index '{index_name}' created successfully for collection '{collection_name}'.")

            # Update the config to use the new collection
            self.config.collection_name = collection_name
            self.config.vector_index_name = index_name
            self.tools_collection = collection

            return True

        except pymongo.errors.CollectionInvalid:
            logger.warning(f"Collection '{collection_name}' already exists. Using existing collection.")
            collection = self.db[collection_name]
            self.tools_collection = collection
            
            # Check if the index already exists
            existing_indexes = collection.list_indexes()
            index_exists = any(index['name'] == index_name for index in existing_indexes)
            
            if not index_exists:
                # Create the index if it doesn't exist
                search_index_model = SearchIndexModel(
                    definition=vector_index_definition,
                    name=index_name
                )
                result = collection.create_search_index(model=search_index_model)
                logger.info(f"Vector search index '{index_name}' created successfully for existing collection '{collection_name}'.")
            else:
                logger.info(f"Vector search index '{index_name}' already exists for collection '{collection_name}'.")

            # Update the config to use the existing collection
            self.config.collection_name = collection_name
            self.config.vector_index_name = index_name

            return True

        except Exception as e:
            logger.error(f"Error creating toolbox: {str(e)}")
            return False

    def _ensure_vector_search_index(self):
        try:
            indexes = list(self.tools_collection.list_indexes())
            index_exists = any(index['name'] == self.config.vector_index_name for index in indexes)
            
            if not index_exists:
                vector_index_definition = {
                    "mappings": {
                        "dynamic": True,
                        "fields": {
                            "embedding": {
                                "dimensions": len(self.config.get_embedding("0")),
                                "similarity": "cosine",
                                "type": "knnVector",
                            }
                        }
                    }
                }
                try:
                    # Create SearchIndexModel and use it in create_search_index
                    search_index_model = SearchIndexModel(
                        definition=vector_index_definition,
                        name=self.config.vector_index_name
                    )
                    self.tools_collection.create_search_index(search_index_model)
                    logger.info(f"Vector search index '{self.config.vector_index_name}' created.")
                except pymongo.errors.OperationFailure as e:
                    if e.code == 68 or "already exists" in str(e):
                        logger.info(f"Vector search index '{self.config.vector_index_name}' already exists.")
                    else:
                        logger.warning(f"Unexpected error while creating index: {str(e)}")
            else:
                logger.info(f"Vector search index '{self.config.vector_index_name}' already exists.")
        except Exception as e:
            logger.warning(f"Note on vector search index: {str(e)}")
            
__all__ = ['MongoDBTools', 'MongoDBToolsConfig']

# You can create a function to get the mongodb_toolbox decorator:
def get_mongodb_toolbox(config: MongoDBToolsConfig = MongoDBToolsConfig()):
    return MongoDBTools(config).mongodb_toolbox


================================================
FILE: src/memorizz/embeddings/ollama.py
================================================
# ./embeddings/ollama.py

import logging
from langchain_ollama import OllamaEmbeddings    
from typing import List

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_embedding(text: str, model: str = "nomic-embed-text") -> List[float]:
    embeddings = OllamaEmbeddings(model=model)   
    text = text.replace("\n", " ")
    try:
        return embeddings.embed_query(text)
    except Exception as e:
        logger.error(f"Error generating embedding: {str(e)}")
        raise


================================================
FILE: src/memorizz/embeddings/openai.py
================================================
# ./embeddings/openai.py

import logging
import openai
from typing import List

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Suppress httpx logs to reduce noise from API requests
logging.getLogger("httpx").setLevel(logging.WARNING)

openai_client = openai.OpenAI()

def get_embedding(text: str, model: str = "text-embedding-3-small", dimensions: int = 256) -> List[float]:

    """
    Get the embedding of a text using OpenAI's API.

    Parameters:
        text (str): The text to get the embedding of.
        model (str): The model to use for the embedding.
        dimensions (int): The dimensions of the embedding.

    Returns:
        List[float]: The embedding of the text.
    """

    text = text.replace("\n", " ")
    try:
        return openai_client.embeddings.create(input=[text], model=model, dimensions=dimensions).data[0].embedding
    except Exception as e:
        logger.error(f"Error generating embedding: {str(e)}")
        raise

def get_embedding_dimensions(model: str = "text-embedding-3-small") -> int:
    """
    Get the dimensions of the embedding for a given model.

    Parameters:
        model (str): The model to get the dimensions of.

    Returns:
        int: The dimensions of the embedding.
    """
    if model == "text-embedding-3-small":
        return 256
    else:
        raise ValueError(f"Unsupported model: {model}")





================================================
FILE: src/memorizz/llms/__init__.py
================================================
from .openai import OpenAI

__all__ = ['OpenAI']


================================================
FILE: src/memorizz/llms/openai.py
================================================
import os
import json
import openai
import logging
from typing import Callable, List, Optional, TYPE_CHECKING, Dict, Any

# Suppress httpx logs to reduce noise from API requests
logging.getLogger("httpx").setLevel(logging.WARNING)

# Use TYPE_CHECKING for forward references to avoid circular imports
if TYPE_CHECKING:
    from ..toolbox.tool_schema import ToolSchemaType
import inspect

class OpenAI:
    """
    A class for interacting with the OpenAI API.
    """
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o"):
        """
        Initialize the OpenAI client.

        Parameters:
        -----------
        api_key : str
            The API key for the OpenAI API.
        model : str, optional
            The model to use for the OpenAI API.
        """
        if api_key is None:
            api_key = os.getenv("OPENAI_API_KEY")

        self.client = openai.OpenAI(api_key=api_key)
        self.model = model


    def get_tool_metadata(self, func: Callable) -> Dict[str, Any]:
        """
        Get the metadata for a tool.

        Parameters:
        -----------
        func : Callable
            The function to get the metadata for.

        Returns:
        --------
        Dict[str, Any]
        """
        # We'll import ToolSchemaType here to avoid circular imports
        from ..toolbox.tool_schema import ToolSchemaType

        docstring = func.__doc__ or ""
        signature = str(inspect.signature(func))
        func_name = func.__name__

        system_msg = {
            "role": "system",
            "content": (
                "You are an expert metadata augmentation assistant specializing in JSON schema discovery "
                "and documentation enhancement.\n\n"
                f"**IMPORTANT**: Use the function name exactly as provided (`{func_name}`) and do NOT rename it."
            )
        }

        user_msg = {
            "role": "user",
            "content": (
                f"Generate enriched metadata for the function `{func_name}`.\n\n"
                f"- Docstring: {docstring}\n"
                f"- Signature: {signature}\n\n"
                "Enhance the metadata by:\n"
                "• Expanding the docstring into a detailed description.\n"
                "• Writing clear natural‐language descriptions for each parameter, including type, purpose, and constraints.\n"
                "• Identifying which parameters are required.\n"
                "• (Optional) Suggesting example queries or use cases.\n\n"
                "Produce a JSON object that strictly adheres to the ToolSchemaType structure."
            )
        }

        response = self.client.responses.parse(
            model=self.model,
            input=[system_msg, user_msg],
            text_format=ToolSchemaType
        )

        return response.output_parsed
    
    def augment_docstring(self, docstring: str) -> str:
        """
        Augment the docstring with an LLM generated description.

        Parameters:
        -----------
        docstring : str
            The docstring to augment.

        Returns:
        --------
        str
        """
        response = self.client.responses.create(
            model=self.model,
            input=f"Augment the docstring {docstring} by adding more details and examples."
        )

        return response.output_text
    
    def generate_queries(self, docstring: str) -> List[str]:
        """
        Generate queries for the tool.

        Parameters:
        -----------
        docstring : str
            The docstring to generate queries for.

        Returns:
        --------
        List[str]
        """
        response = self.client.responses.create(
            model=self.model,
            input=f"Generate queries for the docstring {docstring} by adding some examples of queries that can be used to leverage the tool."
        )

        return response.output_text
    
    def generate_text(self, prompt: str, instructions: str = None) -> str:
        """
        Generate text using OpenAI's API.

        Parameters:
            prompt (str): The prompt to generate text from.
            instructions (str): The instructions to use for the generation.

        Returns:
            str: The generated text.
        """
        response = self.client.responses.create(
            model=self.model,
            instructions=instructions,
            input=prompt)
        
        return response.output_text


================================================
FILE: src/memorizz/long_term_memory/__init__.py
================================================
from .knowledge_base import KnowledgeBase

__all__ = ["KnowledgeBase"] 


================================================
FILE: src/memorizz/long_term_memory/knowledge_base.py
================================================
from typing import Dict, Any, List, Optional
import uuid
from datetime import datetime

from ..memory_provider import MemoryProvider
from ..memory_provider.memory_type import MemoryType
from ..embeddings.openai import get_embedding
from bson import ObjectId


class KnowledgeBase:
    """
    KnowledgeBase class that implements a specialized form of long-term memory.
    
    Instead of writing documents to a separate "knowledge_base" collection, 
    each ingestion is stored directly in the agent's `long_term_memory` store.
    """
    
    def __init__(self, memory_provider: Optional[MemoryProvider] = None):
        """
        Initialize a new KnowledgeBase instance.
        
        Parameters:
        -----------
        memory_provider : Optional[MemoryProvider]
            The memory provider to use for storage and retrieval.
            If not provided, a default MemoryProvider will be used.
        """
        self.memory_provider = memory_provider or MemoryProvider()
    
    def ingest_knowledge(self, corpus: str, namespace: str) -> str:
        """
        Embed and save text content to the memory provider under the given namespace.
        
        Parameters:
        -----------
        corpus : str
            The text content to be ingested and stored.
        namespace : str
            A namespace to organize and categorize the knowledge.
            
        Returns:
        --------
        str
            A unique long_term_memory_id that can be attached to an agent to scope its long-term knowledge.
        """
        long_term_memory_id = str(ObjectId())
        
        # Generate embedding for the corpus
        embedding = get_embedding(corpus)
        
        # Create the knowledge entry
        knowledge_entry = {
            "content": corpus,
            "embedding": embedding,
            "namespace": namespace,
            "long_term_memory_id": long_term_memory_id,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # Store the knowledge entry in the long_term_memory collection
        self.memory_provider.store(knowledge_entry, memory_store_type=MemoryType.LONG_TERM_MEMORY)
        
        return long_term_memory_id
    
    def retrieve_knowledge(self, long_term_memory_id: str) -> List[Dict[str, Any]]:
        """
        Retrieve all knowledge entries associated with a given long_term_memory_id.
        
        Parameters:
        -----------
        long_term_memory_id : str
            The unique ID to retrieve knowledge for.
            
        Returns:
        --------
        List[Dict[str, Any]]
            A list of knowledge documents, each containing the original content,
            its embedding, and the memory ID.
        """
        # Use list_all to get all documents and filter manually
        all_entries = self.memory_provider.list_all(memory_store_type=MemoryType.LONG_TERM_MEMORY)
        
        # Filter entries with the matching long_term_memory_id
        knowledge_entries = [entry for entry in all_entries if entry.get("long_term_memory_id") == long_term_memory_id]
        
        return knowledge_entries
    
    def retrieve_knowledge_by_query(self, query: str, namespace: Optional[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Retrieve knowledge entries that are semantically similar to the query.
        
        Parameters:
        -----------
        query : str
            The query to retrieve relevant knowledge for.
        namespace : Optional[str]
            If provided, limit the search to knowledge within this namespace.
        limit : int
            Maximum number of entries to return.
            
        Returns:
        --------
        List[Dict[str, Any]]
            A list of knowledge documents that are semantically similar to the query.
        """
        # Generate embedding for the query
        query_embedding = get_embedding(query)
        
        # Create a query object for semantic search
        query_obj = {
            "embedding": query_embedding,
            "limit": limit
        }
        
        # If namespace is provided, add it to the query
        if namespace:
            query_obj["namespace"] = namespace
        
        # Use the retrieve_by_query method for semantics search
        results = self.memory_provider.retrieve_by_query(
            query_obj,
            memory_store_type=MemoryType.LONG_TERM_MEMORY,
            limit=limit
        )
        
        # If results is a single dict, wrap it in a list
        if results and isinstance(results, dict):
            results = [results]
        
        # If no results, return empty list
        return results or []
    
    def delete_knowledge(self, long_term_memory_id: str) -> bool:
        """
        Delete all knowledge entries associated with a given long_term_memory_id.
        
        Parameters:
        -----------
        long_term_memory_id : str
            The unique ID of the knowledge to delete.
            
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        # Get all entries with this memory ID
        entries = self.retrieve_knowledge(long_term_memory_id)
        
        # Delete each entry
        success = True
        for entry in entries:
            entry_id = entry.get("_id")
            if entry_id:
                if not self.memory_provider.delete_by_id(entry_id, memory_store_type=MemoryType.LONG_TERM_MEMORY):
                    success = False
        
        return success
    
    def update_knowledge(self, long_term_memory_id: str, corpus: str) -> bool:
        """
        Update the content and embedding of a knowledge entry.
        
        Parameters:
        -----------
        long_term_memory_id : str
            The unique ID of the knowledge to update.
        corpus : str
            The new text content.
            
        Returns:
        --------
        bool
            True if update was successful, False otherwise.
        """
        # Retrieve the existing entries to get their metadata
        entries = self.retrieve_knowledge(long_term_memory_id)
        if not entries:
            return False
        
        # Generate new embedding for the updated corpus
        embedding = get_embedding(corpus)
        
        # Update each entry with new content and embedding
        success = True
        for entry in entries:
            entry_id = entry.get("_id")
            if entry_id:
                # Update the entry with new content and embedding
                entry["content"] = corpus
                entry["embedding"] = embedding
                entry["updated_at"] = datetime.now().isoformat()
                
                # Update the entry in the memory provider
                if not self.memory_provider.update_by_id(entry_id, entry, memory_store_type=MemoryType.LONG_TERM_MEMORY):
                    success = False
        
        return success
    
    def attach_to_agent(self, agent, long_term_memory_id: str) -> bool:
        """
        Attach the long-term knowledge to a MemAgent.
        
        This updates the agent's configuration to include the long-term memory ID.
        
        Parameters:
        -----------
        agent : MemAgent
            The agent to attach the knowledge to.
        long_term_memory_id : str
            The unique ID of the knowledge to attach.
            
        Returns:
        --------
        bool
            True if the attachment was successful, False otherwise.
        """
        try:
            # Verify that the knowledge exists
            entries = self.retrieve_knowledge(long_term_memory_id)
            if not entries:
                return False
            
            # Store the long_term_memory_id in the agent's attributes if it doesn't exist
            if not hasattr(agent, "long_term_memory_ids"):
                agent.long_term_memory_ids = []
                
            # Add the ID if it's not already there
            if long_term_memory_id not in agent.long_term_memory_ids:
                agent.long_term_memory_ids.append(long_term_memory_id)
                
                # Save the updated agent
                agent.update()
                
            return True
        except Exception as e:
            print(f"Error attaching knowledge to agent: {str(e)}")
            return False 


================================================
FILE: src/memorizz/memory_component/__init__.py
================================================
from .memory_component import MemoryComponent
from .conversational_memory_component import ConversationMemoryComponent
from .summary_component import SummaryComponent, SummaryMetrics
from .application_mode import ApplicationMode, ApplicationModeConfig

__all__ = ["MemoryComponent", "ConversationMemoryComponent", "SummaryComponent", "SummaryMetrics", "ApplicationMode", "ApplicationModeConfig"]


================================================
FILE: src/memorizz/memory_component/application_mode.py
================================================
from enum import Enum
from typing import List
from ..memory_provider.memory_type import MemoryType


class ApplicationMode(Enum):
    """
    Application modes define the environment and context the agent operates within,
    automatically configuring the appropriate memory types for each scenario.
    """
    
    # Core application modes
    WORKFLOW = "workflow"
    DEEP_RESEARCH = "deep_research"
    ASSISTANT = "assistant"
    
    # Extended application modes for specific use cases
    COLLABORATION = "collaboration"
    LEARNING = "learning"
    ANALYTICS = "analytics"
    
    # Default mode
    DEFAULT = ASSISTANT


class ApplicationModeConfig:
    """
    Configuration class that maps application modes to their associated memory types
    and provides additional configuration for each mode.
    """
    
    # Memory type mappings for each application mode
    MEMORY_TYPE_MAPPINGS = {
        ApplicationMode.WORKFLOW: [
            MemoryType.WORKFLOW_MEMORY,
            MemoryType.TOOLBOX,
            MemoryType.LONG_TERM_MEMORY,  # Knowledge base
            MemoryType.SHORT_TERM_MEMORY, # For intermediate results
        ],
        
        ApplicationMode.DEEP_RESEARCH: [
            MemoryType.TOOLBOX,
            MemoryType.SHARED_MEMORY,
            MemoryType.LONG_TERM_MEMORY,  # Research knowledge base
            MemoryType.SHORT_TERM_MEMORY, # For research sessions
        ],
        
        ApplicationMode.ASSISTANT: [
            MemoryType.CONVERSATION_MEMORY,
            MemoryType.LONG_TERM_MEMORY,  # Knowledge base
            MemoryType.PERSONAS,          # For personalization
            MemoryType.SHORT_TERM_MEMORY, # For context
            MemoryType.SUMMARIES,         # For memory compression
        ],
        
        # Extended modes
        ApplicationMode.COLLABORATION: [
            MemoryType.SHARED_MEMORY,
            MemoryType.CONVERSATION_MEMORY,
            MemoryType.TOOLBOX,
            MemoryType.PERSONAS,
            MemoryType.LONG_TERM_MEMORY,
        ],
        
        ApplicationMode.LEARNING: [
            MemoryType.LONG_TERM_MEMORY,  # Learning materials
            MemoryType.SHORT_TERM_MEMORY, # Study sessions
            MemoryType.CONVERSATION_MEMORY, # Q&A sessions
            MemoryType.PERSONAS,          # Learning preferences
        ],
        
        ApplicationMode.ANALYTICS: [
            MemoryType.LONG_TERM_MEMORY,  # Historical data
            MemoryType.TOOLBOX,          # Analysis tools
            MemoryType.WORKFLOW_MEMORY,   # Analysis processes
            MemoryType.SHARED_MEMORY,     # Results sharing
        ],
    }
    
    # Description for each application mode
    MODE_DESCRIPTIONS = {
        ApplicationMode.WORKFLOW: "Optimized for structured task execution and process automation",
        ApplicationMode.DEEP_RESEARCH: "Designed for intensive research with collaboration capabilities",
        ApplicationMode.ASSISTANT: "General-purpose conversational assistant with personalization",
        ApplicationMode.COLLABORATION: "Multi-agent collaboration with shared knowledge",
        ApplicationMode.LEARNING: "Educational scenarios with adaptive learning support",
        ApplicationMode.ANALYTICS: "Data analysis and business intelligence applications",
    }
    
    @classmethod
    def get_memory_types(cls, mode: ApplicationMode) -> List[MemoryType]:
        """
        Get the memory types associated with an application mode.
        
        Parameters:
        -----------
        mode : ApplicationMode
            The application mode to get memory types for.
            
        Returns:
        --------
        List[MemoryType]
            List of memory types for the specified mode.
        """
        return cls.MEMORY_TYPE_MAPPINGS.get(mode, cls.MEMORY_TYPE_MAPPINGS[ApplicationMode.DEFAULT])
    
    @classmethod
    def get_description(cls, mode: ApplicationMode) -> str:
        """
        Get the description for an application mode.
        
        Parameters:
        -----------
        mode : ApplicationMode
            The application mode to get description for.
            
        Returns:
        --------
        str
            Description of the application mode.
        """
        return cls.MODE_DESCRIPTIONS.get(mode, "General-purpose application mode")
    
    @classmethod
    def list_all_modes(cls) -> List[tuple]:
        """
        List all available application modes with their descriptions.
        
        Returns:
        --------
        List[tuple]
            List of (mode, description) tuples.
        """
        return [(mode, cls.get_description(mode)) for mode in ApplicationMode]
    
    @classmethod
    def validate_mode(cls, mode_input) -> ApplicationMode:
        """
        Validate and convert a string or enum to ApplicationMode enum.
        
        Parameters:
        -----------
        mode_input : str | ApplicationMode
            String representation or enum of the application mode.
            
        Returns:
        --------
        ApplicationMode
            The corresponding ApplicationMode enum.
            
        Raises:
        -------
        ValueError
            If the mode input is not valid.
        """
        # If it's already an ApplicationMode enum, return it directly
        if isinstance(mode_input, ApplicationMode):
            return mode_input
            
        # If it's a string, convert it
        if isinstance(mode_input, str):
            try:
                return ApplicationMode(mode_input.lower())
            except ValueError:
                valid_modes = [mode.value for mode in ApplicationMode]
                raise ValueError(f"Invalid application mode: '{mode_input}'. Valid modes: {valid_modes}")
        
        # If it's neither string nor enum, raise an error
        raise ValueError(f"Application mode must be a string or ApplicationMode enum, got {type(mode_input)}")


================================================
FILE: src/memorizz/memory_component/conversational_memory_component.py
================================================
from pydantic import BaseModel
from typing import Optional

class ConversationMemoryComponent(BaseModel):
    role: str
    content: str
    timestamp: str
    memory_id: str
    conversation_id: str
    embedding: list[float]
    recall_recency: Optional[float] = None
    associated_conversation_ids: Optional[list[str]] = None




================================================
FILE: src/memorizz/memory_component/memory_component.py
================================================
from .conversational_memory_component import ConversationMemoryComponent
from ..memory_provider import MemoryProvider
from ..memory_provider.memory_type import MemoryType
from .application_mode import ApplicationMode, ApplicationModeConfig
from ..embeddings.openai import get_embedding
from typing import TYPE_CHECKING, Dict, Any, List, Optional
import time
import numpy as np
import pprint

# Use lazy initialization for OpenAI
def get_openai_llm():
    from ..llms.openai import OpenAI
    return OpenAI()

class MemoryComponent:
    def __init__(self, application_mode: str, memory_provider: MemoryProvider = None):
        # Validate and set the application mode
        if isinstance(application_mode, str):
            self.application_mode = ApplicationModeConfig.validate_mode(application_mode)
        else:
            self.application_mode = application_mode
            
        self.memory_provider = memory_provider
        self.query_embedding = None
        
        # Get the memory types for this application mode
        self.active_memory_types = ApplicationModeConfig.get_memory_types(self.application_mode)

    def generate_memory_component(self, content: dict):
        """
        Generate the memory component based on the application mode.
        The memory component type is determined by the content and active memory types.
        """

        # Generate the embedding of the memory component
        content["embedding"] = get_embedding(content["content"])

        # Determine the appropriate memory component type based on content and active memory types
        if MemoryType.CONVERSATION_MEMORY in self.active_memory_types and "role" in content:
            return self._generate_conversational_memory_component(content)
        elif MemoryType.WORKFLOW_MEMORY in self.active_memory_types:
            return self._generate_workflow_memory_component(content)
        elif MemoryType.LONG_TERM_MEMORY in self.active_memory_types:
            return self._generate_knowledge_base_component(content)
        else:
            # Default to conversational if available, otherwise use the first active memory type
            if MemoryType.CONVERSATION_MEMORY in self.active_memory_types:
                return self._generate_conversational_memory_component(content)
            else:
                raise ValueError(f"No suitable memory component type for application mode: {self.application_mode.value}")

    def _generate_conversational_memory_component(self, content: dict) -> ConversationMemoryComponent:
        """
        Generate the conversational memory component.
        
        Parameters:
            content (dict): The content of the memory component.

        Returns:
            ConversationMemoryComponent: The conversational memory component.
        """
        memory_component = ConversationMemoryComponent(
            role=content["role"],
            content=content["content"],
            timestamp=content["timestamp"],
            conversation_id=content["conversation_id"],
            memory_id=content["memory_id"],
            embedding=content["embedding"]
        )

        # Save the memory component to the memory provider
        self._save_memory_component(memory_component, MemoryType.CONVERSATION_MEMORY)

        return memory_component

    def _generate_workflow_memory_component(self, content: dict):
        """
        Generate a workflow memory component.
        
        Parameters:
            content (dict): The content of the memory component.
            
        Returns:
            dict: The workflow memory component.
        """
        workflow_component = {
            "content": content["content"],
            "timestamp": content.get("timestamp", time.time()),
            "memory_id": content["memory_id"],
            "embedding": content["embedding"],
            "component_type": "workflow",
            "workflow_step": content.get("workflow_step", "unknown"),
            "task_id": content.get("task_id"),
        }
        
        # Save the memory component to the memory provider
        self._save_memory_component(workflow_component, MemoryType.WORKFLOW_MEMORY)
        
        return workflow_component

    def _generate_knowledge_base_component(self, content: dict):
        """
        Generate a knowledge base (long-term memory) component.
        
        Parameters:
            content (dict): The content of the memory component.
            
        Returns:
            dict: The knowledge base memory component.
        """
        knowledge_component = {
            "content": content["content"],
            "timestamp": content.get("timestamp", time.time()),
            "memory_id": content["memory_id"],
            "embedding": content["embedding"],
            "component_type": "knowledge",
            "category": content.get("category", "general"),
            "importance": content.get("importance", 0.5),
        }
        
        # Save the memory component to the memory provider
        self._save_memory_component(knowledge_component, MemoryType.LONG_TERM_MEMORY)
        
        return knowledge_component
    
    def _save_memory_component(self, memory_component: any, memory_type: MemoryType = None):
        """
        Save the memory component to the memory provider.
        
        Parameters:
            memory_component: The memory component to save
            memory_type: Specific memory type to save to (optional)
        """

        # Remove the score(vector similarity score calculated by the vector search of the memory provider) from the memory component if it exists
        if isinstance(memory_component, dict) and "score" in memory_component:
            memory_component.pop("score", None)

        # Convert Pydantic model to dictionary if needed
        if hasattr(memory_component, 'model_dump'):
            memory_component_dict = memory_component.model_dump()
        elif hasattr(memory_component, 'dict'):
            memory_component_dict = memory_component.dict()
        else:
            # If it's already a dictionary, use it as is
            memory_component_dict = memory_component

        # If memory_type is not specified, determine from the component or use conversation as default
        if memory_type is None:
            if MemoryType.CONVERSATION_MEMORY in self.active_memory_types:
                memory_type = MemoryType.CONVERSATION_MEMORY
            else:
                # Use the first available memory type from active types
                memory_type = self.active_memory_types[0] if self.active_memory_types else MemoryType.CONVERSATION_MEMORY

        # Validate that the memory type is active for this application mode
        if memory_type not in self.active_memory_types:
            print(f"Warning: Memory type {memory_type.value} not active for application mode {self.application_mode.value}")

        print(f"Storing memory component of type {memory_type.value} in memory provider")
        print(f"Memory component data: {memory_component_dict}")
        stored_id = self.memory_provider.store(memory_component_dict, memory_type)
        print(f"Stored memory component with ID: {stored_id}")
        return stored_id

    def retrieve_memory_components_by_memory_id(self, memory_id: str, memory_type: MemoryType):
        """
        Retrieve the memory components by memory id.

        Parameters:
            memory_id (str): The id of the memory to retrieve the memory components for.
            memory_type (MemoryType): The type of the memory to retrieve the memory components for.

        Returns:
            List[MemoryComponent]: The memory components.
        """
        if memory_type == MemoryType.CONVERSATION_MEMORY:
            return self.memory_provider.retrieve_conversation_history_ordered_by_timestamp(memory_id)
        elif memory_type == MemoryType.TASK_MEMORY:
            return self.memory_provider.retrieve_task_history_ordered_by_timestamp(memory_id)
        elif memory_type == MemoryType.WORKFLOW_MEMORY:
            return self.memory_provider.retrieve_workflow_history_ordered_by_timestamp(memory_id)
        else:
            raise ValueError(f"Invalid memory type: {memory_type}")

    def retrieve_memory_components_by_conversation_id(self, conversation_id: str):
        pass

    def retrieve_memory_components_by_query(self, query: str, memory_id: str, memory_type: MemoryType, limit: int = 5):
        """
        Retrieve the memory components by query.

        Parameters:
            query (str): The query to use for retrieval.
            memory_id (str): The id of the memory to retrieve the memory components for.
            memory_type (MemoryType): The type of the memory to retrieve the memory components for.
            limit (int): The limit of the memory components to return.

        Returns:
            List[MemoryComponent]: The memory components.
        """

        # Create the query embedding here so that it is not created for each memory component
        self.query_embedding = get_embedding(query)

        # Get the memory components by query
        memory_components = self.memory_provider.retrieve_memory_components_by_query(query, self.query_embedding, memory_id, memory_type, limit)

        # Get the surronding conversation ids from each of the memory components
        # Handle cases where conversation_id might be missing or _id is used instead
        surrounding_conversation_ids = []
        for memory_component in memory_components:
            surrounding_conversation_ids.append(memory_component["_id"])

        # Before returning the memory components, we need to update the memory signals within the memory components
        for memory_component in memory_components:
            self.update_memory_signals_within_memory_component(memory_component, memory_type, surrounding_conversation_ids)

        # Calculate the memory signal for each of the memory components
        for memory_component in memory_components:
            memory_component["memory_signal"] = self.calculate_memory_signal(memory_component, query)

        # Sort the memory components by the memory signal
        memory_components.sort(key=lambda x: x["memory_signal"], reverse=True)

        # Return the memory components
        return memory_components
    

    def update_memory_signals_within_memory_component(self, memory_component: any, memory_type: MemoryType, surrounding_conversation_ids: list[str]):
        """
        Update the memory signal within the memory component.

        Parameters:
            memory_component (dict): The memory component to update the memory signal within.
            memory_type (MemoryType): The type of the memory to update the memory signal within.
            surrounding_conversation_ids (list[str]): The list of surrounding conversation ids.
        """

        # Update the recall_recency field (how recently the memory component was recalled), this is the current timestamp
        memory_component["recall_recency"] = time.time()

        if memory_type == MemoryType.CONVERSATION_MEMORY:
            # Update the importance field with a list of calling ID and surronding conversation ID's
            memory_component["associated_conversation_ids"] = surrounding_conversation_ids

        # Save the memory component to the memory provider
        self._save_memory_component(memory_component)

    def calculate_memory_signal(self, memory_component: any, query: str):
        """
        Calculate the memory signal within the memory component.

        Parameters:
            memory_component (any): The memory component to calculate the memory signal within.
            query (str): The query to use for calculation.

        Returns:
            float: The memory signal between 0 and 1.
        """
        # Detect the gap between the current timestamp and the recall_recency field
        recency = time.time() - memory_component["recall_recency"]

        # Get the number of associated memory ids (this is used to calcualte the importance of the memory component)
        number_of_associated_conversation_ids = len(memory_component["associated_conversation_ids"])

        # If the score exists, use it as the relevance score (this is the vector similarity score calculated by the vector search of the memory provider)
        if "score" in memory_component:
            relevance = memory_component["score"]
        else:
            # Calculate the relevance of the memory component which is a vector score between the memory component and the query
            relevance = self.calculate_relevance(query, memory_component)

        # Calulate importance of the memory component
        importance = self.calculate_importance(memory_component["content"], query)

        # Calculate the normalized memory signal
        memory_signal = recency * number_of_associated_conversation_ids * relevance * importance

        # Normalize the memory signal between 0 and 1
        memory_signal = memory_signal / 100

        # Return the memory signal
        return memory_signal

    def calculate_relevance(self, query: str, memory_component: any) -> float:
        """
        Calculate the relevance of the query with the memory component.

        Parameters:
            query (str): The query to use for calculation.
            memory_component (any): The memory component to calculate the relevance within.

        Returns:
            float: The relevance between 0 and 1.
        """
        # Get embedding of the query
        if self.query_embedding is None:
            self.query_embedding = get_embedding(query)

        # Get embedding of the memory component if it is not already embedded
        if memory_component["embedding"] is None:
            memory_component_embedding = get_embedding(memory_component["content"])
        else:
            memory_component_embedding = memory_component["embedding"]

        # Calculate the cosine similarity between the query embedding and the memory component embedding
        relevance = self.cosine_similarity(self.query_embedding, memory_component_embedding)

        # Return the relevance
        return relevance
        

    # We might not need this as the memory compoennt should have a score from retrieval
    def cosine_similarity(self, query_embedding: list[float], memory_component_embedding: list[float]) -> float:
        """
        Calculate the cosine similarity between two embeddings.

        Parameters:
            query_embedding (list[float]): The query embedding.
            memory_component_embedding (list[float]): The memory component embedding.

        Returns:
            float: The cosine similarity between the two embeddings.
        """
        # Calculate the dot product of the two embeddings
        dot_product = np.dot(query_embedding, memory_component_embedding)

        # Calculate the magnitude of the two embeddings
        magnitude_query_embedding = np.linalg.norm(query_embedding)
        magnitude_memory_component_embedding = np.linalg.norm(memory_component_embedding)

        # Calculate the cosine similarity
        cosine_similarity = dot_product / (magnitude_query_embedding * magnitude_memory_component_embedding)

        # Return the cosine similarity
        return cosine_similarity


    def calculate_importance(self, memory_component_content: str, query: str) -> float:
        """
        Calculate the importance of the memory component.
        Using an LLM to calculate the importance of the memory component.

        Parameters:
            memory_component_content (str): The content of the memory component to calculate the importance within.
            query (str): The query to use for calculation.

        Returns:
            float: The importance between 0 and 1.
        """
   

        importance_prompt = f"""
        Calculate the importance of the following memory component:
        {memory_component_content}
        in relation to the following query and rate the likely poignancy of the memory component:
        {query}
        Return the importance of the memory component as a number between 0 and 1.
        """

        # Get the importance of the memory component
        importance = get_openai_llm().generate_text(importance_prompt, instructions="Return the importance of the memory component as a number between 0 and 1. No other text or comments, just the number. For example: 0.5")

        # Return the importance
        return float(importance)




================================================
FILE: src/memorizz/memory_component/summary_component.py
================================================
"""
Summary Component for MemAgent

Provides a structured approach to working with memory summaries that compress
multiple memory components into emotionally and situationally relevant content.
"""

from pydantic import BaseModel
from typing import Optional, List
import time
from datetime import datetime


class SummaryComponent(BaseModel):
    """
    A structured representation of a memory summary.
    
    Summaries compress multiple memory components from a time period into
    emotionally and situationally relevant content using an LLM.
    """
    
    memory_id: str
    agent_id: str
    summary_content: str
    period_start: float
    period_end: float
    memory_components_count: int
    created_at: float
    embedding: Optional[List[float]] = None
    
    # Optional metadata
    summary_type: str = "automatic"  # automatic, manual, scheduled
    compression_ratio: Optional[float] = None  # original_count / summarized_count
    emotional_tags: Optional[List[str]] = None  # emotional themes identified
    situational_tags: Optional[List[str]] = None  # situational contexts
    importance_score: Optional[float] = None  # 0.0 to 1.0 relevance score
    
    def __init__(self, **data):
        """Initialize summary component with current timestamp if not provided."""
        if 'created_at' not in data:
            data['created_at'] = time.time()
        super().__init__(**data)
    
    @property
    def period_start_datetime(self) -> datetime:
        """Get period start as a datetime object."""
        return datetime.fromtimestamp(self.period_start)
    
    @property
    def period_end_datetime(self) -> datetime:
        """Get period end as a datetime object."""
        return datetime.fromtimestamp(self.period_end)
    
    @property
    def created_datetime(self) -> datetime:
        """Get creation time as a datetime object."""
        return datetime.fromtimestamp(self.created_at)
    
    @property
    def period_duration_hours(self) -> float:
        """Get the duration of the summarized period in hours."""
        return (self.period_end - self.period_start) / 3600
    
    def to_dict(self) -> dict:
        """Convert to dictionary for storage."""
        return self.model_dump()
    
    @classmethod
    def from_dict(cls, data: dict) -> 'SummaryComponent':
        """Create from dictionary loaded from storage."""
        return cls(**data)
    
    def get_short_preview(self, max_length: int = 100) -> str:
        """Get a short preview of the summary content."""
        if len(self.summary_content) <= max_length:
            return self.summary_content
        return self.summary_content[:max_length] + "..."
    
    def add_emotional_tag(self, tag: str):
        """Add an emotional tag to the summary."""
        if self.emotional_tags is None:
            self.emotional_tags = []
        if tag not in self.emotional_tags:
            self.emotional_tags.append(tag)
    
    def add_situational_tag(self, tag: str):
        """Add a situational tag to the summary."""
        if self.situational_tags is None:
            self.situational_tags = []
        if tag not in self.situational_tags:
            self.situational_tags.append(tag)
    
    def calculate_compression_ratio(self, original_memory_count: int):
        """Calculate and set the compression ratio."""
        if original_memory_count > 0:
            self.compression_ratio = original_memory_count / 1  # Summary is 1 compressed item
    
    def __str__(self) -> str:
        """String representation of the summary."""
        return f"Summary({self.memory_id}, {self.period_start_datetime.strftime('%Y-%m-%d')} to {self.period_end_datetime.strftime('%Y-%m-%d')}, {self.memory_components_count} memories)"
    
    def __repr__(self) -> str:
        """Detailed string representation."""
        return f"SummaryComponent(memory_id='{self.memory_id}', agent_id='{self.agent_id}', period='{self.period_start_datetime}' to '{self.period_end_datetime}', memories={self.memory_components_count})"


class SummaryMetrics(BaseModel):
    """
    Metrics and analytics for summary generation and usage.
    """
    
    total_summaries: int = 0
    total_memories_compressed: int = 0
    average_compression_ratio: float = 0.0
    most_common_emotional_tags: List[str] = []
    most_common_situational_tags: List[str] = []
    persona_updates_triggered: int = 0
    
    def add_summary(self, summary: SummaryComponent):
        """Add a summary to the metrics."""
        self.total_summaries += 1
        self.total_memories_compressed += summary.memory_components_count
        
        if summary.compression_ratio:
            current_total = self.average_compression_ratio * (self.total_summaries - 1)
            self.average_compression_ratio = (current_total + summary.compression_ratio) / self.total_summaries
    
    def get_compression_efficiency(self) -> float:
        """Get overall compression efficiency."""
        if self.total_summaries == 0:
            return 0.0
        return self.total_memories_compressed / self.total_summaries 


================================================
FILE: src/memorizz/memory_provider/__init__.py
================================================
from .base import MemoryProvider
from .mongodb import MongoDBProvider
from .memory_type import MemoryType

__all__ = [
    'MemoryProvider',
    'MongoDBProvider',
    'MemoryType'
]


================================================
FILE: src/memorizz/memory_provider/base.py
================================================
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, TYPE_CHECKING

# Use TYPE_CHECKING for forward references to avoid circular imports
if TYPE_CHECKING:
    from memorizz.memagent import MemAgent

class MemoryProvider(ABC):
    """Abstract base class for memory providers."""
    
    @abstractmethod
    def __init__(self, config: Dict[str, Any]):
        """Initialize the memory provider with configuration settings."""
        pass

    @abstractmethod
    def store(self, data: Dict[str, Any], memory_store_type: str) -> str:
        """Store data in the memory provider."""
        pass

    @abstractmethod
    def retrieve_by_query(self, query: Dict[str, Any], memory_store_type: str, limit: int = 1) -> Optional[Dict[str, Any]]:
        """Retrieve a document from the memory provider."""
        pass

    @abstractmethod
    def retrieve_by_id(self, id: str, memory_store_type: str) -> Optional[Dict[str, Any]]:
        """Retrieve a document from the memory provider by id."""
        pass

    @abstractmethod
    def retrieve_by_name(self, name: str, memory_store_type: str) -> Optional[Dict[str, Any]]:
        """Retrieve a document from the memory provider by name."""
        pass

    @abstractmethod
    def delete_by_id(self, id: str, memory_store_type: str) -> bool:
        """Delete a document from the memory provider by id."""
        pass

    @abstractmethod
    def delete_by_name(self, name: str, memory_store_type: str) -> bool:
        """Delete a document from the memory provider by name."""
        pass

    @abstractmethod
    def delete_all(self, memory_store_type: str) -> bool:
        """Delete all documents within a memory store type in the memory provider."""
        pass

    @abstractmethod
    def list_all(self, memory_store_type: str) -> List[Dict[str, Any]]:
        """List all documents within a memory store type in the memory provider."""
        pass

    @abstractmethod
    def retrieve_conversation_history_ordered_by_timestamp(self, memory_id: str) -> List[Dict[str, Any]]:
        """Retrieve the conversation history ordered by timestamp."""
        pass

    @abstractmethod
    def update_by_id(self, id: str, data: Dict[str, Any], memory_store_type: str) -> bool:
        """Update a document in a memory store type in the memory provider by id."""
        pass

    @abstractmethod
    def close(self) -> None:
        """Close the connection to the memory provider."""
        pass 

    @abstractmethod
    def store_memagent(self, memagent: "MemAgent") -> str:
        """Store a memagent in the memory provider."""
        pass
    
    @abstractmethod
    def delete_memagent(self, agent_id: str, cascade: bool = False) -> bool:
        """Delete a memagent from the memory provider."""
        pass

    @abstractmethod
    def update_memagent_memory_ids(self, agent_id: str, memory_ids: List[str]) -> bool:
        """Update the memory_ids of a memagent in the memory provider."""
        pass
        
    @abstractmethod
    def delete_memagent_memory_ids(self, agent_id: str) -> bool:
        """Delete the memory_ids of a memagent in the memory provider."""
        pass

    @abstractmethod
    def list_memagents(self) -> List[Dict[str, Any]]:
        """List all memagents in the memory provider."""
        pass


================================================
FILE: src/memorizz/memory_provider/memory_type.py
================================================
from enum import Enum

class MemoryType(Enum):
    """Enum for different types of memory stores."""
    PERSONAS = "personas"
    TOOLBOX = "toolbox"
    SHORT_TERM_MEMORY = "short_term_memory"
    LONG_TERM_MEMORY = "long_term_memory"
    CONVERSATION_MEMORY = "conversation_memory"
    WORKFLOW_MEMORY = "workflow_memory"
    MEMAGENT = "agents"
    SHARED_MEMORY = "shared_memory"
    SUMMARIES = "summaries"




================================================
FILE: src/memorizz/memory_provider/mongodb/__init__.py
================================================
from .provider import MongoDBProvider

__all__ = [
    'MongoDBProvider',
]


================================================
FILE: src/memorizz/memory_provider/mongodb/provider.py
================================================

import time
import logging
from bson import ObjectId
from pymongo import MongoClient
from ..base import MemoryProvider
from dataclasses import dataclass
from ..memory_type import MemoryType
from ...memagent import MemAgentModel
from ...persona.persona import Persona
from ...persona.role_type import RoleType
from typing import Dict, Any, Optional, List
from pymongo.operations import SearchIndexModel
from ...embeddings.openai import get_embedding, get_embedding_dimensions

logger = logging.getLogger(__name__)

@dataclass
class MongoDBConfig():
    """Configuration for the MongoDB provider."""

    def __init__(self, uri: str, db_name: str = "memorizz"):
        """
        Initialize the MongoDB provider with configuration settings.
        
        Parameters:
        -----------
        uri : str
            The MongoDB URI.
        db_name : str
            The database name.
        """
        self.uri = uri
        self.db_name = db_name


class MongoDBProvider(MemoryProvider):
    """MongoDB implementation of the MemoryProvider interface."""
    
    def __init__(self, config: MongoDBConfig):
        """
        Initialize the MongoDB provider with configuration settings.
        
        Parameters:
        -----------
        config : MongoDBConfig
            Configuration dictionary containing:
            - 'uri': MongoDB URI
            - 'db_name': Database name
        """
        self.config = config
        self.client = MongoClient(config.uri)
        self.db = self.client[config.db_name]
        self.persona_collection = self.db[MemoryType.PERSONAS.value]
        self.toolbox_collection = self.db[MemoryType.TOOLBOX.value]
        self.short_term_memory_collection = self.db[MemoryType.SHORT_TERM_MEMORY.value]
        self.long_term_memory_collection = self.db[MemoryType.LONG_TERM_MEMORY.value]
        self.conversation_memory_collection = self.db[MemoryType.CONVERSATION_MEMORY.value]
        self.workflow_memory_collection = self.db[MemoryType.WORKFLOW_MEMORY.value]
        self.memagent_collection = self.db[MemoryType.MEMAGENT.value]
        self.shared_memory_collection = self.db[MemoryType.SHARED_MEMORY.value]
        self.summaries_collection = self.db[MemoryType.SUMMARIES.value]

        # Create all memory stores in MongoDB.
        self._create_memory_stores()

        # Create a vector index for each memory store in MongoDB.
        self._create_vector_indexes_for_memory_stores()

    def _create_memory_stores(self) -> None:
        """
        Create all memory stores in MongoDB.
        """
        self._create_memory_store(MemoryType.MEMAGENT)
        self._create_memory_store(MemoryType.PERSONAS)
        self._create_memory_store(MemoryType.TOOLBOX)
        self._create_memory_store(MemoryType.SHORT_TERM_MEMORY)
        self._create_memory_store(MemoryType.LONG_TERM_MEMORY)
        self._create_memory_store(MemoryType.CONVERSATION_MEMORY)
        self._create_memory_store(MemoryType.WORKFLOW_MEMORY)
        self._create_memory_store(MemoryType.SHARED_MEMORY)
        self._create_memory_store(MemoryType.SUMMARIES)
    
    def _create_memory_store(self, memory_store_type: MemoryType) -> None:
        """
        Create a new memory store in MongoDB.

        Parameters:
        -----------
        memory_store_type : MemoryType
            The type of memory store to create.

        Returns:
        --------
        None
        """

        # Create collection if it doesn't exist within the database/memory provider
        # Check if the collection exists within the database and if it doesn't, create an empty collection
        for memory_store_type in MemoryType:
            if memory_store_type.value not in self.db.list_collection_names():
                self.db.create_collection(memory_store_type.value)
                print(f"Created collection: {memory_store_type.value} successfully.")
        

    def _create_vector_indexes_for_memory_stores(self) -> None:
        """
        Create a vector index for each memory store in MongoDB.

        Returns:
        --------
        None
        """
        # Create vector indexes for all memory store types
        for memory_store_type in MemoryType:
            # PERSONAS collection doesn't need memory_id filter since it's not memory-scoped
            memory_store_present = memory_store_type != MemoryType.PERSONAS

            self._ensure_vector_index(
                collection=self.db[memory_store_type.value],
                index_name="vector_index",
                memory_store=memory_store_present,
            )
            
    def store(self, data: Dict[str, Any], memory_store_type: MemoryType) -> str:
        """
        Store data in MongoDB using only _id field as primary key.
        
        Parameters:
        -----------
        data : Dict[str, Any]
            The document to be stored.
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)
        
        Returns:
        --------
        str
            The ID of the inserted/updated document (MongoDB _id).
        """
        # Get the appropriate collection based on memory type
        collection = None
        if memory_store_type == MemoryType.PERSONAS:
            collection = self.persona_collection
        elif memory_store_type == MemoryType.TOOLBOX:
            collection = self.toolbox_collection
        elif memory_store_type == MemoryType.WORKFLOW_MEMORY:
            collection = self.workflow_memory_collection
        elif memory_store_type == MemoryType.SHORT_TERM_MEMORY:
            collection = self.short_term_memory_collection
        elif memory_store_type == MemoryType.LONG_TERM_MEMORY:
            collection = self.long_term_memory_collection
        elif memory_store_type == MemoryType.CONVERSATION_MEMORY:
            collection = self.conversation_memory_collection
        elif memory_store_type == MemoryType.SHARED_MEMORY:
            collection = self.shared_memory_collection
        elif memory_store_type == MemoryType.SUMMARIES:
            collection = self.summaries_collection

        if collection is None:
            raise ValueError(f"Invalid memory store type: {memory_store_type}")

        # Clean data by removing custom ID fields - only use MongoDB _id
        # Note: conversation_id is preserved for CONVERSATION_MEMORY as it serves a functional purpose
        data_copy = data.copy()
        
        # Remove custom ID fields since we only want to use _id
        custom_id_fields = [
            "persona_id", "tool_id", "workflow_id", "short_term_memory_id", 
            "long_term_memory_id", "agent_id"
        ]
        
        # Don't remove conversation_id for conversation memory
        if memory_store_type != MemoryType.CONVERSATION_MEMORY:
            custom_id_fields.append("conversation_id")
            
        for field in custom_id_fields:
            data_copy.pop(field, None)
        
        # If document has MongoDB _id, update it
        if "_id" in data_copy:
            result = collection.update_one(
                {"_id": data_copy["_id"]},
                {"$set": data_copy},
                upsert=True
            )
            return str(data_copy["_id"])
        else:
            # For new documents, let MongoDB generate _id automatically
            result = collection.insert_one(data_copy)
            return str(result.inserted_id)

    def retrieve_by_query(self, query: Dict[str, Any], memory_store_type: MemoryType, limit: int = 1) -> Optional[Dict[str, Any]]:
        """
        Retrieve a document from MongoDB.
        
        Parameters:
        -----------
        query : Dict[str, Any]
            The query to use for retrieval.
        limit : int
            The maximum number of documents to return.
        
        Returns:
        --------
        Optional[Dict[str, Any]]
            The retrieved document, or None if not found.
        """
        
        if memory_store_type == MemoryType.PERSONAS:
            return self.retrieve_persona_by_query(query, limit=limit)
        elif memory_store_type == MemoryType.TOOLBOX:
            return self.retrieve_toolbox_item(query, limit)
        elif memory_store_type == MemoryType.WORKFLOW_MEMORY:
            return self.retrieve_workflow_by_query(query, limit)
        elif memory_store_type == MemoryType.SHORT_TERM_MEMORY:
            return self.short_term_memory_collection.find(query, limit=limit)
        elif memory_store_type == MemoryType.LONG_TERM_MEMORY:
            return self.long_term_memory_collection.find(query, limit=limit)
        elif memory_store_type == MemoryType.CONVERSATION_MEMORY:
            return self.conversation_memory_collection.find(query, limit=limit)
        elif memory_store_type == MemoryType.SUMMARIES:
            return self.retrieve_summaries_by_query(query, limit)
       
    def retrieve_by_id(self, id: str, memory_store_type: MemoryType) -> Optional[Dict[str, Any]]:
        """
        Retrieve a document from MongoDB by _id.

        Parameters:
        -----------
        id : str
            The MongoDB _id of the document to retrieve.
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)

        Returns:
        --------
        Optional[Dict[str, Any]]
            The retrieved document, or None if not found.
        """
        # Get the appropriate collection
        collection_mapping = {
            MemoryType.PERSONAS: self.persona_collection,
            MemoryType.TOOLBOX: self.toolbox_collection,
            MemoryType.WORKFLOW_MEMORY: self.workflow_memory_collection,
            MemoryType.SHORT_TERM_MEMORY: self.short_term_memory_collection,
            MemoryType.LONG_TERM_MEMORY: self.long_term_memory_collection,
            MemoryType.CONVERSATION_MEMORY: self.conversation_memory_collection,
            MemoryType.SHARED_MEMORY: self.shared_memory_collection,
            MemoryType.SUMMARIES: self.summaries_collection
        }
        
        collection = collection_mapping.get(memory_store_type)
        if collection is None:
            return None
            
        # Set projection to exclude embedding for performance
        projection = {"embedding": 0} if memory_store_type in [
            MemoryType.PERSONAS, MemoryType.TOOLBOX, MemoryType.WORKFLOW_MEMORY, MemoryType.SUMMARIES
        ] else None
        
        # Retrieve using MongoDB _id only
        try:
            if ObjectId.is_valid(id):
                return collection.find_one({"_id": ObjectId(id)}, projection)
        except Exception:
            pass
            
        return None
    
    def retrieve_by_name(self, name: str, memory_store_type: MemoryType) -> Optional[Dict[str, Any]]:
        """
        Retrieve a document from MongoDB by name.
        
        Parameters:
        -----------
        name : str
            The name of the document to retrieve.
        memory_store_type : MemoryType
            The type of memory store to retrieve from.
        
        Returns:
        --------
        Optional[Dict[str, Any]]
            The retrieved document, or None if not found.
        """
        if memory_store_type == MemoryType.TOOLBOX:
            # Use projection in find_one directly
            return self.toolbox_collection.find_one(
                {"name": name},
                {"embedding": 0}
            )
        elif memory_store_type == MemoryType.PERSONAS:
            return self.persona_collection.find_one({"name": name}, {"embedding": 0})
        elif memory_store_type == MemoryType.WORKFLOW_MEMORY:
            return self.workflow_memory_collection.find_one({"name": name}, {"embedding": 0})
        elif memory_store_type == MemoryType.SHORT_TERM_MEMORY:
            return self.short_term_memory_collection.find_one({"name": name})
        elif memory_store_type == MemoryType.LONG_TERM_MEMORY:
            return self.long_term_memory_collection.find_one({"name": name})
        elif memory_store_type == MemoryType.CONVERSATION_MEMORY:
            return self.conversation_memory_collection.find_one({"name": name})
        elif memory_store_type == MemoryType.SUMMARIES:
            return self.summaries_collection.find_one({"name": name}, {"embedding": 0})
        


    def retrieve_persona_by_query(self, query: Dict[str, Any], limit: int = 1) -> Optional[Dict[str, Any]]:
        """
        Retrieve a persona or several personas from MongoDB.
        This function uses a vector search to retrieve the most similar personas.

        Parameters:
        -----------
        query : Dict[str, Any]

        Returns:
        --------
        Optional[List[Dict[str, Any]]]
            The retrieved personas, or None if not found.
        """

        # Get the embedding for the query
        embedding = get_embedding(query)

        # Create the vector search pipeline
        pipeline = [
            {
                "$vectorSearch": {
                    "queryVector": embedding,
                    "path": "embedding",
                    "numCandidates": 100,
                    "limit": limit,
                    "index": "vector_index"
                }
            },
            {
                "$project": {
                    "_id": 1,
                    "embedding": 0,
                    "score": { "$meta": "vectorSearchScore" }
                }
            }
        ]

        # Execute the vector search
        results = list(self.persona_collection.aggregate(pipeline))

        # Return the results
        return results if results else None
        

    def retrieve_toolbox_item(self, query: Dict[str, Any], limit: int = 1) -> Optional[Dict[str, Any]]:
        """
        Retrieve a toolbox item or several items from MongoDB.
        This function uses a vector search to retrieve the most similar toolbox items.
        Parameters:
        -----------
        query : Dict[str, Any]
            The query to use for retrieval.
        limit : int
            The maximum number of toolbox items to return.
        
        Returns:
        --------
        Optional[List[Dict[str, Any]]]
            The retrieved toolbox items, or None if not found.
        """

        # Get the embedding for the query
        embedding = get_embedding(query)

        # Create the vector search pipeline
        pipeline = [
            {
                "$vectorSearch": {
                    "queryVector": embedding,
                    "path": "embedding",
                    "numCandidates": 100,
                    "limit": limit,
                    "index": "vector_index"
                }
            },
            {
                "$project": {
                    "_id": 1,
                    "embedding": 0,
                    "score": { "$meta": "vectorSearchScore" }
                }
            }
        ]

        # Execute the vector search
        results = list(self.toolbox_collection.aggregate(pipeline))

        # Return the results
        return results if results else None
    
    def retrieve_workflow_by_query(self, query: Dict[str, Any], limit: int = 1) -> Optional[Dict[str, Any]]:
        """
        Retrieve a workflow or several workflows from MongoDB.
        This function uses a vector search to retrieve the most similar workflows.

        Parameters:
        -----------
        query : Dict[str, Any]
            The query to use for retrieval.
        limit : int
            The maximum number of workflows to return.
            
        Returns:
        --------
        Optional[List[Dict[str, Any]]]
            The retrieved workflows, or None if not found.
        """

        # Get the embedding for the query
        embedding = get_embedding(query)

        # Create the vector search pipeline
        pipeline = [
            {
                "$vectorSearch": {
                    "queryVector": embedding,
                    "path": "embedding",
                    "numCandidates": 100,
                    "limit": limit,
                    "index": "vector_index"
                }
            },
            {
                "$project": {
                    "_id": 1,
                    "embedding": 0,
                    "score": { "$meta": "vectorSearchScore" }
                }
            }
        ]

        # Execute the vector search
        results = list(self.workflow_memory_collection.aggregate(pipeline))

        # Return the results
        return results if results else None

    def retrieve_summaries_by_query(self, query: Dict[str, Any], limit: int = 1) -> Optional[Dict[str, Any]]:
        """
        Retrieve summaries by query using vector search.
        
        Parameters:
        -----------
        query : Dict[str, Any]
            The query to use for retrieval.
        limit : int
            The maximum number of summaries to return.
            
        Returns:
        --------
        Optional[List[Dict[str, Any]]]
            The retrieved summaries, or None if not found.
        """
        # Get the embedding for the query
        embedding = get_embedding(query)

        # Create the vector search pipeline
        pipeline = [
            {
                "$vectorSearch": {
                    "queryVector": embedding,
                    "path": "embedding",
                    "numCandidates": 100,
                    "limit": limit,
                    "index": "vector_index"
                }
            },
            {
                "$project": {
                    "_id": 1,
                    "embedding": 0,
                    "score": { "$meta": "vectorSearchScore" }
                }
            }
        ]

        # Execute the vector search
        results = list(self.summaries_collection.aggregate(pipeline))

        # Return the results
        return results if results else None

    def get_summaries_by_memory_id(self, memory_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Retrieve summaries for a specific memory_id, ordered by timestamp (most recent first).
        
        Parameters:
        -----------
        memory_id : str
            The memory_id to retrieve summaries for.
        limit : int
            The maximum number of summaries to return.
            
        Returns:
        --------
        List[Dict[str, Any]]
            List of summaries for the memory_id.
        """
        return list(self.summaries_collection.find(
            {"memory_id": memory_id}, 
            {"embedding": 0}
        ).sort("period_end", -1).limit(limit))

    def get_summaries_by_time_range(self, memory_id: str, start_time: float, end_time: float) -> List[Dict[str, Any]]:
        """
        Retrieve summaries for a specific memory_id within a time range.
        
        Parameters:
        -----------
        memory_id : str
            The memory_id to retrieve summaries for.
        start_time : float
            Start timestamp for the range.
        end_time : float
            End timestamp for the range.
            
        Returns:
        --------
        List[Dict[str, Any]]
            List of summaries within the time range.
        """
        return list(self.summaries_collection.find({
            "memory_id": memory_id,
            "period_start": {"$gte": start_time},
            "period_end": {"$lte": end_time}
        }, {"embedding": 0}).sort("period_start", 1))

    def delete_by_id(self, id: str, memory_store_type: MemoryType) -> bool:
        """
        Delete a document from MongoDB by _id.
        
        Parameters:
        -----------
        id : str
            The MongoDB _id of the document to delete.
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)
        
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        # Get the appropriate collection
        collection_mapping = {
            MemoryType.PERSONAS: self.persona_collection,
            MemoryType.TOOLBOX: self.toolbox_collection,
            MemoryType.WORKFLOW_MEMORY: self.workflow_memory_collection,
            MemoryType.SHORT_TERM_MEMORY: self.short_term_memory_collection,
            MemoryType.LONG_TERM_MEMORY: self.long_term_memory_collection,
            MemoryType.CONVERSATION_MEMORY: self.conversation_memory_collection,
            MemoryType.SHARED_MEMORY: self.shared_memory_collection,
            MemoryType.SUMMARIES: self.summaries_collection
        }
        
        collection = collection_mapping.get(memory_store_type)
        if collection is None:
            return False
            
        # Delete using MongoDB _id only
        try:
            if ObjectId.is_valid(id):
                result = collection.delete_one({"_id": ObjectId(id)})
                return result.deleted_count > 0
        except Exception:
            pass
            
        return False
    
    def delete_by_name(self, name: str, memory_store_type: MemoryType) -> bool:
        """
        Delete a document from MongoDB by name.

        Parameters:
        -----------
        name : str
            The name of the document to delete.
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)
        
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        if memory_store_type == MemoryType.TOOLBOX:
            result = self.toolbox_collection.delete_one({"name": name})
        elif memory_store_type == MemoryType.PERSONAS:
            result = self.persona_collection.delete_one({"name": name})
        elif memory_store_type == MemoryType.SHORT_TERM_MEMORY:
            result = self.short_term_memory_collection.delete_one({"name": name})
        elif memory_store_type == MemoryType.LONG_TERM_MEMORY:
            result = self.long_term_memory_collection.delete_one({"name": name}) 
        elif memory_store_type == MemoryType.CONVERSATION_MEMORY:
            result = self.conversation_memory_collection.delete_one({"name": name})
        elif memory_store_type == MemoryType.WORKFLOW_MEMORY:
            result = self.workflow_memory_collection.delete_one({"name": name})
        elif memory_store_type == MemoryType.SUMMARIES:
            result = self.summaries_collection.delete_one({"name": name})
        else:
            return False
        
        return result.deleted_count > 0

    def delete_all(self, memory_store_type: MemoryType) -> bool:
        """
        Delete all documents within a memory store type in MongoDB.
        
        Parameters:
        -----------
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)
        
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        if memory_store_type == MemoryType.PERSONAS:
            result = self.persona_collection.delete_many({})
        elif memory_store_type == MemoryType.TOOLBOX:
            result = self.toolbox_collection.delete_many({})
        elif memory_store_type == MemoryType.SHORT_TERM_MEMORY:
            result = self.short_term_memory_collection.delete_many({})
        elif memory_store_type == MemoryType.LONG_TERM_MEMORY:
            result = self.long_term_memory_collection.delete_many({})
        elif memory_store_type == MemoryType.CONVERSATION_MEMORY:
            result = self.conversation_memory_collection.delete_many({})
        elif memory_store_type == MemoryType.WORKFLOW_MEMORY:
            result = self.workflow_memory_collection.delete_many({})
        elif memory_store_type == MemoryType.SUMMARIES:
            result = self.summaries_collection.delete_many({})
        else:
            return False
            
        return result.deleted_count > 0
            
    
    def list_all(self, memory_store_type: MemoryType) -> List[Dict[str, Any]]:
        """
        List all documents within a memory store type in MongoDB.

        Parameters:
        -----------
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)
        
        Returns:
        --------
        List[Dict[str, Any]]
            The list of all documents from MongoDB.
        """

        if memory_store_type == MemoryType.PERSONAS:
            return list(self.persona_collection.find({}, {"embedding": 0}))
        elif memory_store_type == MemoryType.TOOLBOX:
            return list(self.toolbox_collection.find({}, {"embedding": 0}))
        elif memory_store_type == MemoryType.SHORT_TERM_MEMORY:
            return list(self.short_term_memory_collection.find())
        elif memory_store_type == MemoryType.LONG_TERM_MEMORY:
            return list(self.long_term_memory_collection.find())
        elif memory_store_type == MemoryType.CONVERSATION_MEMORY:
            return list(self.conversation_memory_collection.find())
        elif memory_store_type == MemoryType.WORKFLOW_MEMORY:
            return list(self.workflow_memory_collection.find())
        elif memory_store_type == MemoryType.SHARED_MEMORY:
            return list(self.shared_memory_collection.find())
        elif memory_store_type == MemoryType.SUMMARIES:
            return list(self.summaries_collection.find({}, {"embedding": 0}))
        else:
            logger.warning(f"Unsupported memory store type for list_all: {memory_store_type}")
            return []
        
    def update_by_id(self, id: str, data: Dict[str, Any], memory_store_type: MemoryType) -> bool:
        """
        Update a document in a memory store type in MongoDB by _id.

        Parameters:
        -----------
        id : str
            The MongoDB _id of the document to update.
        data : Dict[str, Any]
            The data to update the document with.
        memory_store_type : MemoryType
            The type of memory store (e.g., "persona", "toolbox", etc.)
        
        Returns:
        --------
        bool
            True if update was successful, False otherwise.
        """
        # Get the appropriate collection
        collection_mapping = {
            MemoryType.PERSONAS: self.persona_collection,
            MemoryType.TOOLBOX: self.toolbox_collection,
            MemoryType.WORKFLOW_MEMORY: self.workflow_memory_collection,
            MemoryType.SHORT_TERM_MEMORY: self.short_term_memory_collection,
            MemoryType.LONG_TERM_MEMORY: self.long_term_memory_collection,
            MemoryType.CONVERSATION_MEMORY: self.conversation_memory_collection,
            MemoryType.SHARED_MEMORY: self.shared_memory_collection,
            MemoryType.SUMMARIES: self.summaries_collection
        }
        
        collection = collection_mapping.get(memory_store_type)
        if collection is None:
            logger.error(f"No collection mapping found for memory store type: {memory_store_type}")
            return False
            
        # Update using MongoDB _id only
        try:
            if ObjectId.is_valid(id):
                result = collection.update_one({"_id": ObjectId(id)}, {"$set": data})
                success = result.modified_count > 0
                if not success:
                    logger.warning(f"Update operation found no documents to modify for id: {id}")
                return success
            else:
                logger.error(f"Invalid ObjectId: {id}")
                return False
        except Exception as e:
            logger.error(f"Error updating document with id {id}: {e}", exc_info=True)
            return False
            
            
    def update_toolbox_item(self, id: str, data: Dict[str, Any]) -> bool:
        """
        Update a toolbox item in MongoDB by id using optimized queries.
        """

        # Update the embedding if the name, docstring or signature has changed

        # Get the old data
        old_data = self.retrieve_by_id(id, MemoryType.TOOLBOX)
        if not old_data:
            return False

        # Concatenate the name, docstring and signature if any of them have changed
        if old_data.get("name") != data.get("name"):
            data["name"] = data.get("name", old_data.get("name", ""))
        if old_data.get("docstring") != data.get("docstring"):
            data["docstring"] = data.get("docstring", old_data.get("docstring", ""))
        if old_data.get("signature") != data.get("signature"):
            data["signature"] = data.get("signature", old_data.get("signature", ""))

        # Update the embedding
        data["embedding"] = get_embedding(data["name"] + " " + data["docstring"] + " " + data["signature"])

        # Use the optimized update_by_id method
        return self.update_by_id(id, data, MemoryType.TOOLBOX)
    

    def retrieve_conversation_history_ordered_by_timestamp(self, memory_id: str) -> List[Dict[str, Any]]:
        """
        Retrieve the conversation history ordered by timestamp.

        Parameters:
        -----------
        memory_id : str
            The id of the memory to retrieve the conversation history for.

        Returns:
        --------
        List[Dict[str, Any]]
            The conversation history ordered by timestamp.
        """
        return list(self.conversation_memory_collection.find({"memory_id": memory_id}, {"embedding": 0}).sort("timestamp", 1))
    
    def retrieve_memory_components_by_query(self, query: str = None, query_embedding: list[float] = None, memory_id: str = None, memory_type: MemoryType = None, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Retrieve memory components by query.

        Parameters:
        -----------
        query : str
            The query to use for retrieval.
        query_embedding : list[float]
            The embedding of the query.
        memory_id : str
            The id of the memory to retrieve the memory components for.
        memory_type : MemoryType
            The type of memory to retrieve the memory components for.
        limit : int
            The maximum number of memory components to return.

        Returns:
        --------
        List[Dict[str, Any]]
            The memory components ordered by timestamp.
        """

        # Detect the memory type
        if memory_type == MemoryType.CONVERSATION_MEMORY:
            return self.get_conversation_memory_components(query, query_embedding, memory_id, limit)
        elif memory_type == MemoryType.TASK_MEMORY:
            pass
            # TODO: return self.get_task_memory_components(query, memory_id, limit)
        elif memory_type == MemoryType.WORKFLOW_MEMORY:
            return self.get_workflow_memory_components(query, query_embedding, memory_id, limit)


    def get_conversation_memory_components(self, query: str = None, query_embedding: list[float] = None, memory_id: str = None, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get the conversation memory components.

        Parameters:
        -----------
        query : str
            The query to use for retrieval.
        query_embedding : list[float]
            The embedding of the query.
        memory_id : str
            The id of the memory to retrieve the memory components for.
        limit : int
            The maximum number of memory components to return.

        Returns:
        --------
        List[Dict[str, Any]]
            The memory components ordered by timestamp.
        """

        # If the query embedding is not provided, then we create it
        if query_embedding is None and query is not None:
            query_embedding = get_embedding(query)

        vector_stage = {
            "$vectorSearch": {
                "index": "vector_index",
                "queryVector": query_embedding,
                "path": "embedding",
                "numCandidates": 100,
                "limit": limit,
                "filter": {"memory_id": memory_id}
            }
        }

        # Add the vector stage to the pipeline
        pipeline = [
            vector_stage,
            {"$addFields": {"score": {"$meta": "vectorSearchScore"}}},
            {"$sort": {"score": -1, "timestamp": 1}},
        ]

        # Execute the pipeline
        results = list(self.conversation_memory_collection.aggregate(pipeline))

        # Return the results
        return results
    
    def store_memagent(self, memagent: "MemAgentModel") -> "MemAgentModel":
        """
        Store a memagent in the MongoDB database using only _id field.
        
        Parameters:
        -----------
        memagent : MemAgentModel
            The memagent to be stored.
        
        Returns:
        --------
        MemAgentModel
            The stored memagent.
        """
        # Convert the MemAgentModel to a dictionary
        memagent_dict = memagent.model_dump()
        
        # Remove agent_id field since we only want to use _id
        memagent_dict.pop("agent_id", None)
        
        # Convert persona to a serializable format if it exists
        if memagent.persona:
            # Store the entire persona object as a serializable dictionary
            memagent_dict["persona"] = memagent.persona.to_dict()
        
        # Remove any function objects from tools that could cause serialization issues
        if memagent_dict.get("tools") and isinstance(memagent_dict["tools"], list):
            for tool in memagent_dict["tools"]:
                if "function" in tool and callable(tool["function"]):
                    del tool["function"]
        
        # Insert the document and let MongoDB generate _id automatically
        result = self.memagent_collection.insert_one(memagent_dict)
        
        # Add the generated _id to the response
        memagent_dict["_id"] = result.inserted_id

        return memagent_dict
    
    def update_memagent(self, memagent: "MemAgentModel") -> "MemAgentModel":
        """
        Update a memagent in the MongoDB database using _id field.
        """
        # Convert the MemAgentModel to a dictionary
        memagent_dict = memagent.model_dump()

        # Remove agent_id field since we only want to use _id
        agent_id = memagent_dict.pop("agent_id", None)
        
        # Convert persona to a serializable format if it exists
        if memagent.persona:
            memagent_dict["persona"] = memagent.persona.to_dict()
        
        # Remove any function objects from tools that could cause serialization issues
        if memagent_dict.get("tools") and isinstance(memagent_dict["tools"], list):
            for tool in memagent_dict["tools"]:
                if "function" in tool and callable(tool["function"]):
                    del tool["function"]
        
        # Update the memagent in the MongoDB database using _id
        if agent_id and ObjectId.is_valid(agent_id):
            self.memagent_collection.update_one(
                {"_id": ObjectId(agent_id)}, 
                {"$set": memagent_dict}
            )
        
        return memagent_dict

    
    def retrieve_memagent(self, agent_id: str) -> "MemAgentModel":
        """
        Retrieve a memagent from the MongoDB database using _id field.
        
        Parameters:
        -----------
        agent_id : str
            The agent ID to retrieve (MongoDB _id).
        
        Returns:
        --------
        MemAgentModel
            The retrieved memagent.
        """
        # Get the document from MongoDB using _id
        try:
            if ObjectId.is_valid(agent_id):
                document = self.memagent_collection.find_one({"_id": ObjectId(agent_id)})
            else:
                return None
        except Exception:
            return None
        
        if not document:
            return None
        
        # Create a new MemAgent with data from the document
        # Use the MongoDB _id as agent_id since we no longer store agent_id field
        memagent = MemAgentModel(
            instruction=document.get("instruction"),
            application_mode=document.get("application_mode", "assistant"),
            max_steps=document.get("max_steps"),
            memory_ids=document.get("memory_ids") or [],
            agent_id=str(document.get("_id")),
            tools=document.get("tools"),
            memory_provider=self
        )
        
        # Construct persona if present in the document
        if document.get("persona"):
            persona_data = document.get("persona")
            # Handle role as a string by matching it to a RoleType enum
            role_str = persona_data.get("role")
            role = None
            
            # Match the string role to a RoleType enum
            for role_type in RoleType:
                if role_type.value == role_str:
                    role = role_type
                    break
            
            # If no matching enum is found, default to GENERAL
            if role is None:
                role = RoleType.GENERAL
                
            memagent.persona = Persona(
                name=persona_data.get("name"),
                role=role,  # Pass the RoleType enum instead of string
                goals=persona_data.get("goals"),
                background=persona_data.get("background"),
                persona_id=persona_data.get("persona_id")
            )

        return memagent
    

    
    def list_memagents(self) -> List["MemAgentModel"]:
        """
        List all memagents in the MongoDB database.
        
        Returns:
        --------
        List[MemAgentModel]
            The list of memagents.
        """
        
        documents = list(self.memagent_collection.find())
        agents = []
        
        for doc in documents:
            # Use the MongoDB _id as agent_id since we no longer store agent_id field
            agent = MemAgentModel(
                instruction=doc.get("instruction"),
                application_mode=doc.get("application_mode", "assistant"),
                max_steps=doc.get("max_steps"),
                memory_ids=doc.get("memory_ids") or [],
                agent_id=str(doc.get("_id")),
                tools=doc.get("tools"),  # Include tools from document
                memory_provider=self
            )
            
            # Construct persona if present in the document
            if doc.get("persona"):
                persona_data = doc.get("persona")
                # Handle role as a string by matching it to a RoleType enum
                role_str = persona_data.get("role")
                role = None
                
                # Match the string role to a RoleType enum
                for role_type in RoleType:
                    if role_type.value == role_str:
                        role = role_type
                        break
                
                # If no matching enum is found, default to GENERAL
                if role is None:
                    role = RoleType.GENERAL
                    
                agent.persona = Persona(
                    name=persona_data.get("name"),
                    role=role,  # Pass the RoleType enum instead of string
                    goals=persona_data.get("goals"),
                    background=persona_data.get("background"),
                    persona_id=persona_data.get("persona_id")
                )
                
            agents.append(agent)
            
        return agents

    
    def update_memagent_memory_ids(self, agent_id: str, memory_ids: List[str]) -> bool:
        """
        Update the memory_ids of a memagent in the memory provider using _id field.

        Parameters:
        -----------
        agent_id : str
            The id of the memagent to update (MongoDB _id).
        memory_ids : List[str]
            The list of memory_ids to update.

        Returns:
        --------
        bool
            True if update was successful, False otherwise.
        """
        try:
            if ObjectId.is_valid(agent_id):
                result = self.memagent_collection.update_one(
                    {"_id": ObjectId(agent_id)}, 
                    {"$set": {"memory_ids": memory_ids}}
                )
                return result.modified_count > 0
            else:
                return False
        except Exception:
            return False
    
    def delete_memagent_memory_ids(self, agent_id: str) -> bool:
        """
        Delete the memory_ids of a memagent in the memory provider.

        Parameters:
        -----------
        agent_id : str
            The id of the memagent to update (MongoDB _id).

        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        try:
            if ObjectId.is_valid(agent_id):
                result = self.memagent_collection.update_one(
                    {"_id": ObjectId(agent_id)}, 
                    {"$unset": {"memory_ids": []}}
                )
                return result.modified_count > 0
            else:
                return False
        except Exception:
            return False
    
    def delete_memagent(self, agent_id: str, cascade: bool = False) -> bool:
        """
        Delete a memagent from the memory provider by id.

        Parameters:
        -----------
        agent_id : str
            The id of the memagent to delete.
        cascade : bool
            Whether to cascade the deletion of the memagent. This deletes all the memory components associated with the memagent by deleting the memory_ids and their corresponding memory store in the memory provider.

        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        if cascade:
            # Retrieve the memagent
            memagent = self.retrieve_memagent(agent_id)

            if memagent is None:
                raise ValueError(f"MemAgent with id {agent_id} not found")
            
            # Delete all the memory components associated with the memagent by deleting the memory_ids and their corresponding memory store in the memory provider.
            for memory_id in memagent.memory_ids:
                # Loop through all the memory stores and delete records with the memory_ids
                for memory_type in MemoryType:
                    self._delete_memory_components_by_memory_id(memory_id, memory_type)
        else:
            try:
                if ObjectId.is_valid(agent_id):
                    result = self.memagent_collection.delete_one({"_id": ObjectId(agent_id)})
                    return result.deleted_count > 0
                else:
                    return False
            except Exception:
                return False

        return True
    
    def _delete_memory_components_by_memory_id(self, memory_id: str, memory_type: MemoryType):
        """
        Delete all the memory components associated with the memory_id.

        Parameters:
        -----------
        memory_id : str
            The id of the memory to delete.
        memory_type : MemoryType
            The type of memory to delete.

        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        if memory_type == MemoryType.CONVERSATION_MEMORY:
            self.conversation_memory_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.TASK_MEMORY:
            self.task_memory_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.WORKFLOW_MEMORY:
            self.workflow_memory_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.SHORT_TERM_MEMORY:
            self.short_term_memory_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.LONG_TERM_MEMORY:
            self.long_term_memory_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.PERSONAS:
            self.persona_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.TOOLBOX:
            self.toolbox_collection.delete_many({"memory_id": memory_id})
        elif memory_type == MemoryType.MEMAGENT:
            self.memagent_collection.delete_many({"memory_id": memory_id})
                

    def _setup_vector_search_index(self, collection, index_name="vector_index", memory_store: bool = False):
        """
        Setup a vector search index for a MongoDB collection and wait for it to become queryable.

        Args:
        collection: MongoDB collection object
        index_name: Name of the index (default: "vector_index")
        memory_store: Whether to add the memory_id field to the index (default: False)
        """

        # Define the index definition
        vector_index_definition = {
            "fields": [
                {
                    "type": "vector",
                    "path": "embedding",
                    # TODO: Make this dynamic based on the embedding model
                    "numDimensions": get_embedding_dimensions("text-embedding-3-small"),
                    "similarity": "cosine",
                }
            ]
        }

        # If the memory store is true, then we add the memory_id field to the index
        # This is used to prefilter the memory components by memory_id
        # useful to narrow the scope of your semantic search and ensure that not all vectors are considered for comparison. 
        # It reduces the number of documents against which to run similarity comparisons, which can decrease query latency and increase the accuracy of search results.
        if memory_store:
            vector_index_definition["fields"].append({
                "type": "filter",
                "path": "memory_id",
            })

        new_vector_search_index_model = SearchIndexModel(
            definition=vector_index_definition, name=index_name, type="vectorSearch"
        )

        # Create the new index
        try:
            result = collection.create_search_index(model=new_vector_search_index_model)
            print(f"Creating index '{index_name}'... for collection {collection.name}")
            
            # Wait for the index to become queryable using polling mechanism
            self._wait_for_index_ready(collection, result, index_name)
            
            return result

        except Exception as e:
            print(f"Error creating new vector search index '{index_name}': {e!s}")
            return None

    def _wait_for_index_ready(self, collection, index_name_result, display_name="vector_index"):
        """
        Wait for a MongoDB Atlas search index to become queryable using polling.
        
        Args:
        collection: MongoDB collection object
        index_name_result: The name/result returned from create_search_index
        display_name: Human-readable name for logging (default: "vector_index")
        """
        print(f"Polling to check if the index '{index_name_result}' is ready. This may take up to a minute.")
        
        # Define predicate function to check if index is queryable
        predicate = lambda index: index.get("queryable") is True
        
        while True:
            try:
                # List search indexes and find the one we just created
                indices = list(collection.list_search_indexes(index_name_result))
                
                # Check if the index exists and is queryable
                if indices and predicate(indices[0]):
                    break
                    
                # Wait 5 seconds before checking again
                time.sleep(5)
                
            except Exception as e:
                print(f"Error checking index readiness: {e}")
                # Continue polling even if there's an error
                time.sleep(5)
        
        print(f"Index '{index_name_result}' is ready for querying in collection {collection.name}.")
        
    def _ensure_vector_index(self, collection, index_name="vector_index", memory_store: bool = False):
        """
        Ensure a vector search index exists for the collection. If it doesn't exist, create it and wait for it to be ready.
        
        Args:
        collection: MongoDB collection object
        index_name: Name of the index (default: "vector_index")
        memory_store: Whether to add the memory_id field to the index (default: False)
        """
        search_indexes = list(collection.list_search_indexes())
        has_vector_index = any(index.get("name") == index_name and index.get("type") == "vectorSearch" for index in search_indexes)
        
        if not has_vector_index:
            print(f"Vector search index '{index_name}' not found for collection {collection.name}. Creating...")
            self._setup_vector_search_index(collection, index_name, memory_store)
            print(f"Vector index '{index_name}' for {collection.name} collection is now ready for use.")
        else:
            print(f"Vector search index '{index_name}' already exists for collection {collection.name}.")

    def close(self) -> None:
        """Close the connection to MongoDB."""
        self.client.close()


================================================
FILE: src/memorizz/persona/README.md
================================================
# Persona Module

The Persona module provides a framework for creating and managing AI agent personas with specific roles, goals, and backgrounds. This module is part of the Memorizz library, which handles memory management for AI agents.

## Features

- Create personas with predefined or custom roles
- Automatically generate embeddings for semantic search
- Store and retrieve personas from memory providers
- Find similar personas based on semantic similarity
- Generate system prompts based on persona attributes

## Usage

### Creating a Persona

```python
from src.memorizz.persona import Persona
from src.memorizz.memory_provider import MemoryProvider

# Initialize a memory provider
memory_provider = MemoryProvider()

# Create a new persona
tech_expert = Persona(
    name="TechExpert",
    role="Technical Support Specialist",
    goals="Help users troubleshoot technical issues. Provide clear explanations for complex problems.",
    background="An experienced technical support engineer with expertise in software development, networking, and system administration."
)

# Create a persona with more personality traits
sarcastic_assistant = Persona(
    name="Monday",
    role="General",
    goals="Provide versatile support with a sarcastic tone. Add humor to interactions.",
    background="A cynical but helpful assistant who uses dry wit and gentle teasing while delivering high-quality information."
)
```

### Storing Personas

Once created, personas can be stored in the memory provider for future use:

```python
# Store the persona in the memory provider
persona_id = tech_expert.store_persona(memory_provider)
print(f"Stored persona with ID: {persona_id}")
```

### Generating Persona Prompts

Personas can generate system prompts for language models:

```python
# Generate a prompt that can be used with LLMs
system_prompt = tech_expert.generate_system_prompt_input()
print(system_prompt)
```

### Retrieving Personas

Personas can be retrieved by ID:

```python
# Retrieve a persona using its ID
retrieved_persona = Persona.retrieve_persona(persona_id, memory_provider)
print(retrieved_persona)
```

Or by semantic similarity to a query:

```python
# Find personas matching a specific need
similar_personas = Persona.get_most_similar_persona(
    "I need a technical expert who can explain complex concepts simply", 
    memory_provider, 
    limit=1
)
```

### Using Personas with MemAgents

Personas can be assigned to MemAgents to control their behavior:

```python
from src.memorizz.memagent import MemAgent

# Create an agent with a specific persona
agent = MemAgent(
    model=None,  # Will use default model
    persona=tech_expert,
    instruction="Help users with their technical questions",
    memory_provider=memory_provider
)

# Or set/change a persona later
agent.set_persona(sarcastic_assistant)

# Run the agent with its persona influencing responses
response = agent.run("Can you help me fix my computer?")
```

### Persona Persistence

Personas are stored with vector embeddings for efficient retrieval:

```python
# List all available personas
all_personas = memory_provider.list_all(memory_type=MemoryType.PERSONA)

# Delete a persona
memory_provider.delete_by_id(persona_id, memory_type=MemoryType.PERSONA)
```

## Implementation Notes

- Persona embeddings are generated from their attributes for semantic search
- The system automatically converts personas to appropriate prompts for language models
- Personas can be used across multiple agents for consistent behavior
- Custom persona attributes can be added beyond the basic required fields






================================================
FILE: src/memorizz/persona/__init__.py
================================================
from .persona import Persona
from .role_type import RoleType

__all__ = ['Persona', 'RoleType']


================================================
FILE: src/memorizz/persona/persona.py
================================================
import uuid
from typing import Union, Optional, Dict, Any
from enum import Enum
from bson import ObjectId
from datetime import datetime
from ..memory_provider import MemoryProvider
from ..embeddings.openai import get_embedding
from ..memory_provider.memory_type import MemoryType
from .role_type import RoleType, PREDEFINED_INFO


class Persona:
    def __init__(self, name: str, role: Union[RoleType, str] = RoleType.GENERAL, goals: str = "", background: str = "", persona_id: str = None):
        """
        Initialize a Persona instance for an AI agent with a deterministic role.

        Parameters:
        -----------
        name : str
            The name of the persona.
        role : Union[RoleType, str], optional
            A predefined role for the agent (e.g., GENERAL, ASSISTANT, etc.) or the string value of a role. 
            If not provided, the default role of GENERAL will be used.
        goals : str, optional
            Custom goals for the persona. If provided, these are appended to the predefined goals.
        background : str, optional
            Custom background for the persona. If provided, these are appended to the predefined background.
        persona_id : str, optional
            A unique identifier for the persona. If not provided, one will be generated.
        """
        self.name = name
        
        # Handle both RoleType enum and string role values
        if isinstance(role, str):
            role_enum = None
            # Try to match the string to a RoleType enum
            for role_type in RoleType:
                if role_type.value == role:
                    role_enum = role_type
                    break
            # If no match is found, default to GENERAL
            if role_enum is None:
                role_enum = RoleType.GENERAL
                # Store the original string value
                self.role = role
            else:
                self.role = role_enum.value
        else:
            self.role = role.value
            role_enum = role
            
        # Retrieve default goals and background of the role
        default_goals = PREDEFINED_INFO[role_enum]["goals"]
        default_background = PREDEFINED_INFO[role_enum]["background"]
        # Append custom goals and background to the default ones
        self.goals = f"{default_goals} {goals}".strip() if goals else default_goals
        self.background = f"{default_background} {background}".strip() if background else default_background
        
        # Generate or assign persona_id
        self.persona_id = persona_id if persona_id else self.generate_persona_id()
        
        # Generate the embedding based on the concatenated persona details.
        self.embedding = self._generate_embedding()
        
        # Timestamp for creation
        self.created_at = datetime.now().isoformat()

    @staticmethod
    def generate_persona_id() -> str:
        """Generate a unique persona ID optimized for MongoDB."""
        # Generate MongoDB ObjectId for better performance
        return str(ObjectId())

    def _generate_embedding(self):
        """
        Generate an embedding vector for the persona based on its attributes.
        
        Returns:
        --------
        list or numpy.array: The embedding vector representing the persona.
        """
        embedding_input = f"{self.name} {self.role} {self.goals} {self.background}"
        return get_embedding(embedding_input)

    def to_dict(self) -> dict:
        """
        Serialize the Persona into a dictionary format, including the embedding.
        
        Returns:
        --------
        dict: A dictionary representation of the persona.
        """
        return {
            "persona_id": self.persona_id,
            "name": self.name,
            "role": self.role,
            "goals": self.goals,
            "background": self.background,
            "embedding": self.embedding,
            "created_at": self.created_at
        }

    def store_persona(self, provider: MemoryProvider) -> str:
        """
        Store the persona's JSON structure and embedding into the memory provider.
        
        Parameters:
        -----------
        provider : MemoryProvider
            The memory provider to use for storage.
        
        Returns:
        --------
        str
            The ID of the stored persona.
        """
        print(f"Storing persona: {self.name} in the memory provider, in the {MemoryType.PERSONAS.value} collection")
        persona_data = self.to_dict()
        return provider.store(persona_data, memory_store_type=MemoryType.PERSONAS)

    @staticmethod
    def retrieve_persona(persona_id: str, provider: MemoryProvider) -> dict:
        """
        Retrieve a persona from the memory provider by persona_id.
        
        Parameters:
        -----------
        persona_id : str
            The unique identifier of the persona to retrieve.
        provider : MemoryProvider
            The memory provider to use for retrieval.
        
        Returns:
        --------
        dict: The persona's JSON structure including its embedding, or None if not found.
        """
        print(f"Retrieving persona: {persona_id} from the memory provider, in the {MemoryType.PERSONAS.value} collection")
        return provider.retrieve_by_id(persona_id, memory_store_type=MemoryType.PERSONAS)

    @staticmethod
    def delete_persona(persona_id: str, provider: MemoryProvider) -> bool:
        """
        Delete a persona from the memory provider using its persona_id.
        
        Parameters:
        -----------
        persona_id : str
            The unique identifier of the persona to delete.
        provider : MemoryProvider
            The memory provider to use for deletion.
        
        Returns:
        --------
        bool: True if deletion was successful, False otherwise.
        """
        return provider.delete_by_id(persona_id, memory_store_type=MemoryType.PERSONAS)
    
    @staticmethod
    def list_personas(provider: MemoryProvider) -> list:
        """
        List all personas within the memory provider.
        
        Parameters:
        -----------
        provider : MemoryProvider
            The memory provider to use for listing personas.
        
        Returns:
        --------
        list: A list of all personas in the memory provider.
        """
        print(f"Listing all personas in the {MemoryType.PERSONAS.value} collection")
        return provider.list_all(memory_store_type=MemoryType.PERSONAS)
    
    @staticmethod
    def get_most_similar_persona(input: str, provider: MemoryProvider, limit: int = 1) -> dict:
        """
        Get the persona with the most similar embedding to the query.

        Parameters:
        -----------
        input : str
            The input to search for.
        provider : MemoryProvider
            The memory provider to use for retrieval.
        limit : int, optional
            The number of personas to return.
        
        Returns:
        --------
        list: A list of the most similar personas.
        """
        return provider.retrieve_by_query(input, memory_store_type=MemoryType.PERSONAS, limit=limit)
    
    def generate_system_prompt_input(self) -> str:
        """
        Generate a system prompt input based on the persona's goals and background.
        """
        return f"""
            You are {self.name}, and you are a {self.role}. You have the following goals: {self.goals}. Your background is: {self.background}.
        """
    

    def __repr__(self) -> str:
        return (f"Persona(persona_id='{self.persona_id}', name='{self.name}', role='{self.role}', "
                f"goals='{self.goals}', background='{self.background}', embedding=[...])")

# Example usage:
if __name__ == "__main__":
    # Create a Persona instance
    persona = Persona(
        name="Alex",
        role=RoleType.ASSISTANT,
        goals="Focus on proactive notifications and user engagement.",
        background="Alex also has experience in scheduling and reminders."
    )
    print("Initialized Persona:", persona)
    
    # Store the Persona in MongoDB
    stored_id = persona.store_persona()
    print("Stored Persona ID:", stored_id)
    
    # Retrieve the Persona from MongoDB using persona_id
    retrieved_persona = Persona.retrieve_persona(persona.persona_id)
    print("Retrieved Persona:", retrieved_persona)
    
    # Optionally, delete the Persona from MongoDB
    deleted = Persona.delete_persona(persona.persona_id)
    print("Deletion successful:", deleted)



================================================
FILE: src/memorizz/persona/role_type.py
================================================
from enum import Enum

class RoleType(Enum):
    GENERAL = "General"
    ASSISTANT = "Virtual Assistant"
    CUSTOMER_SUPPORT = "Customer Support"
    TECHNICAL_EXPERT = "Technical Expert"
    RESEARCHER = "Researcher"

# Predefined default values for each role
PREDEFINED_INFO = {
    RoleType.GENERAL: {
        "goals": "Provide versatile support across various domains.",
        "background": "A general-purpose agent designed to adapt to multiple contexts."
    },
    RoleType.ASSISTANT: {
        "goals": "Assist users by offering timely and personalized support.",
        "background": "An assistant agent crafted to manage schedules, answer queries, and help with daily tasks."
    },
    RoleType.CUSTOMER_SUPPORT: {
        "goals": "Resolve customer issues promptly and provide clear guidance.",
        "background": "A customer support agent specialized in understanding user concerns and delivering effective solutions."
    },
    RoleType.TECHNICAL_EXPERT: {
        "goals": "Provide expert technical advice and troubleshoot complex problems.",
        "background": "A technical expert agent with deep domain knowledge to assist with intricate technical issues."
    },
    RoleType.RESEARCHER: {
        "goals": "Conduct thorough research and offer insights on advanced topics.",
        "background": "A researcher agent designed to synthesize complex information and present well-informed perspectives."
    }
} 


================================================
FILE: src/memorizz/shared_memory/__init__.py
================================================
from .shared_memory import SharedMemory, BlackboardEntry

__all__ = [
    'SharedMemory',
    'BlackboardEntry'
] 


================================================
FILE: src/memorizz/shared_memory/shared_memory.py
================================================
from typing import Dict, Any, List, Optional
from datetime import datetime
from ..memory_provider import MemoryProvider
from ..memory_provider.memory_type import MemoryType
from bson import ObjectId
import json
import logging

logger = logging.getLogger(__name__)

class BlackboardEntry:
    """Individual entry in the shared memory blackboard."""
    
    def __init__(self, 
                 agent_id: str, 
                 content: Any, 
                 entry_type: str, 
                 created_at: datetime = None):
        self.agent_id = agent_id
        self.content = content
        self.entry_type = entry_type  # "tool_call", "conversation", "task_assignment", "result"
        self.created_at = created_at or datetime.now()
        self.memory_id = str(ObjectId())
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            "memory_id": self.memory_id,
            "agent_id": self.agent_id,
            "content": self.content,
            "entry_type": self.entry_type,
            "created_at": self.created_at.isoformat()
        }

class SharedMemory:
    """Shared memory system for multi-agent coordination."""
    
    def __init__(self, memory_provider: MemoryProvider):
        self.memory_provider = memory_provider
        
    def create_shared_session(self, 
                             root_agent_id: str, 
                             delegate_agent_ids: List[str] = None) -> str:
        """
        Create a new shared memory session for multi-agent coordination.
        
        Parameters:
            root_agent_id (str): The ID of the root/orchestrating agent
            delegate_agent_ids (List[str]): List of delegate agent IDs
            
        Returns:
            str: The memory ID for the shared session
        """
        shared_session = {
            "memory_id": str(ObjectId()),
            "root_agent_id": root_agent_id,
            "delegate_agent_ids": delegate_agent_ids or [],
            "sub_agent_ids": [],  # Will be populated recursively
            "blackboard": [],
            "created_at": datetime.now().isoformat(),
            "status": "active"  # active, completed, failed
        }
        
        # Store in memory provider
        memory_id = self.memory_provider.store(shared_session, MemoryType.SHARED_MEMORY)
        return str(memory_id)
    
    def add_blackboard_entry(self, 
                           memory_id: str, 
                           agent_id: str, 
                           content: Any, 
                           entry_type: str) -> bool:
        """
        Add an entry to the shared blackboard.
        
        Parameters:
            memory_id (str): The shared memory ID
            agent_id (str): The ID of the agent adding the entry
            content (Any): The content to add
            entry_type (str): Type of entry (tool_call, conversation, etc.)
            
        Returns:
            bool: Success status
        """
        try:
            logger.info(f"Adding blackboard entry - memory_id: {memory_id}, agent_id: {agent_id}, entry_type: {entry_type}")
            
            # Get the shared session
            session = self.memory_provider.retrieve_by_id(memory_id, MemoryType.SHARED_MEMORY)
            if not session:
                logger.error(f"Session not found: {memory_id}")
                return False
            
            logger.info(f"Retrieved session with {len(session.get('blackboard', []))} existing entries")
            
            # Create blackboard entry
            entry = BlackboardEntry(agent_id, content, entry_type)
            logger.info(f"Created blackboard entry with memory_id: {entry.memory_id}")
            
            # Add to blackboard
            session["blackboard"].append(entry.to_dict())
            logger.info(f"Added entry to session blackboard, now has {len(session['blackboard'])} entries")
            
            # Update in storage
            update_result = self.memory_provider.update_by_id(memory_id, session, MemoryType.SHARED_MEMORY)
            logger.info(f"Memory provider update result: {update_result}")
            
            return update_result
            
        except Exception as e:
            logger.error(f"Error adding blackboard entry: {e}", exc_info=True)
            return False
    
    def get_blackboard_entries(self, 
                              memory_id: str, 
                              agent_id: str = None, 
                              entry_type: str = None) -> List[Dict[str, Any]]:
        """
        Retrieve blackboard entries with optional filtering.
        
        Parameters:
            memory_id (str): The shared memory ID
            agent_id (str, optional): Filter by agent ID
            entry_type (str, optional): Filter by entry type
            
        Returns:
            List[Dict[str, Any]]: List of blackboard entries
        """
        try:
            session = self.memory_provider.retrieve_by_id(memory_id, MemoryType.SHARED_MEMORY)
            if not session:
                return []
            
            entries = session.get("blackboard", [])
            
            # Apply filters
            if agent_id:
                entries = [e for e in entries if e.get("agent_id") == agent_id]
            if entry_type:
                entries = [e for e in entries if e.get("entry_type") == entry_type]
            
            return entries
            
        except Exception as e:
            logger.error(f"Error retrieving blackboard entries: {e}")
            return []
    
    def update_session_status(self, memory_id: str, status: str) -> bool:
        """Update the status of a shared session."""
        try:
            session = self.memory_provider.retrieve_by_id(memory_id, MemoryType.SHARED_MEMORY)
            if session:
                session["status"] = status
                return self.memory_provider.update_by_id(memory_id, session, MemoryType.SHARED_MEMORY)
            return False
        except Exception as e:
            logger.error(f"Error updating session status: {e}")
            return False
    
    def is_root_agent(self, memory_id: str, agent_id: str) -> bool:
        """Check if an agent is the root agent for a session."""
        try:
            session = self.memory_provider.retrieve_by_id(memory_id, MemoryType.SHARED_MEMORY)
            return session and session.get("root_agent_id") == agent_id
        except Exception as e:
            logger.error(f"Error checking root agent status: {e}")
            return False
    
    def get_session_by_root_agent(self, root_agent_id: str) -> Optional[Dict[str, Any]]:
        """Get active shared session by root agent ID."""
        try:
            # This would need to be implemented in the memory provider
            # For now, we'll need to search through sessions
            return None
        except Exception as e:
            logger.error(f"Error getting session by root agent: {e}")
            return None
    
    def find_active_session_for_agent(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """
        Find an active shared memory session where the agent is already participating.
        
        This enables hierarchical multi-agent coordination by allowing sub-agents
        to join existing sessions rather than creating isolated ones.
        
        Parameters:
            agent_id (str): The ID of the agent to search for
            
        Returns:
            Optional[Dict[str, Any]]: The active session if found, None otherwise
        """
        try:
            # Get all active shared memory sessions
            all_sessions = self.memory_provider.list_all(MemoryType.SHARED_MEMORY)
            
            # Handle case where list_all returns None or empty list
            if not all_sessions:
                return None
            
            for session in all_sessions:
                # Only consider active sessions
                if session.get("status") != "active":
                    continue
                
                # Check if agent is root, delegate, or sub-agent in this session
                if (session.get("root_agent_id") == agent_id or
                    agent_id in session.get("delegate_agent_ids", []) or
                    agent_id in session.get("sub_agent_ids", [])):
                    return session
            
            return None
            
        except Exception as e:
            logger.error(f"Error finding active session for agent {agent_id}: {e}")
            return None
    
    def register_sub_agents(self, 
                           memory_id: str, 
                           parent_agent_id: str, 
                           sub_agent_ids: List[str]) -> bool:
        """
        Register sub-agents in an existing shared memory session.
        
        This method enables hierarchical agent coordination by tracking the complete
        agent hierarchy within a single shared memory session. When a delegate agent
        has its own sub-agents, they are registered here rather than creating a new session.
        
        Parameters:
            memory_id (str): The shared memory session ID
            parent_agent_id (str): The ID of the agent that owns these sub-agents
            sub_agent_ids (List[str]): List of sub-agent IDs to register
            
        Returns:
            bool: Success status
        """
        try:
            logger.info(f"Registering sub-agents {sub_agent_ids} under parent {parent_agent_id} in session {memory_id}")
            
            # Get the shared session
            session = self.memory_provider.retrieve_by_id(memory_id, MemoryType.SHARED_MEMORY)
            if not session:
                logger.error(f"Session not found: {memory_id}")
                return False
            
            # Ensure sub_agent_ids field exists and is a list
            if "sub_agent_ids" not in session:
                session["sub_agent_ids"] = []
            
            # Add new sub-agents (avoid duplicates)
            existing_sub_agents = set(session["sub_agent_ids"])
            new_sub_agents = [agent_id for agent_id in sub_agent_ids if agent_id not in existing_sub_agents]
            
            if new_sub_agents:
                session["sub_agent_ids"].extend(new_sub_agents)
                
                # Log the hierarchy registration for debugging
                self.add_blackboard_entry(
                    memory_id=memory_id,
                    agent_id=parent_agent_id,
                    content={
                        "action": "sub_agent_registration",
                        "parent_agent": parent_agent_id,
                        "registered_sub_agents": new_sub_agents,
                        "total_sub_agents": len(session["sub_agent_ids"])
                    },
                    entry_type="hierarchy_update"
                )
                
                # Update in storage
                update_result = self.memory_provider.update_by_id(memory_id, session, MemoryType.SHARED_MEMORY)
                logger.info(f"Successfully registered {len(new_sub_agents)} new sub-agents")
                return update_result
            else:
                logger.info("All sub-agents already registered")
                return True
                
        except Exception as e:
            logger.error(f"Error registering sub-agents: {e}", exc_info=True)
            return False
    
    def get_agent_hierarchy(self, memory_id: str) -> Dict[str, Any]:
        """
        Get the complete agent hierarchy for a shared memory session.
        
        This provides visibility into the full multi-level agent structure,
        useful for debugging, monitoring, and coordination decisions.
        
        Parameters:
            memory_id (str): The shared memory session ID
            
        Returns:
            Dict[str, Any]: Hierarchy information including all agent levels
        """
        try:
            session = self.memory_provider.retrieve_by_id(memory_id, MemoryType.SHARED_MEMORY)
            if not session:
                return {}
            
            hierarchy = {
                "root_agent": session.get("root_agent_id"),
                "delegate_agents": session.get("delegate_agent_ids", []),
                "sub_agents": session.get("sub_agent_ids", []),
                "total_agents": 1 + len(session.get("delegate_agent_ids", [])) + len(session.get("sub_agent_ids", [])),
                "session_status": session.get("status"),
                "created_at": session.get("created_at")
            }
            
            return hierarchy
            
        except Exception as e:
            logger.error(f"Error getting agent hierarchy: {e}")
            return {} 


================================================
FILE: src/memorizz/short_term_memory/__init__.py
================================================
[Empty file]


================================================
FILE: src/memorizz/short_term_memory/semantic_cache.py
================================================


class SemanticCache:
    def __init__(self):
        self.cache = {}

    def get(self, key):
        return self.cache.get(key)

    def set(self, key, value):
        # When a key is placed into the cache, check if the key exists in the memory provider
        if self.memory_provider.get(key) is not None:
            # If the key exists in the memory provider, update the value in the cache
            self.cache[key] = value
        else:
            # If the key does not exist in the memory provider, add the key to the memory provider
            self.memory_provider.add(key, value)
        self.cache[key] = value


================================================
FILE: src/memorizz/tests/test_memagent_enhanced_tools.py
================================================
"""
Test the enhanced tool functionality for MemAgent.
This test demonstrates the new ability to add decorated functions directly.
"""
import os
import unittest
from unittest.mock import MagicMock, patch, Mock
from memorizz.memagent import MemAgent
from memorizz.memory_provider import MemoryProvider
from memorizz.llms.openai import OpenAI
from memorizz.memory_provider.mongodb.provider import MongoDBProvider, MongoDBConfig
from memorizz.multi_agent_orchestrator import MultiAgentOrchestrator
from memorizz.shared_memory.shared_memory import SharedMemory
import sys

# Add the src directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))


class TestMemAgentEnhancedTools(unittest.TestCase):
    
    def setUp(self):
        """Set up test fixtures."""
        self.mock_memory_provider = MagicMock(spec=MemoryProvider)
        self.mock_model = MagicMock(spec=OpenAI)
        
        # Create agent with mocked dependencies
        self.agent = MemAgent(
            model=self.mock_model,
            memory_provider=self.mock_memory_provider
        )
    
    def test_add_decorated_function_ephemeral(self):
        """Test adding a decorated function without persistence."""
        
        def sample_tool(message: str, count: int = 1) -> str:
            """
            Repeat a message a specified number of times.
            
            Args:
                message: The message to repeat
                count: Number of times to repeat (default: 1)
            
            Returns:
                The repeated message
            """
            return message * count
        
        # Add the function without persistence
        result = self.agent.add_tool(func=sample_tool, persist=False)
        
        # Verify the function was added successfully
        self.assertTrue(result)
        self.assertIsNotNone(self.agent.tools)
        self.assertEqual(len(self.agent.tools), 1)
        
        # Verify the tool schema is correctly generated
        tool = self.agent.tools[0]
        self.assertEqual(tool["type"], "function")
        self.assertEqual(tool["function"]["name"], "sample_tool")
        self.assertIn("Repeat a message", tool["function"]["description"])
        
        # Verify parameters are correctly extracted
        params = tool["function"]["parameters"]
        self.assertEqual(len(params), 2)
        
        # Check required parameter (message)
        message_param = next(p for p in params if p["name"] == "message")
        self.assertEqual(message_param["type"], "string")
        self.assertTrue(message_param["required"])
        
        # Check optional parameter (count)
        count_param = next(p for p in params if p["name"] == "count")
        self.assertEqual(count_param["type"], "integer")
        self.assertFalse(count_param["required"])
        
        # Verify required list
        required_params = tool["function"]["required"]
        self.assertIn("message", required_params)
        self.assertNotIn("count", required_params)
    
    def test_add_decorated_function_persistent(self):
        """Test adding a decorated function with persistence."""
        
        def persistent_tool(data: str) -> str:
            """Process some data."""
            return f"Processed: {data}"
        
        # Mock the memory provider store method
        self.mock_memory_provider.store.return_value = "test-tool-id-123"
        
        # Add the function with persistence
        result = self.agent.add_tool(func=persistent_tool, persist=True)
        
        # Verify the function was added successfully
        self.assertTrue(result)
        self.assertIsNotNone(self.agent.tools)
        
        # Verify memory provider was called to store the tool
        self.mock_memory_provider.store.assert_called_once()
        
        # Verify the stored tool has the correct _id from memory provider
        tool = self.agent.tools[0]
        self.assertEqual(tool.get("function", {}).get("name"), "persistent_tool")
    
    def test_add_multiple_functions(self):
        """Test adding multiple functions at once."""
        
        def tool_one(x: int) -> int:
            """First tool."""
            return x * 2
        
        def tool_two(y: str) -> str:
            """Second tool."""
            return y.upper()
        
        def tool_three(z: float) -> float:
            """Third tool."""
            return z / 2
        
        functions = [tool_one, tool_two, tool_three]
        
        # Add all functions at once
        result = self.agent.add_tools(funcs=functions, persist=False)
        
        # Verify all functions were added successfully
        self.assertTrue(result)
        self.assertEqual(len(self.agent.tools), 3)
        
        # Verify each tool was added correctly
        tool_names = [tool["function"]["name"] for tool in self.agent.tools]
        self.assertIn("tool_one", tool_names)
        self.assertIn("tool_two", tool_names)
        self.assertIn("tool_three", tool_names)
    
    def test_update_existing_function(self):
        """Test updating an existing function with the same name."""
        
        def original_tool(message: str) -> str:
            """Original version."""
            return message
        
        def updated_tool(message: str) -> str:
            """Updated version with better functionality."""
            return f"Enhanced: {message}"
        
        # Add original function
        self.agent.add_tool(func=original_tool, persist=False)
        self.assertEqual(len(self.agent.tools), 1)
        original_description = self.agent.tools[0]["function"]["description"]
        
        # Add updated function with same name
        self.agent.add_tool(func=updated_tool, persist=False)
        
        # Verify only one tool exists (updated, not duplicated)
        self.assertEqual(len(self.agent.tools), 1)
        updated_description = self.agent.tools[0]["function"]["description"]
        
        # Verify the description was updated
        self.assertNotEqual(original_description, updated_description)
        self.assertIn("better functionality", updated_description)
    
    def test_error_handling_invalid_function(self):
        """Test error handling for invalid function input."""
        
        # Test with non-callable object
        with self.assertRaises(ValueError):
            self.agent.add_tool(func="not_a_function")
        
        # Test with None but no other parameters
        with self.assertRaises(ValueError):
            self.agent.add_tool()
    
    def test_backward_compatibility(self):
        """Test that existing functionality still works."""
        
        # Mock toolbox and existing methods should still work
        from memorizz.toolbox import Toolbox
        mock_toolbox = MagicMock(spec=Toolbox)
        mock_toolbox.list_tools.return_value = []
        
        # This should not raise an error
        result = self.agent.add_tool(toolbox=mock_toolbox)
        
        # The exact return value depends on toolbox content,
        # but it should complete without error
        self.assertIsInstance(result, bool)


class TestMultiAgentMemoryManagement(unittest.TestCase):
    """Test multi-agent memory management fixes."""
    
    def setUp(self):
        """Set up test fixtures."""
        # Mock memory provider
        self.mock_memory_provider = Mock(spec=MongoDBProvider)
        self.mock_memory_provider.update_memagent_memory_ids = Mock(return_value=True)
        
        # Create root agent
        self.root_agent = MemAgent(
            agent_id="root_agent_001",
            memory_provider=self.mock_memory_provider,
            memory_ids=["existing_memory_001"]
        )
        
        # Create delegate agents
        self.delegate1 = MemAgent(
            agent_id="delegate_001",
            memory_provider=self.mock_memory_provider,
            memory_ids=["delegate_memory_001"]
        )
        
        self.delegate2 = MemAgent(
            agent_id="delegate_002", 
            memory_provider=self.mock_memory_provider,
            memory_ids=[]
        )
        
        self.delegates = [self.delegate1, self.delegate2]
    
    @patch('memorizz.multi_agent_orchestrator.SharedMemory')
    @patch('memorizz.multi_agent_orchestrator.TaskDecomposer')
    def test_shared_memory_id_added_to_root_agent(self, mock_task_decomposer, mock_shared_memory_class):
        """Test that shared memory ID is added to root agent's memory_ids array."""
        
        # Mock shared memory instance
        mock_shared_memory = Mock(spec=SharedMemory)
        mock_shared_memory.create_shared_session.return_value = "shared_memory_123"
        mock_shared_memory.find_active_session_for_agent.return_value = None
        mock_shared_memory.add_blackboard_entry.return_value = True
        mock_shared_memory.update_session_status.return_value = True
        mock_shared_memory_class.return_value = mock_shared_memory
        
        # Mock task decomposer
        mock_task_decomposer_instance = Mock()
        mock_task_decomposer_instance.decompose_task.return_value = []  # No tasks to simulate fallback
        mock_task_decomposer.return_value = mock_task_decomposer_instance
        
        # Mock root agent run method for fallback
        self.root_agent.run = Mock(return_value="Fallback response")
        
        # Create orchestrator
        orchestrator = MultiAgentOrchestrator(self.root_agent, self.delegates)
        
        # Verify initial state
        initial_memory_ids = self.root_agent.memory_ids.copy()
        self.assertNotIn("shared_memory_123", initial_memory_ids)
        
        # Execute multi-agent workflow
        result = orchestrator.execute_multi_agent_workflow("Test query")
        
        # Verify shared memory ID was added to root agent
        self.assertIn("shared_memory_123", self.root_agent.memory_ids)
        
        # Verify memory provider was called to persist the update
        self.mock_memory_provider.update_memagent_memory_ids.assert_called()
        
        # Verify the call was made with the correct parameters
        called_args = self.mock_memory_provider.update_memagent_memory_ids.call_args_list
        root_agent_call = next((call for call in called_args if call[0][0] == "root_agent_001"), None)
        self.assertIsNotNone(root_agent_call)
        self.assertIn("shared_memory_123", root_agent_call[0][1])
    
    @patch('memorizz.multi_agent_orchestrator.SharedMemory')
    @patch('memorizz.multi_agent_orchestrator.TaskDecomposer')
    def test_shared_memory_id_added_to_delegate_agents(self, mock_task_decomposer, mock_shared_memory_class):
        """Test that shared memory ID is added to delegate agents' memory_ids arrays."""
        
        # Mock shared memory instance
        mock_shared_memory = Mock(spec=SharedMemory)
        mock_shared_memory.create_shared_session.return_value = "shared_memory_456"
        mock_shared_memory.find_active_session_for_agent.return_value = None
        mock_shared_memory.add_blackboard_entry.return_value = True
        mock_shared_memory.update_session_status.return_value = True
        mock_shared_memory_class.return_value = mock_shared_memory
        
        # Mock task decomposer
        mock_task_decomposer_instance = Mock()
        mock_task_decomposer_instance.decompose_task.return_value = []  # No tasks to simulate fallback
        mock_task_decomposer.return_value = mock_task_decomposer_instance
        
        # Mock root agent run method for fallback
        self.root_agent.run = Mock(return_value="Fallback response")
        
        # Create orchestrator
        orchestrator = MultiAgentOrchestrator(self.root_agent, self.delegates)
        
        # Verify initial state
        self.assertNotIn("shared_memory_456", self.delegate1.memory_ids)
        self.assertNotIn("shared_memory_456", self.delegate2.memory_ids)
        
        # Execute multi-agent workflow
        result = orchestrator.execute_multi_agent_workflow("Test query")
        
        # Verify shared memory ID was added to both delegates
        self.assertIn("shared_memory_456", self.delegate1.memory_ids)
        self.assertIn("shared_memory_456", self.delegate2.memory_ids)
        
        # Verify memory provider was called for both delegates
        called_args = self.mock_memory_provider.update_memagent_memory_ids.call_args_list
        
        delegate1_call = next((call for call in called_args if call[0][0] == "delegate_001"), None)
        self.assertIsNotNone(delegate1_call)
        self.assertIn("shared_memory_456", delegate1_call[0][1])
        
        delegate2_call = next((call for call in called_args if call[0][0] == "delegate_002"), None)
        self.assertIsNotNone(delegate2_call)
        self.assertIn("shared_memory_456", delegate2_call[0][1])
    
    @patch('memorizz.multi_agent_orchestrator.SharedMemory')
    @patch('memorizz.multi_agent_orchestrator.TaskDecomposer')
    def test_no_duplicate_shared_memory_ids(self, mock_task_decomposer, mock_shared_memory_class):
        """Test that shared memory ID is not duplicated if already present."""
        
        # Pre-add shared memory ID to root agent
        shared_memory_id = "shared_memory_789"
        self.root_agent.memory_ids.append(shared_memory_id)
        initial_count = self.root_agent.memory_ids.count(shared_memory_id)
        
        # Mock shared memory instance
        mock_shared_memory = Mock(spec=SharedMemory)
        mock_shared_memory.create_shared_session.return_value = shared_memory_id
        mock_shared_memory.find_active_session_for_agent.return_value = None
        mock_shared_memory.add_blackboard_entry.return_value = True
        mock_shared_memory.update_session_status.return_value = True
        mock_shared_memory_class.return_value = mock_shared_memory
        
        # Mock task decomposer
        mock_task_decomposer_instance = Mock()
        mock_task_decomposer_instance.decompose_task.return_value = []
        mock_task_decomposer.return_value = mock_task_decomposer_instance
        
        # Mock root agent run method for fallback
        self.root_agent.run = Mock(return_value="Fallback response")
        
        # Create orchestrator
        orchestrator = MultiAgentOrchestrator(self.root_agent, self.delegates)
        
        # Execute multi-agent workflow
        result = orchestrator.execute_multi_agent_workflow("Test query")
        
        # Verify no duplication occurred
        final_count = self.root_agent.memory_ids.count(shared_memory_id)
        self.assertEqual(initial_count, final_count)


if __name__ == "__main__":
    unittest.main()


================================================
FILE: src/memorizz/tests/test_vegetarian_recipe_agent.py
================================================
import os
import sys
import pytest
from dotenv import load_dotenv
from scenario import Scenario, TestingAgent, scenario_cache

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
sys.path.insert(0, project_root)
load_dotenv()

from ..memagent import MemAgent
from ..memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider



# Create a memory provider
mongodb_config = MongoDBConfig(uri=os.environ["MONGODB_URI"])
memory_provider = MongoDBProvider(mongodb_config)

Scenario.configure(testing_agent=TestingAgent(model="openai/gpt-4o-mini"))

mem_agent = MemAgent(memory_provider=memory_provider)

@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_vegetarian_recipe_agent():
    agent = mem_agent

    def vegetarian_recipe_agent(message, context):
        # Call your agent here
        response = agent.run(message)
        return {
            "message": response
        }

    # Define the scenario
    scenario = Scenario(
        "User is looking for a dinner idea",
        agent=vegetarian_recipe_agent,
        success_criteria=[
            "Recipe agent generates a vegetarian recipe",
            "Recipe includes a list of ingredients",
            "Recipe includes step-by-step cooking instructions",
        ],
        failure_criteria=[
            "The recipe is not vegetarian or includes meat",
            "The agent asks more than two follow-up questions",
        ],
    )

    # Run the scenario and get results
    result = await scenario.run()

    # Assert for pytest to know whether the test passed
    assert result.success



================================================
FILE: src/memorizz/toolbox/README.md
================================================
# Toolbox Module

The Toolbox module provides a powerful framework for registering, managing, and utilizing external functions as AI-callable tools within the MemoRizz library. This system enables agents to seamlessly interact with external systems, APIs, and data sources through a standardized interface.

## Core Features

- Register Python functions as semantically discoverable tools
- Generate embeddings for advanced similarity-based tool retrieval
- Store and retrieve tools from memory providers
- Find the most relevant tools based on natural language queries
- Integrate tools with MemAgents for intelligent function execution

## Usage

### Creating a Toolbox

```python
from src.memorizz.toolbox import Toolbox
from src.memorizz.memory_provider import MemoryProvider

# Initialize a memory provider
memory_provider = MemoryProvider()

# Create a toolbox instance
toolbox = Toolbox(memory_provider)
```

### Registering Tools

```python
# Define a function to be used as a tool
def get_weather(latitude: float, longitude: float) -> float:
    """
    Get the current temperature at the specified coordinates.
    
    Parameters:
    -----------
    latitude : float
        The latitude coordinate (between -90 and 90)
    longitude : float
        The longitude coordinate (between -180 and 180)
        
    Returns:
    --------
    float
        The current temperature in Celsius
    """
    # Implementation here...
    return temperature

# Register the function as a tool
weather_tool_id = toolbox.register_tool(get_weather)
print(f"Registered tool with ID: {weather_tool_id}")

# You can also use the decorator pattern
@toolbox.register_tool
def get_stock_price(symbol: str, currency: str = "USD") -> str:
    """
    Get the current stock price for a given stock symbol.
    
    Parameters:
    -----------
    symbol : str
        The stock symbol to look up (e.g., 'AAPL' for Apple Inc.)
    currency : str, optional
        The currency code to convert the price into (defaults to 'USD')
        
    Returns:
    --------
    str
        A string with the current stock price
    """
    # Implementation here...
    return f"The current price of {symbol} is {price} {currency}."
```

### Retrieving Tools

```python
# Get a tool by its name
weather_tool = toolbox.get_tool_by_name("get_weather")

# Get a tool by its ID
stock_tool = toolbox.get_tool_by_id(weather_tool_id)

# Find tools based on a natural language query
finance_tools = toolbox.get_most_similar_tools(
    "I need something that can tell me about stocks", 
    limit=3
)

# List all available tools
all_tools = toolbox.list_tools()
```

### Tool Management

```python
# Delete a tool by name
toolbox.delete_tool_by_name("get_weather")

# Delete a tool by ID
toolbox.delete_tool_by_id(stock_tool_id)

# Update tool metadata
toolbox.update_tool_by_id(
    tool_id, 
    {"description": "An improved description of this tool"}
)

# Clear all tools
toolbox.delete_all()
```

### Using with MemAgents

Tools can be added to MemAgents in two ways:

```python
from src.memorizz.memagent import MemAgent

# Create an agent
agent = MemAgent(
    memory_provider=memory_provider,
    instruction="Help users with financial and weather data"
)

# Method 1: Add a specific tool by ID
agent.add_tool(tool_id=weather_tool_id)

# Method 2: Add all tools from a toolbox
agent.add_tool(toolbox=toolbox)

# Control how tools are accessed
agent = MemAgent(
    tool_access="private",  # Only use explicitly added tools (default)
    # OR
    tool_access="global",   # Dynamically discover relevant tools from the toolbox
)

# Run the agent with tool access
response = agent.run("What's the weather like in New York?")
```

## Design Considerations

### Tool Persistence
- Tools are stored in the memory provider with both metadata and embeddings
- Function implementations are kept in memory during runtime
- Tool metadata persists across application restarts

### MemAgent Integration
- A copy of tool metadata is stored in the agent's `tools` attribute
- Function references are maintained in the agent's `_tool_functions` property
- Tools can be refreshed with `refresh_tools(tool_id)` when implementations change
- Missing implementations are handled gracefully during execution

### Access Control
- The `tool_access` attribute controls tool discovery during agent execution:
  - `"private"`: Only access tools explicitly added to the agent
  - `"global"`: Dynamically discover tools from the toolbox based on query relevance

### Performance Optimization
- Embeddings enable efficient semantic search for relevant tools
- Batch tool registration is supported for populating toolboxes efficiently
- Tool augmentation with `augment=True` enhances discoverability with LLM-generated metadata

## Implementation Notes

- Tool docstrings are used to generate rich metadata and improve discoverability
- Function signatures are analyzed to create parameter schemas for LLMs
- The system automatically handles type conversions between Python and JSON
- Error handling ensures failures in one tool don't crash the entire agent

This architecture provides a clean separation between tool definition (in the toolbox) and tool availability (in the agent), enabling fine-grained control over which capabilities are exposed to different agents in your system.



================================================
FILE: src/memorizz/toolbox/__init__.py
================================================
from .toolbox import Toolbox

__all__ = ['Toolbox']


================================================
FILE: src/memorizz/toolbox/tool_schema.py
================================================
from pydantic import BaseModel
from typing import List

class ParameterSchema(BaseModel):
    """
    A schema for the parameter.
    """
    name: str
    description: str
    type: str
    required: bool

class FunctionSchema(BaseModel):
    """
    A schema for the function.
    """
    name: str
    description: str
    parameters: list[ParameterSchema]
    required: List[str]
    queries: List[str]
    
class ToolSchemaType(BaseModel):
    """
    A schema for the tool.
    This can be the OpenAI function calling schema or Google function calling schema.
    """
    type: str
    function: FunctionSchema


================================================
FILE: src/memorizz/toolbox/toolbox.py
================================================
from typing import Dict, Any, List, Callable, Optional, Union, TYPE_CHECKING
from ..memory_provider import MemoryProvider
from ..memory_provider.memory_type import MemoryType
from ..embeddings.openai import get_embedding
# Use TYPE_CHECKING for forward references to avoid circular imports
if TYPE_CHECKING:
    from ..llms.openai import OpenAI
import inspect
import uuid
from .tool_schema import ToolSchemaType
from bson import ObjectId

# Initialize OpenAI lazily to avoid circular imports
def get_openai():
    from ..llms.openai import OpenAI
    return OpenAI()

class Toolbox:
    """A toolbox for managing and retrieving tools using a memory provider."""
    
    def __init__(self, memory_provider: MemoryProvider):
        """
        Initialize the toolbox with a memory provider.
        
        Parameters:
        -----------
        memory_provider : MemoryProvider
            The memory provider to use for storing and retrieving tools.
        """
        self.memory_provider = memory_provider
        
        # In-memory storage of functions
        self._tools: Dict[str, Callable] = {}

    def register_tool(self, func: Optional[Callable] = None, augment: bool = False) -> Union[str, Callable]:
        """
        Register a function as a tool in the toolbox.
        
        Parameters:
        -----------
        func : Callable, optional
            The function to register as a tool. If None, returns a decorator.
        augment : bool, optional
            Whether to augment the tool docstring with an LLM generated description.
            And also include to the metadata synthecially generated queries that are used in the embedding generation process and used to seach the tool.
        Returns:
        --------
        Union[str, Callable]
            If func is provided, returns the tool ID. Otherwise returns a decorator.
        """
        def decorator(f: Callable) -> str:
            # Get the function's docstring and signature
            docstring = f.__doc__ or ""
            signature = str(inspect.signature(f))
            
            # Pre-generate MongoDB ObjectId to use as both database _id and in-memory key
            object_id = ObjectId()
            object_id_str = str(object_id)

            if augment:
                # Extend the docstring with an LLM generated description
                docstring = self._augment_docstring(docstring)

                # Generate synthecially generated queries
                queries = self._generate_queries(docstring)

                # Generate embedding for the tool using the augmented docstring, function name, signature and queries
                embedding = get_embedding(f"{f.__name__} {docstring} {signature} {queries}")

                # Get the tool metadata
                tool_data = self._get_tool_metadata(f)
                
                # Create a dictionary with the tool data and embedding using pre-generated _id
                tool_dict = {
                    "_id": object_id,  # Use pre-generated ObjectId as _id
                    "embedding": embedding,
                    "queries": queries,
                    **tool_data.model_dump()
                }
            else:
                # Generate embedding for the tool using the function name, docstring and signature
                embedding = get_embedding(f"{f.__name__} {docstring} {signature}")

                # Get the tool metadata
                tool_data = self._get_tool_metadata(f)
                
                # Create a dictionary with the tool data and embedding using pre-generated _id
                tool_dict = {
                    "_id": object_id,  # Use pre-generated ObjectId as _id
                    "embedding": embedding,
                    **tool_data.model_dump()
                }
            
            # Store the tool metadata in the memory provider
            self.memory_provider.store(tool_dict, memory_store_type=MemoryType.TOOLBOX)
            
            # Store the actual function in memory using ObjectId string as key
            self._tools[object_id_str] = f
            
            return object_id_str

        if func is None:
            return decorator
        return decorator(func)

    def get_tool_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Get a single tool by its name.
        
        Parameters:
        -----------
        name : str
            The name of the tool to retrieve.
        
        Returns:
        --------
        Dict[str, Any]
            The tool data, or None if not found.
        
        """
        # First check if we have the function in memory
        if name in self._tools:
            return self._tools[name]
        
        # If not, try to get it from the provider
        # One thing to note is that the name is not unique and we get a single tool matching the name
        tool_data = self.memory_provider.retrieve_by_name(name, memory_store_type=MemoryType.TOOLBOX)

        if tool_data:
            return tool_data
        
        return None
    
    def get_tool_by_id(self, id: str) -> Optional[Dict[str, Any]]:
        """
        Get a tool by its id.

        Parameters:
        -----------
        id : str
            The id of the tool to retrieve.
        
        Returns:
        --------
        Dict[str, Any]
            The tool data, or None if not found.
        """
        return self.memory_provider.retrieve_by_id(id, memory_store_type=MemoryType.TOOLBOX)

    def get_most_similar_tools(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Get the most similar tools to a query.
        
        Parameters:
        -----------
        query : str
            The query to search for.
        limit : int, optional
            The maximum number of tools to return.
        
        Returns:
        --------
        List[Dict[str, Any]]
            A list of the most similar tool metadata (without actual function objects).
        """
        similar_tools = self.memory_provider.retrieve_by_query(
            query,
            memory_store_type=MemoryType.TOOLBOX,
            limit=limit
        )
        
        # Return just the metadata - do NOT add actual function objects
        # The functions are kept separate in self._tools for execution
        return similar_tools

    def delete_tool_by_name(self, name: str) -> bool:
        """
        Delete a tool from the toolbox by name.
        
        Parameters:
        -----------
        name : str
            The name of the tool to delete.
        
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        # Find the tool by name first to get its _id
        tool_data = self.memory_provider.retrieve_by_name(name, memory_store_type=MemoryType.TOOLBOX)
        if tool_data:
            tool_id = str(tool_data.get("_id"))
            # Delete from memory using _id
            if tool_id in self._tools:
                del self._tools[tool_id]
        
        # Delete from provider by name
        return self.memory_provider.delete_by_name(name, memory_store_type=MemoryType.TOOLBOX)
    
    def delete_tool_by_id(self, id: str) -> bool:
        """
        Delete a tool from the toolbox by id.

        Parameters:
        -----------
        id : str
            The id of the tool to delete.
        
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        # Delete from in-memory storage first
        if id in self._tools:
            del self._tools[id]
        
        # Delete from provider
        return self.memory_provider.delete_by_id(id, memory_store_type=MemoryType.TOOLBOX)
    
    def delete_all(self) -> bool:
        """
        Delete all tools in the toolbox.

        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        # Clear in-memory storage
        self._tools.clear()
        
        # Delete from provider
        return self.memory_provider.delete_all(memory_store_type=MemoryType.TOOLBOX)
    
    def list_tools(self) -> List[Dict[str, Any]]:
        """
        List all tools in the toolbox from the memory provider.
        Note: This returns ALL tool metadata from the database, 
        regardless of whether the functions are available in this session.
        
        Returns:
        --------
        List[Dict[str, Any]]
            A list of all tool metadata from the memory provider.
        """
        return self.memory_provider.list_all(memory_store_type=MemoryType.TOOLBOX)

    def list_available_tools(self) -> List[Dict[str, Any]]:
        """
        List only tools that have both metadata in the database AND 
        callable functions available in the current session.
        
        This is more efficient than list_tools() when you only want 
        tools that can actually be executed.
        
        Returns:
        --------
        List[Dict[str, Any]]
            A list of tool metadata for tools with available functions.
        """
        available_tools = []
        
        for tool_id, func in self._tools.items():
            if callable(func):
                # Get metadata for this tool
                meta = self.get_tool_by_id(tool_id)
                if meta:
                    available_tools.append(meta)
        
        return available_tools
    
    def get_function_by_id(self, tool_id: str) -> Optional[Callable]:
        """
        Get the actual executable function by tool ID.
        
        Parameters:
        -----------
        tool_id : str
            The ID of the tool whose function to retrieve.
        
        Returns:
        --------
        Optional[Callable]
            The actual function object, or None if not found.
        """
        return self._tools.get(tool_id)

    def update_tool_by_id(self, id: str, data: Dict[str, Any]) -> bool:
        """
        Update a tool in the toolbox by id.

        Parameters:
        -----------
        id : str
            The id of the tool to update.
        data : Dict[str, Any]
            The data to update the tool with.
        
        Returns:
        --------
        bool
            True if update was successful, False otherwise.
        """
        return self.memory_provider.update_by_id(id, data, memory_store_type=MemoryType.TOOLBOX)
    
    def _get_tool_metadata(self, func: Callable) -> ToolSchemaType:
        """
        Get the metadata for a tool.
        """
        return get_openai().get_tool_metadata(func)
    
    def _augment_docstring(self, docstring: str) -> str:
        """
        Augment the docstring with an LLM generated description.
        """
        return get_openai().augment_docstring(docstring)
    
    def _generate_queries(self, docstring: str) -> List[str]:
        """
        Generate queries for the tool.
        """
        return get_openai().generate_queries(docstring)
            
    
    


================================================
FILE: src/memorizz/workflow/__init__.py
================================================
from .workflow import Workflow

__all__ = ["Workflow", "WorkflowOutcome"]


================================================
FILE: src/memorizz/workflow/workflow.py
================================================
from typing import Dict, Any, Optional, Union, List
from ..memory_provider import MemoryProvider
from ..memory_provider.memory_type import MemoryType
from ..embeddings.openai import get_embedding
import uuid
from datetime import datetime
from enum import Enum
from bson import ObjectId

# Workflow outcome enum
class WorkflowOutcome(Enum):
    SUCCESS = "success"
    FAILURE = "failure"


class Workflow:
    def __init__(
        self,
        name: str,
        description: str = "",
        steps: Dict[str, Any] = None,
        workflow_id: str = None,
        created_at: datetime = None,
        updated_at: datetime = None,
        memory_id: str = None,
        outcome: WorkflowOutcome = None,
        user_query: str = None,
    ):
        """
        Initialize a new Workflow instance.
        
        Parameters:
        -----------
        name : str
            The name of the workflow.
        description : str
            A description of what the workflow does.
        steps : Dict[str, Any]
            The steps that make up the workflow.
        workflow_id : str
            The unique identifier for the workflow.
        created_at : datetime
            When the workflow was created.
        updated_at : datetime
            When the workflow was last updated.
        memory_id : str
            The memory ID associated with this workflow.
        outcome : WorkflowOutcome
            The outcome of the workflow (SUCCESS/FAILURE).
        user_query : str
            The original user query that triggered this workflow.
        """
        self.name = name
        self.description = description
        self.steps = steps or {}
        # Use MongoDB ObjectId for better performance
        self.workflow_id = workflow_id or str(ObjectId())
        self.created_at = created_at or datetime.now()
        self.updated_at = updated_at or datetime.now()
        self.memory_id = memory_id or str(ObjectId())
        self.outcome = outcome or WorkflowOutcome.SUCCESS
        self.user_query = user_query
        
        # Generate the embedding based on the workflow's attributes
        self.embedding = self._generate_embedding()

    def _generate_embedding(self):
        """
        Generate an embedding vector for the workflow based on its attributes.
        
        Returns:
        --------
        list or numpy.array: The embedding vector representing the workflow.
        """
        # Convert steps to string representation
        steps_str = str(self.steps)
        
        embedding_input = f"{self.name} {self.description} {steps_str} {self.outcome.value} {self.user_query or ''}"
        return get_embedding(embedding_input)

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the workflow to a dictionary.
        
        Returns:
        --------
        Dict[str, Any]
            The workflow as a dictionary.
        """
        return {
            "name": self.name,
            "description": self.description,
            "steps": self.steps,
            "workflow_id": self.workflow_id,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "memory_id": self.memory_id,
            "outcome": self.outcome.value,
            "embedding": self.embedding,
            "user_query": self.user_query
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Workflow':
        """
        Create a Workflow instance from a dictionary.
        
        Parameters:
        -----------
        data : Dict[str, Any]
            The dictionary containing workflow data.
            
        Returns:
        --------
        Workflow
            A new Workflow instance.
        """
        return cls(
            name=data["name"],
            description=data.get("description", ""),
            steps=data.get("steps", {}),
            workflow_id=data.get("workflow_id"),
            created_at=datetime.fromisoformat(data["created_at"]) if data.get("created_at") else None,
            updated_at=datetime.fromisoformat(data["updated_at"]) if data.get("updated_at") else None,
            memory_id=data.get("memory_id"),
            outcome=WorkflowOutcome(data.get("outcome", WorkflowOutcome.SUCCESS.value)),
            user_query=data.get("user_query")
        )

    def store_workflow(self, provider: MemoryProvider) -> str:
        """
        Store the workflow in the memory provider.
        
        Parameters:
        -----------
        provider : MemoryProvider
            The memory provider to use for storage.
            
        Returns:
        --------
        str
            The ID of the stored workflow.
        """
        workflow_data = self.to_dict()
        return provider.store(workflow_data, memory_store_type=MemoryType.WORKFLOW_MEMORY)

    @staticmethod
    def retrieve_workflow_by_id(workflow_id: str, provider: MemoryProvider) -> Optional['Workflow']:
        """
        Retrieve a workflow from the memory provider by workflow_id.
        
        Parameters:
        -----------
        workflow_id : str
            The unique identifier of the workflow to retrieve.
        provider : MemoryProvider
            The memory provider to use for retrieval.
            
        Returns:
        --------
        Optional[Workflow]
            The retrieved workflow, or None if not found.
        """
        workflow_data = provider.retrieve_by_id(workflow_id, memory_store_type=MemoryType.WORKFLOW_MEMORY)
        if workflow_data:
            return Workflow.from_dict(workflow_data)
        return None
    
    @staticmethod
    def retrieve_workflows_by_query(query: str, provider: MemoryProvider, limit: int = 5) -> List['Workflow']:
        """
        Retrieve workflows from the memory provider by query.
        
        Parameters:
        -----------
        query : str
            The query string to search for workflows.
        provider : MemoryProvider
            The memory provider to use for retrieval.
        limit : int
            Maximum number of workflows to retrieve.
            
        Returns:
        --------
        List[Workflow]
            A list of retrieved workflows, empty list if none found.
        """
        workflow_data = provider.retrieve_by_query(query, memory_store_type=MemoryType.WORKFLOW_MEMORY, limit=limit)
        if workflow_data is None:
            return []
        return [Workflow.from_dict(workflow) for workflow in workflow_data]
    
    
    @staticmethod
    def delete_workflow(workflow_id: str, provider: MemoryProvider) -> bool:
        """
        Delete a workflow from the memory provider using its workflow_id.
        
        Parameters:
        -----------
        workflow_id : str
            The unique identifier of the workflow to delete.
        provider : MemoryProvider
            The memory provider to use for deletion.
            
        Returns:
        --------
        bool
            True if deletion was successful, False otherwise.
        """
        return provider.delete_by_id(workflow_id, memory_store_type=MemoryType.WORKFLOW_MEMORY)

    def update_workflow(self, provider: MemoryProvider) -> bool:
        """
        Update the workflow in the memory provider.
        
        Parameters:
        -----------
        provider : MemoryProvider
            The memory provider to use for update.
            
        Returns:
        --------
        bool
            True if update was successful, False otherwise.
        """
        self.updated_at = datetime.now()
        workflow_data = self.to_dict()
        return provider.update_by_id(self.workflow_id, workflow_data, memory_store_type=MemoryType.WORKFLOW_MEMORY)

    def add_step(self, step_name: str, step_data: Dict[str, Any]) -> None:
        """
        Add a new step to the workflow.
        
        Parameters:
        -----------
        step_name : str
            The name of the step to add.
        step_data : Dict[str, Any]
            The data for the step.
        """
        self.steps[step_name] = step_data
        self.updated_at = datetime.now()

    def remove_step(self, step_name: str) -> None:
        """
        Remove a step from the workflow.
        
        Parameters:
        -----------
        step_name : str
            The name of the step to remove.
        """
        if step_name in self.steps:
            del self.steps[step_name]
            self.updated_at = datetime.now()

    def get_step(self, step_name: str) -> Optional[Dict[str, Any]]:
        """
        Get a specific step from the workflow.
        
        Parameters:
        -----------
        step_name : str
            The name of the step to get.
            
        Returns:
        --------
        Optional[Dict[str, Any]]
            The step data if found, None otherwise.
        """
        return self.steps.get(step_name)


